{"entries":[{"title":"","url":"/404.html","tags":[],"body":""},{"title":"","url":"/about.html","tags":[],"body":""},{"title":"","url":"/contribute.html","tags":[],"body":""},{"title":"","url":"/faq.html","tags":[],"body":"Overview Questions What is this website? What are the tutorials for? What audiences are the tutorials for? How is the content licensed? How can I advertise the training materials on my posters? How do I use this material? How can I get help? For Instructors Where do I start? How can I fix mistakes or expand an existing tutorial using the GitHub interface? Sustainability of the training-material and metadata Overview Questions What is this website? This website is a collection of hands-on tutorials that are designed to be interactive. This material is developed and maintained by the VIB Bioinformatics Core. What are the tutorials for? These tutorials can be used for learning and teaching how for general data analysis, and for learning/teaching specific domains such as metagenomcis and differential gene expression analysis with RNA-Seq data. What audiences are the tutorials for? There are two distinct audiences for these materials. Self-paced individual learners. These tutorials provide everything you need to learn a topic, from explanations of concepts to detailed hands-on exercises. Instructors. They are also designed to be used by instructors in teaching/training settings. Slides, and detailed tutorials are provided. How is the content licensed? The content of this website is licensed under the Creative Commons Attribution 4.0 License. How can I advertise the training materials on my posters? We provide some QR codes and logos in the images folder. How do I use this material? Many topics include slide decks and if the topic you are interested in has slides then start there. These will introduce the topic and important concepts. How can I get help? If you have questions about this training material, you can reach us sending an email to bits@vib.be. For Instructors This material can also be used to teach the content in a group setting to students and researchers. Where do I start? Spend some time exploring the different tutorials and the different resources that are available. Become familiar with the structure of the tutorials and think about how you might use them in your teaching. How can I fix mistakes or expand an existing tutorial using the GitHub interface? Please submit an issue via github. Sustainability of the training-material and metadata This repository is hosted on GitHub using git as a DVCS. Therefore the community is hosting backups of this repository in a decentralised way. The repository is self-contained and contains all needed content and all metadata."},{"title":"","url":"/hall-of-fame.html","tags":[],"body":""},{"title":"","url":"/topics/data-management-plans/","tags":[],"body":""},{"title":"","url":"/topics/ngs/","tags":[],"body":""},{"title":"","url":"/topics/eln/","tags":[],"body":""},{"title":"","url":"/topics/basic-statistics/","tags":[],"body":""},{"title":"","url":"/topics/metagenomics/","tags":[],"body":""},{"title":"","url":"/topics/python-programming/","tags":[],"body":""},{"title":"","url":"/","tags":[],"body":""},{"title":"","url":"/topics/gimp-inkscape/","tags":[],"body":""},{"title":"","url":"/topics/basic-bioinformatics/","tags":[],"body":""},{"title":"","url":"/topics/functional_analysis/","tags":[],"body":""},{"title":"","url":"/topics/linux/","tags":[],"body":""},{"title":"","url":"/topics/qbase-plus/","tags":[],"body":""},{"title":"","url":"/topics/graphpad/","tags":[],"body":""},{"title":"","url":"/topics/protein-structure-analysis/","tags":[],"body":""},{"title":"","url":"/topics/chip-seq/","tags":[],"body":""},{"title":"","url":"/topics/git-introduction/","tags":[],"body":""},{"title":"","url":"/topics/ngs-intro/","tags":[],"body":""},{"title":"","url":"/topics/R/","tags":[],"body":""},{"title":"","url":"/badges/","tags":[],"body":""},{"title":"ChIP-Seq Analysis","url":"/topics/chip-seq/slides/introduction.html","tags":[],"body":"### ChiP-Seq Analysis ### [slides](http://data.bits.vib.be/pub/trainingen/NGSChIPSEQ/booklet/thomas-chollier_2020.pdf)"},{"title":"Introduction to Basic Bioinformatics Concepts, Databases and Tools","url":"/topics/basic-bioinformatics/slides/introduction.html","tags":[],"body":"### How to fill the slide decks? Please follow our [tutorial to learn how to fill the slides](/topics/contributing/tutorials/create-new-tutorial-slides/slides.html)"},{"title":"Protein Structure Analysis","url":"/topics/protein-structure-analysis/slides/introduction.html","tags":[],"body":"### Protein Structure Analysis ### [slides](https://material.bits.vib.be/courses/?https://raw.githubusercontent.com/vibbits/material-liascript/master/slides-PSA.md) - Sequences, structures and databases - Experimental methods (X-rays, electrons and NMR) - Finding and visualising structures from the Protein Data Bank - Comparing structures - Modelling mutations - Creating homology models"},{"title":"","url":"/search/search-results.html","tags":[],"body":""},{"title":"","url":"/search/search.html","tags":[],"body":""},{"title":"","url":"/search.html","tags":[],"body":""},{"title":"Introduction to Genome Assembly","url":"/topics/basic-bioinformatics/tutorials/general-introduction/slides.html","tags":[],"body":".enlarge120[ # ***De novo* Genome Assembly** ] #### With thanks to T Seemann, D Bulach, I Cooke and Simon Gladman --- .enlarge120[ # ***De novo* assembly** ] .pull-left[ **The process of reconstructing the original DNA sequence from the fragment reads alone.** * Instinctively like a jigsaw puzzle * Find reads which \"fit together\" (overlap) * Could be missing pieces (sequencing bias) * Some pieces will be dirty (sequencing errors) ] .pull-right[ ![](../../images/Humpty.jpg) ] --- # **Another View** ![](../../images/newspaper.png) --- # **Assembly: An Example** --- # **A small \"genome\"** ![](../../images/shakespear1.png) --- # **Shakespearomics** ![](../../images/shakespear2.png) --- # **Shakespearomics** ![](../../images/shakespear3.png) --- # **Shakespearomics** ![](../../images/shakespear4.png) --- # **So far, so good!** --- # **The Awful Truth** ![](../../images/notsimply.png) ## \"Genome assembly is impossible.\" - A/Prof. Mihai Pop --- .enlarge120[ # **Why is it so hard?** ] .pull-left[ * Millions of pieces * Much, much shorter than the genome * Lots of them look similar * Missing pieces * Some parts can't be sequenced easily * Dirty Pieces * Lots of errors in reads ] .pull-right[ ![](../../images/worlds_hardest.png) ] --- # **Assembly recipe** * Find all overlaps between reads * Hmm, sounds like a lot of work.. * Build a graph * A picture of the read connections * Simplify the graph * Sequencing errors will mess it up a lot * Traverse the graph * Trace a sensible path to produce a consensus --- ![](../../images/olc_pic.png) --- # **A more realistic graph** ![](../../images/real_graph.png) --- # .image-15[![](../../images/nofun.png)] **What ruins the graph?** * Read errors * Introduces false edges and nodes * Non haploid organisms * Heterozygosity causes lots of detours * Repeats * If they are longer than the read length * Causes nodes to be shared, locality confusion. --- # **Repeats** --- .enlarge120[ # **What is a repeat?** ] .pull-left[ #### ***A segment of DNA which occurs more than once in the genome sequence*** * Very common * Transposons (self replicating genes) * Satellites (repetitive adjacent patterns) * Gene duplications (paralogs) ] .pull-right[ ![](../../images/triplets.png) ] --- # **Effect on Assembly** ![](../../images/repeat_effect.png) --- .enlarge120[ # **The law of repeats** .image-15[![](../../images/repeatafterme.png)] ] ## **It is impossible to resolve repeats of length S unless you have reads longer than S** ## **It is impossible to resolve repeats of length S unless you have reads longer than S** --- # **Scaffolding** --- .enlarge120[ # **Beyond contigs** ] .pull-left[ Contig sizes are limited by: * the length of the repeats in your genome * Can't change this * the length (or \"span\") of the reads * Use long read technology * Use tricks with other technology ] --- .enlarge120[ # **Types of reads** ] .pull-left[.enlarge120[**Example fragment**]] .remark-code[.enlarge120[atcgtatgatcttgagattctctcttcccttatagctgctata]] .pull-left[.enlarge120[**\"Single-end\" read**]] .remark-code[.enlarge120[**atcgtatg**atcttgagattctctcttcccttatagctgctata]] sequence *one* end of the fragment .pull-left[.enlarge120[**\"Paired-end\" read**]] .remark-code[.enlarge120[**atcgtatg**atcttgagattctctcttcccttatag**ctgctata**]] sequence both ends of the same fragment **We can exploit this information!** --- .enlarge120[# **Scaffolding**] * **Paired end reads** * Known sequences at each end of fragment * Roughly known fragment length * **Most ends will occur in same contig** * **Some will occur in different contigs** * ***evidence that these contigs are linked*** --- .enlarge120[# **Contigs to Scaffolds**] ![](../../images/scaffolding.png) --- .enlarge120[# **Assessing assemblies**] * We desire * Total length similar to genome size * Fewer, larger contigs * Correct contigs * Metrics * No generally useful measure. (No real prior information) * Longest contigs, total base pairs in contigs, **N50**, ... --- .enlarge120[# **The \"N50\"**] .enlarge120[***The length of that contig from which 50% of the bases are in it and shorter contigs***] * Imagine we have 7 contigs with lengths: * 1, 1, 3, 5, 8, 12, 20 * Total * 1+1+3+5+8+12+20 = 50 * N50 is the \"halfway sum\" = 25 * 1+1+3+5+8+**12** = 30 (>25) so **N50 is 12** --- .enlarge120[# **2 levels of assembly**] * Draft assembly * Will contain a number of non-linked scaffolds with gaps of unknown sequence * Fairly easy to get to * Closed (finished) assembly * One sequence for each chromosome * Takes a **lot** more work * Small genomes are becoming easier with long read tech * Large genomes are the province of big consortia (e.g. Human Genome Consortium) --- .enlarge120[# **How do I do it?**] --- .enlarge120[ # **Example** * Culture your bacterium * Extract your genomic DNA * Send it to your sequencing centre for Illumina sequencing * 250bp paired end * Get back 2 files * .remark-code[MRSA_R1.fastq.gz] * .remark-code[MRSA_R2.fastq.gz] * ***Now what?*** ] --- .enlarge120[# **Assembly tools** * **Genome** * **Velvet, Velvet Optimizer, Spades,** Abyss, MIRA, Newbler, SGA, AllPaths, Ray, SOAPdenovo, ... * Meta-genome * Meta Velvet, SGA, custom scripts + above * Transcriptome * Trinity, Oases, Trans-abyss ***And many, many others...*** ] --- .enlarge120[ # **Assembly Exercise #1** * We will do a simple assembly using **Velvet** in **Galaxy** * We can do a number of different assemblies and compare some assembly metrics. ]"},{"title":"07 Wrap-up exercise","url":"/topics/python-programming/tutorials/7_wrap_up_exercise/tutorial.html","tags":[],"body":"As we introduced a lot of new concepts it is important that you practice them. hands_on Exercise 7 Write a program that does the following: Ask the user for a full DNA sequence Make sure the sequence contains only GACT Once you have a valid sequence For each DNA fragment the user enters: Check if it occurs in the full sequence Print out the sequence position if so Track each fragment Keep on asking the user for DNA fragments, stop if they just press return As a summary, print out all fragments with their position that you tracked Tips to complete this exercise in case you get stuck. Use while loops: you can use the condition to decide when to end the loop depending on the user input Track the sequence fragment and position data using a list Use string methods! To check the full DNA sequence, you can count how many times each GACT letter occurs, add up these counts, and compare this value to the total length of the full DNA sequence solution Solution # This variable will be used for the while loop validSequence = False # Keep on going as long as the DNA sequence is not valid while not validSequence: # Get a string from the user fullDnaSequence = input(\"Please enter your full DNA sequence:\") fullDnaSequence = fullDnaSequence.upper() # Count the GACT characters in the sequence gactCount = 0 for code in 'GACT': gactCount += fullDnaSequence.count(code) # Check if the number of GACT characters matches the full length of the sequence # and set validSequence to True if so - this will stop the while: loop if gactCount == len(fullDnaSequence): validSequence = True else: print(\"\\nInvalid sequence, only GACT allowed, try again!.\\n\") # Print some line breaks print(\"\\n\\n\") # Prime the list to track the DNA fragments and the variable for the while loop dnaFragmentInfo = [] dnaFragment = input(\"Please give a DNA fragment to check:\") while dnaFragment: # Check if present at all dnaFragmentCount = fullDnaSequence.count(dnaFragment) if dnaFragmentCount: currentDnaSequenceIndex = 0 for i in range(dnaFragmentCount): # Equivalent to currentDnaSequenceIndex = currentDnaSequenceIndex + fullDna... currentDnaSequenceIndex += fullDnaSequence[currentDnaSequenceIndex:].index(dnaFragment) print(\"\\n Fragment {} present at position {}.\\n\".format(dnaFragment,currentDnaSequenceIndex + 1)) dnaFragmentInfo.append((currentDnaSequenceIndex + 1,dnaFragment)) currentDnaSequenceIndex += 1 else: print(\"\\n Fragment {} not present!\\n\".format(dnaFragment)) dnaFragment = input(\"Please give a DNA fragment to check:\") # Print some line breaks print(\"\\n\\n\") # Print out the fragment information again, first sort it dnaFragmentInfo.sort() for (dnaFragmentPosition,dnaFragment) in dnaFragmentInfo: print(\"Found {} at position {}\".format(dnaFragment,dnaFragmentPosition))"},{"title":"02 Improving the quality of NGS data","url":"/topics/ngs-intro/tutorials/improving-quality-ngs-data/tutorial.html","tags":[],"body":"Group exercises You can solve most quality issues found by FASTQC e.g. trimming contaminating adapters, low quality bases at the end of your reads, filtering low quality reads… There’s is a lot of debate on whether it is required to do this. Reads that are contaminated with adapter sequences will not map but if these reads make up a large fraction of the total number of reads they might slow down the mapping a lot. While it is true that mappers can use noisy info (still containing adapters, low quality bases…), the mapping results will be negatively affected by this noise. Cleaning is in my opinion worthwhile especially when working with small reads and in case of extensive adapter contamination (almost always). Quality control in Galaxy Links: European Galaxy Raw Arabidopsis data in European Galaxy Groomed Arabidopsis data in European Galaxy Clean Arabidopsis data in European Galaxy Raw E. coli data in European Galaxy Groomed E. coli data in European Galaxy Filtered E. coli data in European Galaxy [Main Galaxy](http://usegalaxy.org Raw Arabidopsis data in main Galaxy Groomed Arabidopsis data in main Galaxy Galaxy is a bioinformatics server that contains many tools, data and analysis results. Before you can upload your data to Galaxy, you have to register or log in to Galaxy (see slides). Upload data to Galaxy If you want to work on your data in Galaxy, you have to first get the data into Galaxy. To accomplish this you can use the Upload file tool in the Get data section. Instead I shared the file on Galaxy so you can import it using this link. Make sure that you are logged on to Galaxy before you do this. When you click this link you are redirected to a web page where you can import the file: The history Data sets that are uploaded or created by running a tool appear in the history in the right Galaxy pane. To give a history a new name, click the history’s current name, type a new one and hit enter. Clicking the name of a data set unfolds a preview, a short description and tools to manipulate the data. Icons in the History Clicking the floppy (Download) icon will download the file to your computer To visualize a file in the middle pane, click the eye (View data) icon next to the name of the file. Colors of files in the HistoryData sets in the history have different colors representing different states. Grey: The job is placed in the waiting queue. You can check the status of queued jobs by refreshing the History pane. Yellow: The job is running. Green: When the job has been run the status will change from yellow to green if completed successfully. Red: When the job has been run the status will change from yellow to red if problems were encountered. Running Groomer in Galaxy If you select a tool in Galaxy it will automatically detect all data sets in your history that it can use as input. In the case shown below the tool does not recognize the fastq file in the history: The fact that the tool does not recognize the fastq file means that the fastq file is so messy that the tool can’t read it. Remember that there is a tool to clean messy fastq files: FASTQ Groomer Check the quality encoding in your fastq file (e.g. in FASTQC), and click the Execute button to start the tool: Grooming takes long (30 min when Galaxy traffic is low). You can choose to wait but if it takes too long you can click the Delete button in the History (see slides) to stop the tool. I have provided the groomed file: import it in Galaxy using [https://usegalaxy.eu/u/janick/h/groomeddata this link]. Using Trimmomatic in Galaxy To clean your data use the Trimmomatic tool in the Quality Control section of tools. Click the name of the tool to display its parameters in the middle pane. See this page for an overview of the Trimmomatic parameters. A bit more explanation: The input file with the reads: Galaxy will automatically suggest a file from your History that has the right format, in this case: a fastq file. If Galaxy doesn’t make a suggestion it means it cannot find any files in your History with the right format. The sequence of the adapter: provide a custom sequence. If you analyze your own data you know which adapter sequences were used. Since this is public data we don’t really know the name of the adapter. However, remember that FASTQC gives you a list of contaminating adapter sequences so you have the sequence of the adapter. Choose custom adapter sequence and paste the adapter sequence from FASTQC. You can only enter one sequence. Click Execute to run the tool. In the history you see a new item, colored in yellow as long as the tool is running. Regularly hit the Refresh button in the History to check if the tool has finished. Clipping should go fast, after a few minutes you should have the result. Running FASTQC in Galaxy Search for FASTQC in the tools pane and click the resulting FastQC link to open the parameter settings in the middle pane: FASTQC automatically recognizes all files it can use as an input. Select the file you want to use. The FASTQC implementation in Galaxy can take an optional file containing a list of contaminants. If you don’t specify one, FASTQC will look for standardly used Illumina adapters. In most cases you keep the default settings and click Execute. Quality control in GenePattern Genepattern is very similar to Galaxy. It’s as user-friendly as Galaxy, allows analysis of NGS data just like Galaxy… It provides easy access to hundreds of tools for different kinds of analyses (e.g. RNA-seq, microarray, proteomics and flow cytometry, sequence variation, copy number and network analysis) via a web browser. Links BITS Genepattern server fasta file containing Arabidopsis adapter sequence fasta file containing E. coli adapter sequence Overview of Trimmomatic parameters Consult the GenePattern tutorial for more info. Running Groomer in GenePattern The Broad Genepattern server does not contain the Groomer tool, but we have added the tool to our BITS Genepattern server. Search for the Groomer tool in GenePattern. Define the parameters: one of the parameters you need to define is Input format: the encoding of the fastq file you want to clean. The encoding is important because it determines the offset of the quality scores (ASCII offset 33 or ASCII offset 64). If you’re not sure you can check the encoding of your file in the FastQC report (take into account that FastQC sporadically makes the wrong guess). Run the Groomer tool. Running FastQC in GenePattern Search for the FASTQC tool Fill in the parameters Run the FASTQC tool You can open the resulting HTML report in your browser: Click the name of the output file at the bottom of the page Select Open Link Running Trimmomatic in GenePattern In GenePattern you can improve the quality of your NGS data using the Trimmomatic tool. Search for the Trimmomatic tool Fill in the parameters: See this page for an overview of the Trimmomatic parameters. Run Trimmomatic Removing adapters using command line tools See exercise on using cutadapt to trim adapter sequences"},{"title":"03 Mapping of NGS data","url":"/topics/ngs-intro/tutorials/mapping-ngs-data/tutorial.html","tags":[],"body":"After quality control, the next step is to align the reads to a reference sequence. The reference is in most cases the full genome sequence but sometimes, a library of EST sequences is used. In either way, aligning your reads to a reference sequence is called mapping. The most used mappers are BWA and Bowtie for DNA-Seq data and Tophat , STAR , STAR article , or HISAT2 for RNA-Seq data. Mappers differ in methodology, parameters, how fast and how accurate they are and whether they tolerate spliced alignments or not (relevant for RNA-Seq). Bowtie is faster than BWA, but looses some sensitivity (does not map an equal amount of reads to the correct position in the genome as BWA). BWA and Bowtie cannot align spliced reads while Tophat, STAR and HISAT2 can. At the moment STAR is the most popular RNASeq mapper and HISAT2 is being pushed over TopHat. Mapping in Galaxy Links: Mapped data for Arabidopsis in the European Galaxy paper on intron sizes in various organisms Sorted and indexed data for E.coli in the European Galaxy fasta file containing the E.coli K12 genome Bowtie manual Running RNA STAR in Galaxy STAR has a large number of parameters, we’ll give an overview of the most important ones: Single end or paired end data: the parameters you have to set will adjust accordingly RNASeq Fastq file: STAR automatically detects files it can use as input, select the file you want to map. Custom or built-in reference genome: many reference genomes are built-in in Galaxy just select the correct organism from the list of reference genomes. Length of the genomic sequence around annotated junctions: the default is 100 but the ideal value is read length-1. Count number of reads per gene: map reads and create a count table (table with counts of how many reads map to each gene). Would you like to set output parameters (formatting and filtering)?: in most cases yes because the default settings will most likely not be ideal for your data Would you like to set additional output parameters (formatting and filtering)?: in most cases yes because the default settings will most likely not be ideal for your data Would you like unmapped reads included in the SAM?: by default STAR does not save the unmapped reads, so if you want to analyze them (BLAST…) you need to change this setting. Maximum number of alignments to output a read’s alignment results, plus 1: default is 10 meaning that reads that map to more than 10 locations in the genome are excluded from the results. Multimappers are common when you map short reads. What to do with them is a complicated issue. You could use them to represent expression of whole classes/families of RNAs (e.g. transposons, gene families…). It can be useful to have two separate files: one for unique mappers and one for multimappers. Maximum number of mismatches to output an alignment, plus 1: maximum number of mismatches for a read (single-end) or a pair of reads (paired-end). Default is 10. The value you should choose is dependent on the read length. For short quality trimmed reads you typically allow 5% mismatches. Maximum ratio of mismatches to read length: how many mismatches you allow in the alignment (number is represented as a fraction of the total read length). Typically you choose 0.05 (= 5%) but this depends on the quality of the reads. In case of reads with many sequencing errors you need to increase the fraction of mismatches you allow. Other parameters (seed, alignment, limits and chimeric alignment): choose extended parameter list because the default settings will most likely not be ideal for your data Alignment parameters: Maximum intron size: maximum distance between reads from a pair when mapped to the genome. Two-pass mode: Use two pass mode to better map reads to unknown splice junctions: for the most accurate mapping, you should run STAR in 2-pass mode. It allows to detect more reads mapping to novel splice junctions. The basic idea is to run STAR with standard parameters, then collect the junctions detected in this first pass, and use them as annotated junctions for the second pass mapping. Parameters related to chimeric reads: chimeric reads occur when one read aligns to two distinct portions of the genome. In RNA-Seq chimeric reads may indicate the presence of chimeric genes. Many chimeric genes form through errors in DNA replication or DNA repair so that pieces of two different genes are combined. Chimeric genes can also occur when a retrotransposon accidentally copies the transcript of a gene and inserts it into the genome in a new location. Depending on where the new retrogene appears, it can produce a chimeric gene… Click Execute to start the mapping. STAR produces 3 result files: bam file containing all alignments (multimappers, reads that map to multiple locations, are printed at each location) tab file containing all detected splice junctions log file containing mapping statistics Running Bowtie for Illumina (= Bowtie1) in Galaxy This is an overview of the main parameters: Will you select a reference genome from your history or use a built-in index? Galaxy has many built-in genomes for Bowtie 1 but you can also use a fasta file from the history when the organism you work is not supported. Is this library mate-paired? single end or paired end ? FASTQ file Galaxy will automatically detect potential input files, select the file you want to use as input. Bowtie settings to use ask for full parameter list since the defaults are most likely not ideal for your data Trim n bases from high-quality (left) end of each read before alignment (-5) trim bases from high-quality (left) end of each read before alignment, default is 0. Trim n bases from low-quality (right) end of each read before alignment (-3) trim bases from low-quality (right) end of each read before alignment, default is 0. Alignment mode when the default -n option is used, bowtie determines which alignments are valid according to the following policy: alignments may have no more than n mismatches (where n is a number 0-3, set with Maximum number of mismatches permitted in the seed (-n)) in the first l bases (where l is a number 5 or greater, set with Seed length (-l)) on the high-quality (left) end of the read. The first l bases are called the “seed”. The sum of the Phred quality scores at all mismatched positions (not just in the seed) may not exceed e (set with Maximum permitted total of quality values at all mismatched read positions (-e)). In -v mode, alignments may have no more than v mismatches, where v may be a number from 0 through 3 set using the Maximum number of mismatches (-v) option. Quality values are ignored. Suppress all alignments for a read if more than n reportable alignments exist (-m) default is no limit. Bowtie is designed to be very fast for small -m but can become significantly slower for larger values of -m Download mapping results from Galaxy Click the name of the file containing the sorted alignments in the history. Click the download button at the bottom of the description. You should download two files: the bam file containing the mapping results and an index file (.bai) for fast access to the bam file. In Galaxy, indexing of bam files is done automatically. You need to download both files into the same folder. Mapping in GenePattern Links: Parameters of STAR paper on intron sizes in various organisms fasta file containing the E.coli K12 genome Bowtie manual Running STAR in GenePattern Search for the STAR aligner tool Fill in the parameters of STAR, you can find a detailed description of the parameters on this page Run STAR Store the resulting bam file in your uploads folder View the …align_summary.txt file in your browser to get an overview of the mapping results. Running Bowtie_1 indexer in GenePattern Search for the Bowtie_1 indexer tool. Here’s a detailed description of the main parameters: fasta files one or several fasta files containing the DNA sequence of the genome to index. index name a name for the bowtie 1 index files. Run the indexer, it will produce 6 files: .1.ebwt .2.ebwt .3.ebwt .4.ebwt .rev.1.ebwt .rev.2.ebwt For easy handling in GenePattern Bowtie_1.indexer puts all these files in a ZIP archive, which can be given as input to Bowtie_1.aligner. Store the resulting zip file in your uploads folder. Running Picard SortSam in GenePattern Some downstream tools cannot handle raw bam files since they are so large and chaotic, they need sorted and indexed bam files. Bam files can be sorted and indexed with samtools or Picard. Search for a tool that can sort sam or bam files Sort the file, keep the results in bam format. Sorting will add an index to the bam file (this is the .bai file that is generated) Download the sorted bam and bai files to your computer Mapping via command line tools On our Linux command line page you can find: an exercise on mapping with Bowtie via the command line. We will handle the mapping in detail in advanced NGS trainings, so we are not going into more detail now. Visualisation of mapping results in IGV bam-file for Arabidopsis thaliana from GenePattern bai-file for Arabidopsis thaliana from GenePattern bam-file for Arabidopsis thaliana from Galaxy bai-file for Arabidopsis thaliana from Galaxy bam-file for E. coli from GenePattern bai-file for E. coli from GenePattern bam-file for E. coli from Galaxy bai-file for E. coli from Galaxy IGV needs a sorted bam file and an index (.bai) file. Open IGV by clicking its icon on the Desktop. Be patient, it might take a few minutes for the program to start. If necessary change the genome in IGV from Human hg19 to the one you used in the mapping. Load the mapped reads via File in the top menu and Load from File. Select the .bam file to open. You don’t need to load the .bai file, it’s suffcient that it is present in the same folder as the .bam file. This loads the data into the center view. At this point, you can’t see the reads, you have to zoom in to view them. To zoom in on a gene type its accession number in the top toolbar and clicking Go: Zooming in can be done using the zoom bar in the top toolbar: The reads are represented by grey arrows, the arrow indicating the orietation of the mapping. Hovering your mouse over a read gives additional info on the mapping. The colored nucleotides indicate mismatches between the read and the reference. By default IGV calculates and displays the coverage track (red) for an alignment file. When IGV is zoomed to the alignment read visibility threshold (by default 30 KB), the coverage track displays the depth of the reads displayed at each locus as a gray bar chart. If a nucleotide differs from the reference sequence in greater than 20% of quality weighted reads, IGV colors the bar in proportion to the read count of each base (A, C, G, T). You can view count details by hovering the mouse over a coverage bar: Quality control of mapping results using Qualimap Qualimap is very similar to FastQC. It has an easy-to-use user interface and works on any platform: Windows, Mac, Linux. It’s installed on the BITS laptops: you can run it by clicking the icon on the desktop. You can do several analyses in Qualimap: we will focus on the BAM Quality Control and the RNA-Seq Quality Control. Starting a BAM QC analysis in Qualimap gtf-file for Arabidopsis thaliana from Ensembl Plants In the top menu, expand File and select New analysis and BAM QC A parameters form is opened. Select a .bam file as input file and leave all other parameters at their default setting: With the default settings the mapping is evaluated over the full reference sequence but you can limit the evaluation to certain regions by selecting the Analyze regions option and providing a gtf file containing the regions of interest. There are parameters for specific types of NGS experiments e.g. stranded libraries (Library strand specificity) and paired-end reads (Detect overlapping paired-end reads). A BAM Quality Control report is generated, very similar to the report that FastQC produces. Let’s take a look at some of the figures in the report: Coverage across reference: In the top figure you see the coverage (red line; average coverage in a window of a certain size) across the reference sequence. In the bottom figure you see the GC content (black line) across the reference. Coverage histograms: What percentage of the genome is not covered, covered at least once…. Starting a RNA-Seq QC analysis in Qualimap Specifically for RNA-Seq data you can do a RNA-Seq QC in Qualimap. In the top menu, expand File and select New analysis and RNA-seq QC A parameters form is opened. You need to provide an annotation file so Qualimap knows where the exons are located on the reference sequence. This annotation file is in gtf format and can be downloaded from the Ensembl or EnsemblGenomes ftp site. GTF stands for general transfer format, used for linking features (exons, introns, genes, transcripts, repeats, mutations…) to locations in the genome. Select the .gtf file as annotation file Select the .bam file as input file Leave all other parameters at their default setting A RNA-seq Quality Control report is generated. Coverage Profile (Total): The plot shows mean coverage profile of the transcripts. All transcripts with non-zero coverage are used to calculate this plot. Coverage Profile (Low): The plot shows mean coverage profile of 500 lowest-expressed genes. Coverage Profile (Total): The plot shows mean coverage profile of 500 highest-expressed genes. Coverage Histogram (0-50x): Coverage of transcripts from 0 to 50X. If certain genes have higher coverage level they are added to the last column (50X). Junction Analysis: This pie chart shows analysis of junction positions in spliced alignments. Known category represents percentage of alignments where both junction sides are known. Partly known represents alignments where only one junction side is known. All other alignments with junctions are marked as Novel. Solutions of Group Exercises"},{"title":"05 Introduction to RNA-Seq analysis","url":"/topics/ngs-intro/tutorials/rna-seq-analysis/tutorial.html","tags":[],"body":"Download the slides for this training session. The dataset comes from a 2014 publication on Human Airway Smooth Muscle Transcriptome Changes in Response to Asthma Medications. The goal of the analysis is to find DE genes (differentially expressed: genes with different expression levels in one group of samples compared to other groups of samples). Typically the groups of samples represent different treatments: one consisting of biological replicates that have received a control treatment, others consisting of replicates that received a specific biological treatment. In this experiment the data consists of four groups (treatment): The dex group: samples from 4 cell lines after treatment with the glucocorticoid dexamethasone (dex), used as astma medication The alb group: samples from the same cell lines after treatment with albuterol (alb), another astma medication The alb_dex group: samples from the same cell lines after treatment with both astma medications The untreated group: samples from the same untreated cell lines cultured in parallel. So all samples come from the same 4 cell lines (cells). # run_accession read_count samples cells treatment 1 SRR1039508 22935521 CL1_untreated CL1 untreated 2 SRR1039509 21155707 CL1_Dex CL1 Dex 3 SRR1039510 22852619 CL1_Alb CL1 Alb 4 SRR1039511 21938637 CL1_Alb_Dex CL1 Alb_Dex 5 SRR1039512 28136282 CL2_untreated CL2 untreated ... The data comes from a paired-end sequencing experiment so we have two files for each sample. For simplicity we will do the analysis on a single sample, SRR1039509, obtained from dexamethasone treated cell line 1. Quality checks Before you analyze the data, it is crucial to check the quality of the data. We use the standard tool for checking the quality of NGS data generated on the Illumina platform: FASTQC Correct interpretation of the FASTQC report is very important. If the quality of your data is good, you can proceed with the analysis. !! If the quality of your data is very bad, don’t immediately throw the data in the recycle bin but contact an expert and ask for his/her opinion. !! Double click the FASTQC icon on the Desktop and open the fastq file (it’s in the summer folder of your home folder). FASTQC consists of multiple modules each checking a specific aspect of the quality of the data. On the first page you can select the module you wish to view. The names of the modules are preceded by an icon that reflects the quality of the data. The icon indicates whether the results of the module seem normal (green tick), slightly abnormal (orange triangle) or very unusual (red cross). However, these evaluations must be interpreted in the context of what you expect from your library. A ‘normal’ sample as far as FastQC is concerned is random and diverse. Some experiments may be expected to produce libraries which are biased. You should treat the icons as pointers to where you should concentrate your attention on and understand why your library may not look normal. General information on the reads How long are the reads in this file ? 63 nucleotides Checking the quality scores of the reads Phred scores represent base call quality. The higher the score the more reliable the base call. Often the quality of reads degrades over the length of the read. Therefore, it is common practice to determine the average quality of the first, second, third,…nth base by plotting the distribution of the Phred scores on each position of the reads using box plots. Evaluate the quality scores per position Go to the Per base sequence quality module: The y-axis on the graph shows the Phred quality scores, the x-axis shows the position in the read. So again you see that the reads are 63 bases long. The average Phred score is depicted by the blue line, the median Phred score by the red line. The yellow boxes contain 50% of all Phred scores on a certain position. As expected the quality is steadily declining. The background of the graph divides the y-axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red; Phred score hg19.fa The > sign indicates that the output of the command (so the fasta file with the reference sequence) needs to be written to a file called hg19.fa. It will take a few minutes to generate the file. Installing TopHat Mapping RNA-Seq reads is done using the TopHat tool. So we need to install the TopHat tool. Go to the TopHat website and fetch the download link. Go to the TopHat website Right click the Linux download link Select Copy Link Location Download the file. Go to the terimnal Navigate to the /usr/bin/NGS/ folder Type **wget ** Press the Shift and Insert keys simultaneously to paste the url TopHat is downloaded as a .tar.gz file Decompress the file For decompressing a .tar.gz file you need the following command: tar -xzvf tophat-2.1.0.Linux_x86_64.tar.gz Remember to use tab autocompletion ! This creates a new folder: Go into the tophat folder and type: ./tophat If this opens the help of tophat, it means the software has been installed correctly. It does not mean that you can use the software now. Well you can but you will always have to type the commands from inside the tophat folder or provide the full path to the tophat folder. To avoid this we can create a symbolic link for tophat2. Create a symbolic link for tophat2 For creating the link you need the following command: ln -s /usr/bin/NGS/tophat-2.1.0.Linux_x86_64/tophat2 /usr/local/bin/tophat2 Remember to use tab autocompletion ! Now go to a different folder and type tophat2. If you see the help file, the link works. Installing samtools When you navigate to the tophat folder in /usr/bin/NGS/ you see that samtools is automatically installed when TopHat was installed: If you see the samtools help page when you type ./samtools_0.1.18 it means that samtools is indeed installed If you want to use samtools from anywhere in the file system you can create a soft link. Create a soft link for samtools. Create a link using the ln -s command: ln -s /usr/bin/NGS/tophat-2.1.0_Linux_x86_64/samtools-0.1.18/samtools /usr/local/bin/samtools Go up one directory in the file system and check if the command works. If you type samtools view (one of the possible samtools commands) you should see the manual of the command. Mapping the reads We are not going to do the actual mapping since it takes almost 25 minutes even with the chromosome22-limited datasets. If we were to map the reads we would use the following command: folder=/home/bits/NGS/RNASeq/ tophat2 --no-coverage-search ${folder}reference/hg19 ${folder}chr22_SRR1039509_1.fastq.gz ${folder}chr22_SRR1039509_2.fastq.gz –no-coverage-search: is related to how TopHat finds splice junctions. I’m not going to go into detail here but in the TopHat manual the developers of TopHat say: “We only suggest users use the –coverage-search option for short reads (= 5) means reads with a mate mapped on a different chromosome having a mapping quality greater than 5 Compare the number of forward and reverse reads in the paired-end experiment. the counts of forward and reverse reads are to be found on the lines ending with read1 and read2 respectively. As you see the number of forward reads exceeds the number of reverse reads by 55. How many reads were mapped as a pair in the paired-end experiment? 814320 reads were properly mapped as a pair, that’s 99,46% of the total number of reads. Tools like Qualimap, RSeqQC and Picard will give much more detailed information on the quality of the mapping. Unfortunately we do not have time to use them. Calculating a count table In order to compute differential expression between groups of samples, we need to convert mapping results to read counts for each gene in each sample. The counting can also be done in R using various packages but will be slower as compared to command-line tools. We will use the popular HTSeq-count tool to compute gene counts. Prepare the alignment file We need to sort the bam file since we have paired-end reads. HTSeq assumes the file is sorted so that reads belonging to the same pair are in adjacent lines. If you don’t sort the bam file by read name, HTSeq will think there are lot of reads with missing mates. In the samtools manual we can look up which command we need to do the sorting. Sort the reads in the .bam file by name As you can see in the manual the samtools sort command sorts .bam files: The input and output file are located in the /home/bits/NGS/RNASeq/ folder (or the /usr/bin/NGS/tophat-2.1.0.Linux_x86_64/tophat_out/ folder if you have done the mapping yourself). We are going to create a folder variable: folder=/home/bits/NGS/RNASeq/ samtools_0.1.18 sort -n ${folder}accepted_hits.bam ${folder}accepted_hits_sort Go to the folder where input and output file are stored and check if the sorted .bam file was generated: According to the HTSeq manual the input file for HTSeq contains the aligned reads in SAM format. In our case the mapping generated a .bam file. Fortunately samtools contains scripts to convert BAM format to SAM. In the samtools manual we can look up which command we need to do the transformation. Transform the .bam into a .sam file As you can see in the manual the samtools view command can transform any alignment format into standard SAM format: We are going to reuse the folder variable: samtools_0.1.18 view ${folder}accepted_hits_sort.bam > ${folder}accepted_hits.sam Go to the folder where input and output file are stored and check if the .sam file was generated. Obtaining a reference annotation file To calculate read counts we need a gtf file containing the annotation of all exons. You can obtain such files from genome annotation databases such as NCBI, Ensembl, and UCSC. The problem is that there are small differences between the formats of annotation files coming from different databases. These differences have implications for counting the reads. For instance, we used pre-built index files from the Bowtie website for the mapping. These files have UCSC format. So it seems obvious to use UCSC annotation files for the counting. However, HTSeq prefers Ensembl gtf files. As stated in the HTSeq documentation using gtf file generated by UCSC will result in very low counts. In the UCSC files, the gene_id incorrectly contains the same value as the transcript_id. Hence, if a read maps to an exon shared by several transcripts of the same gene, this will appear to htseq-count as an overlap between different genes since the different transcripts have different gene_ids. The read will be considered ambiguous and not counted. Therefore, the counts will be incorrect. As a solution, HTSeq recommends to use a gtf file from Ensembl. You can find Ensembl gtf files on the Ensembl ftp server. The version that we need is called grch37 (this corresponds to UCSC genome build hg19). So you can download the gtf file from this web site. Navigate to the /home/bits/NGS/RNASeq/reference/ folder: Decompress the gtf file. .gz files are decompressed by the gunzip command: gunzip Homo_sapiens.GRCh37.82.gtf.gz Use Tab autocompletion for the name of the file. Look at the first 10 lines of the gtf file. Use the head command to preview the file: head Homo_sapiens.GRCh37.82.gtf Use Tab autocompletion for the name of the file. As you can see the first column of the file contains chromosome numbers. Ensembl uses 1, 2, 3… as chromosome IDs. Look at the first 10 lines of the sam file. Use the head command to preview the file: head accepted_hits.sam Use Tab autocompletion for the name of the file. As you can see the third column of the file contains chromosome IDs but they have UCSC format: chr22 (remember that all reads come from chromosome 22 in our example). So we need to: Filter the annotation for chromosome 22 from the gtf file to limit processing time. Transform Ensembl chromosome IDs into UCSC format. First of all we’ll give the gtf file a simple name to simplify processing. Use the move command to rename the file to hg19_EnsGene.gtf Use the mv command to rename the file: mv Homo_sapiens.GRCh37.82.gtf ./hg19_EnsGene.gtf The ./ defines to move the file to the current folder (the folder that you are in when you type the command). So you will move the file to the same folder but under another name, which corresponds to just renaming it. Filter chromsome 22 annotations from the gtf file. Name the resulting file chr22_Ens.gtf Chromosome 22 annotations are lines starting with 22. Use the grep command to filter the file: grep \"^22\" hg19_EnsGene.gtf > chr22_Ens.gtf The ^ defines the start of a line. So ^22 means: search for lines that start with 22. Look at the first 10 lines of the filtered gtf file. Use the head command to preview the file: head chr22_Ens.gtf Use Tab autocompletion for the name of the file. Now, we still need to transform the Ensembl chromosome IDs into UCSC format, meaning that we simply need to add the prefix chr to each line in the filtered gtf file. You’ll need the sed command for this. Look at the sed documentation the sed documentation] before you try to do the substitution. To add the word chr to the start of each line, you essentially need to replace the start of a line by chr. Add the prefix to each line of the gtf file. Name the resulting file chr22_Ens_corr.gtf To do a replacement or substitution you need to use the s command, followed by what you want to replace and what to replace it with, each separated by a /. Remember from the filtering exercise that the start of a line is represented by ^. So use the following command to make the substitution: sed 's/^/chr/' chr22_Ens.gtf > chr22_Ens_corr.gtf Use Tab autocompletion for the name of the file. Look at the first 10 lines of the substituted gtf file. Use the head command to preview the file: head chr22_Ens_corr.gtf Use Tab autocompletion for the name of the file. Installing HTSeq HTSeq is a Python script. Python scripts can be installed using the pip install command. Remember you need administrator privileges for installing tools. Try to install HTSeq. What happens ? Use the following command to install the tool: pip install HTSeq As you can see this generates an error and the tool is not installed. Looking up the error in Google leads to this web page, where you can find the solution to the problem: some dependencies are missing. Install the missing dependencies and try again. What happens ? Use the following command to install the dependencies: apt-get install python-dev Then try to install HTseq again: pip install HTSeq As you can see this generates a new error and the tool is not installed. Looking up the error in Google leads to this web page, where you can find the solution to the problem: the C compiler is missing. Install the missing compiler and try again. What happens ? Use the following command to install the dependencies: apt-get install g++ Then try to install HTseq again: pip install HTSeq As you can see this does not generate an error. To check if the tool works type: htseq-count If this displays the help file, you know that the tool was correctly installed Calculating the count table HTSeq counts reads in different modes: HTSeq In the HTSeq manual we get an overview of the options we can use. How to define that we want to use the union mode ? The -m option allows to define the mode: How to define that this was not a strand-specific experiment ? The -s option allows to define if a strand-specific library was used: How to define the minimum alignment quality score for a read to be counted ? The -a option allows to define the minimum alignment score: We’ll go for the default mininimum alignment score of 10 (90% confidence). How to define we want to count based on exon annotation ? The -t option allows to define the feature to base the count on: For a .gtf file exon is the default. It means HTSeq will count the number of reads that align to each exon and then combine the counts for all exons of a transcript variant. How to define the feature we want to use as an ID after the counting ? The -i option allows to define the feature to use as ID. For a .gtf file gene_id is the default: it means that the output of HTSeq will be a list of gene_ids and for each gene_id you’ll see the number of reads that align to all its exons. Calculate the count table HTSeq was installed by pip install which automatically creates a link. So the HTSeq commands will work from anywhere in the file system. We will go to the folder that contains the input sam file: /home/bits/NGS/RNAseq/ and run the command from there or create a variable folder containing the path. In the HTSeq manual we get an overview of the options we can use. Default options should not be defined. folder=/home/bits/NGS/RNAseq/ htseq-count -m union -s no ${folder}accepted_hits.sam ${folder}reference/chr22_Ens_corr.gtf > ${folder}chr22_counts.txt View the first 10 lines of the resulting count table head chr22_counts.txt You nicely see the read counts for each gene…"},{"title":"04 Linux command line","url":"/topics/ngs-intro/tutorials/linux-command-line/tutorial.html","tags":[],"body":"Your first command Print the word hello to the screen echo hello Print the sentence “hello my friends to the screen (with open quotation and without end quotation) Remember you can use the up arrow to go back to previously typed commands echo \"hello my friends Now the terminal hangs. We typed an incorrect command and the terminal does not know what to do. What to type when the terminal hangs ? Ctrl-C If Ctrl-C fails, try hitting ESC. In most of the cases, this will do the trick. Open the manual of the echo command ? man echo The synopsis of this command is: echo [-n] [string ...] Things in between square brackets are optional, so it means that you can use echo without options and arguments. When the manual page is longer than the terminal, you can scroll down the page one line at a time by pressing the down arrow key, or one page at a time by pressing the spacebar. To exit the man page, press q (for quit). The manual explains that echo prints its argument by default to the screen and then puts the prompt on a new line. The way it does this is by appending a character called a newline (a special character that literally puts the text on a new line). Because echo is often used in programs to print out a sequence of strings not separated by newlines, there is an option to prevent the newline from being inserted. By reading the man page, find the command to print hello without a newline, and verify that it works as expected. Again remember to use the up arrow to go to previously typed commands. echo -n hello Open the manual of the sleep command, find how to make the terminal “sleep” for 5 seconds, and execute the command. man sleep According to the manual sleep has a required argument called number representing the number of seconds to sleep. sleep 5 Make the terminal sleep for 5000 seconds and rescue the terminal. sleep 5000 That’s more than an hour so use Ctrl-C to break off the command. Navigating the Linux file system Type the following command in the terminal: cd cd stands for change directory and is used for navigating the Linux file system Which directory are you in ? To view the name of the current working directory, type pwd pwd stands for print working directory. You see that using cd without arguments leads you to your home directory, on the BITS laptops this is /home/bits. Which directories are located in your home directory ? To view a list of the files and directories that are located in the current working directory, type ls ls stands for list and is used for listing all files and directories in the current directory. On the BITS laptops the home directory /home/bits contains a set of folders like Desktop, Documents, Downloads… List all files and directories in your home directory that start with the letter D ls D* D(star) means everything which name starts with a D A common pattern when using the command line is changing directories using cd and then immediately typing ls to view the contents of the directory. List the detailed content of your home directory ? ls -l the l in -l stands for long output. Among others, the detailed list shows a date and time indicating the last time a file was modified. The number before the date is the size of the file in bytes. List the content of the /usr/local/bin directory ? ls /usr/local/bin /usr/local/bin corresponds to a directory in the file system (/), with bin a subdirectory of local and local a subdirectory of usr. If you have to reuse a variable often then it can be helpful to create a name for a variable, especially when the variable is long. Suppose you want to work in a directory called Illumina_exp4_20042004_mapping_to_hg19_results. To avoid repeating this long name over and over you can create a variable for it, give it a short name and use that in your commands. Name the variable folder Use the following command: folder=Illumina_exp4_20042004_mapping_to_hg19_results To create a new directory use the mkdir (make directory) command. Create the folder using the newly created variable If you want to refer to a named variable in a command you have to preceed the name by a $ sign to indicate that what is following is a reference to a variable. So use the following command: mkdir ${folder} The curly braces delineate the start and end of the variable name. Check if the folder is created using the ls command. To remove a directory, use the rm (remove) command. You could use rmdir but this only works on empty folders. To remove a folder with the rm command you need to use the -r option. This stands for recursively which means it will remove the folder and its complete content. Remove the Illumina_exp4_20042004_mapping_to_hg19_results directory. Use the variable as an argument of the rm command: rm -r ${folder} Check if it’s removed using the ls command. Now navigate to the NGS folder which is located in the /home/bits/ folder. Navigate to this location. Since you want to navigate, you need to use the cd command. Since the NGS folder is located in the folder that you are currently in, you can simply give the name of the folder (NGS) as an argument: cd NGS If you want to move to a folder that’s located in another location of the file system, you have to give the full path to the folder. Go to the /usr/bin folder cd /usr/bin Go back to your home folder cd Manipulating files Even without a text editor, there are ways to create a file with text using the redirect operator > Create a file called test1.txt containing the text “Why do bioinformaticians work on the command line?” using echo echo \"Why do bioinformaticians work on the command line?\" > test1.txt The redirect operator > takes the text output of echo and redirects its contents to a file called test1.txt Check if it worked by viewing the content of the file on the screen cat test1.txt The name cat is short for “concatenate”. The command can be used to combine the contents of multiple files, but we use it here to dump the content of a single file to the screen. Cat is as a “quick-and-dirty” way to view the content of a file, less is a neater way. Add the line “Because they don’t want to scare you with huge amounts of data!” to the file and check if it worked To add lines of text to a file, use the append operator »: echo \"Because they don't want to scare you with huge amounts of data!\" >> test1.txt cat test1.txt The append operator » appends the text output of echo to the file test1.txt Create an empty file called test2.txt and check if it exists To create an empty file, use the touch command: touch test2.txt ls List the names of all text files in your current directory ls *.txt Here *.txt automatically expands to all filenames that match the pattern “any string followed by .txt”. Rename the file test2.txt to test_partII.txt using mv and check if it worked To rename a file use the mv command, short for move: mv test2.txt test_partII.txt ls *.txt Copy the file test_partII.txt to test2.txt and check if it worked To copy a file use the cp command, short for copy: cp test_partII.txt test2.txt ls *.txt You don’t have to type out test_partII.txt, instead you can type something like test_-Tab thereby making use of tab completion. Tab completion involves automatically completing a word if there’s only one valid match on the system. For example, if the only file starting with the letters “test_” is test_partII.txt, test_-Tab refers to test_partII.txt Especially with longer names, tab completion can save a huge amount of typing. Remove the file test_partII.txt and check if it worked To remove a file use the rm command, short for remove: rm test_partII.txt ls *.txt Download the file called exams.txt, containing the results of the spelling and maths exams of all 10-year olds of a school, into your home folder. Use wget to download the file from http://data.bits.vib.be/pub/trainingen/NGSIntro/exams.txt Download the file. wget http://data.bits.vib.be/pub/trainingen/NGSIntro/exams.txt Show the first 10 lines of the file. head exams.txt Two complementary commands for inspecting files are head and tail, which allow to view the beginning (head) and end (tail) of a file. The head command shows the first 10 lines of a file. Show the last 10 lines of the file. Similarly, tail shows the last 10 lines of a file. tail exams.txt Open the manual of head to check out the options of head. Learn how to look at the first n lines of the file. Save the first 30 lines of exams.txt in a file called test.txt head -n 30 exams.txt > test.txt Look at test.txt using the less command less test.txt There are many commands to look at the full content of a file. The oldest of these programs is called more, and the more recent and powerful variant is called less. Less lets you navigate through the file in several ways, such as moving one line up or down with the arrow keys, pressing space bar to move a page down… Perhaps the most powerful aspect of less is the forward slash key /, which lets you search through the file from beginning to end. Search for Jasper in test.txt The way to do this in less is to type /Jasper The last three essential less commands are G to move to the end of the file and 1G to move back to the beginning. To quit less, type q (for quit). Look at the last 10 lines of the first 20 lines of exams.txt head -n 20 exams.txt | tail The command runs head -n 20 exams.txt and then pipes the result through tail using the pipe symbol | The reason the pipe works is that the tail command, in addition to taking a filename as an argument, can take input from “standard in”, which in this case is the output of the command before the pipe. The tail program takes this input and processes it the same way it processes a file. Running tools Bioinformatics tools are just commands on the commands line. You use them in exactly the same way as all the commands we have run up to now, by defining options and arguments. A list of options and arguments can be found in the help file. Installing and running sl We have seen ls the list command and use it frequently to view the contents of a folder but because of miss-typing sometimes you would result in sl, how about getting a little fun in the terminal and not command not found. This is a general linux command, you can install it from a repository. Install sl For installing you need superuser privileges ! sudo apt-get install sl Find out in the manual what sl stands for man sl You can find the solution in the description section of the manual. run the command sl :o) Try out some of the options !! Running blastp In the folder /home/bits/Linux/ you find a file called [http://data.bits.vib.be/pub/trainingen/NGSIntro/sprot.fasta sprot.fasta] containing a set of protein sequences. We will use this file as a database for blast. The query sequence is the following: MLLFAPCGCNNLIVEIGQRCCRFSCKNTPCPMVHNITAKVTNRTKYPKHLKEVYDVLGGSAAWE Create a fasta file containing the query sequence using echo called seq.fasta echo \">query seq\" > seq.fasta cat seq.fasta echo MLLFAPCGCNNLIVEIGQRCCRFSCKNTPCPMVHNITAKVTNRTKYPKHLKEVYDVLGGSAAWE >> seq.fasta cat seq.fasta Blast can be done via the [https://blast.ncbi.nlm.nih.gov/Blast.cgi blast website], but you can also download the blast tool and run it locally (on your computer) via the command line. For instance if you want to blast against you own database of sequences, you have to do it locally. Blast has been installed on the bits laptops. First you have transform your own database (the sprot.fasta file in our case) into a database that can be searched by blast using the makeblastdb command. Look at the help file of makeblastdb and find the options to define the input fasta file and the database type makeblastdb -help You have to define the input fasta file using the -in option and the type of sequences using the -dbtype option Create the blast database makeblastdb -in sprot.fasta -dbtype prot Now you can perform a blastp search using the blastp command. Write the results to a tabular text file with comments called output.txt Look at the help file of blastp and find the options to define input, database, output and output format blastp -help You need the -query, the -db, the -out and the -outfmt option Perform the blast and open the results with less blastp -query seq.fasta -db sprot.fasta -out output.txt -outfmt 7 less output.txt Running cutadapt In this exercise we’ll do some real NGS analysis on the SRR074262.fastq file that is stored in folder /home/bits/NGS/Intro. Go to this folder and look at the 10 first lines of the file. cd /home/bits/NGS/Intro head SRR0474262.fastq This data sets contain a high number of adapter sequences. These are reads that consist solely or partly of adapter sequence. You have to remove this adapter contamination using command line tools like [https://code.google.com/p/cutadapt/ cutadapt]. This tool is installed on the bits laptops. It is not a regular bash command (it’s a python program) so it doesn’t have a manual but it does have a help file. Check the help file of cutadapt for the option to define the adapter sequence and trim at the 3’ends of the reads. To open the cutadapt help file type: cutadapt -h The -a option trims adapter sequences at the 3’ end of the reads. At the top of the help file you see that the standard usage of the command is: cutadapt -a ADAPTER -o output.fastq input.fastq The sequence of the adapter is GATCGGAAGAGCTCGTATGCCGTCTTCTGCTTGAAA Trim the adapter sequence and store the trimmed sequences in a file called SRR074262trim.fastq Go to the folder where the input file is located and type: cutadapt -a GATCGGAAGAGCTCGTATGCCGTCTTCTGCTTGAAA -o SRR074262trim.fastq SRR074262.fastq Look at the first lines of the trimmed file Go to the folder where the input file is located and type: head SRR0474262trim.fastq Running Picard The trimmed fastq file is subsequently mapped resulting in a bam file that you can download from http://data.bits.vib.be/pub/trainingen/NGSIntro/1271011_reads.pair.1.list.accepted_hits.bam Download the file via the command line wget http://data.bits.vib.be/pub/trainingen/NGSIntro/1271011_reads.pair.1.list.accepted_hits.bam Rename the file SRR074262.bam Remember to use tab autocompletion ! mv 1271011_reads.pair.1.list.accepted_hits.bam SRR074262.bam This is a raw unsorted bam file, if we want to visualize the mapping results in IGV, we need to sort and index the file. We can do the sorting using one of [http://broadinstitute.github.io/picard/ the Picard tools], called SortSam. Picard can be downloaded from https://github.com/broadinstitute/picard/releases/download/2.8.2/picard-2.8.2.jar Download the file Remember to use tab autocompletion ! wget https://github.com/broadinstitute/picard/releases/download/2.8.2/picard-2.8.2.jar ll For the tools to run properly, you must have Java 1.8 installed. To check your java version run the following command: java -version Running Java tools from the command line requires a special syntax: you have to start the command with java and then the name of the java tool and its options and arguments. Java jar-files are archives of multiple java files (similar to tar archives of multiple regular files). They require an even more elaborate syntax. You have to start the command with java -jar and then the name of the jar file and its options and arguments. As you can see the picard tools come as a jar-file. Test the installation by opening the help file java -jar picard-2.8.2.jar -h Bam files are enormous files that are hard to search through. The order of the reads in a bam file is the same as in the original fastq file. However, if you want to visualize the mapping results or if you want to calculate mapping statistics it’s much more efficient to sort the reads according to genomic location. This can be achieved with the SortSam tool. Look in [https://broadinstitute.github.io/picard/command-line-overview.html the picard documentation] for the SortSam tool. Sort the bam file to SRR074262sorted.bam Remember to use tab autocompletion ! java -jar picard-2.8.2.jar SortSam \\ I=SRR074262.bam \\ O=SRR074262sorted.bam \\ SORT_ORDER=coordinate Bam files contain duplicate reads unless you removed them during the quality control step. MarkDuplicates locates and tags duplicate reads in a bam or sam file. Duplicate reads originate from the same fragment and were typically introduced during library construction using PCR. Duplicate reads can also result from a single cluster on the flow cell, incorrectly detected as multiple clusters by the optical sensor of the sequencing instrument. MarkDuplicates compares sequences of reads and detects duplicates. The tool’s output is a new SAM or BAM file, in which duplicates have been identified in the SAM flags field. If needed, duplicates can be removed using the REMOVE_DUPLICATE and REMOVE_SEQUENCING_DUPLICATES options. (See [https://broadinstitute.github.io/picard/command-line-overview.html#MarkDuplicates the Picard documentation] for more details). Remove duplicates from the sorted bam file Remember to use tab autocompletion ! java -jar picard.jar MarkDuplicates \\ I=SRR074262sorted.bam \\ O=SRR074262sortednodup.bam \\ M=marked_dup_metrics.txt \\ REMOVE_DUPLICATES=true For visualization and easy access you can build an index to the bam file using BuildBamIndex. Look in [https://broadinstitute.github.io/picard/command-line-overview.html the picard documentation] for the BuildBam Index tool. Build the bai file for SRR074262sortednodup.bam Remember to use tab autocompletion ! java -jar picard-2.8.2.jar BuildBamIndex \\ I=SRR074262sortednodup.bam Check if the files were generated. File compression Compress the SRR074262.bam file to .gz format Remember to use tab autocompletion ! gzip SRR074262.bam ll and unzip it again Remember to use tab autocompletion ! gunzip SRR074262.bam.gz ll Writing scripts Writing and executing bash scripts We are going to make additions to the bash script you find below: #this program pretends to hack sites !Define a variable str equal to \" 0 1 23 45 6 789\" clear !Print to screen: \"hacking www.bits.vib.be\" !Do nothing for 2 seconds !Print to screen: \"Server hacking module is loading\" !Do nothing for 2 seconds !Print to screen: \"Hack module is starting in 2 seconds\" !Do nothing for 1 second !Print to screen: \"1 second\" !Do nothing for 1 second ping -c 3 www.bits.vib.be !Do nothing for 1 second netstat !Do nothing for 1 second for i in {1..1000} do number1=$RANDOM let \"number1 %= ${#str}\" number2=$RANDOM let \"number2 %=4\" !Print to screen without newlines and with backslash escapes: \"\\033[1m${str:number1:1}\\033[0m\" done !Print to screen: \"453572345763425834756376534\" !Do nothing for 3 seconds !Print to screen: \"www.bits.vib.be succesfully hacked!\" !Print to screen: \"PASSWORD ACCEPTED: token is 453572345763425834756376534\" Open gedit and paste the code. Replace all lines that start with ! by the appropriate command #this program pretends to hack sites str=\" 0 1 23 45 6 789\" clear echo \"hacking www.bits.vib.be\" sleep 2 echo \"Server hacking module is loading\" sleep 2 echo \"Hack module is starting in 2 seconds\" sleep 1 echo \"1 second\" sleep 1 ping -c 3 www.bits.vib.be sleep 2 netstat sleep 1 for i in {1..1000} do number1=$RANDOM let \"number1 %= ${#str}\" number2=$RANDOM let \"number2 %=4\" echo -n -e \"\\033[1m${str:number1:1}\\033[0m\" done echo \"453572345763425834756376534\" sleep 3 echo \"www.bits.vib.be succesfully hacked!\" echo \"PASSWORD ACCEPTED: token is 453572345763425834756376534\" ```. > Add a shebang line to the top of the script #!/usr/bin/env bash #this program pretends to hack sites str=” 0 1 23 45 6 789” clear … Save the script as HackIt.sh > If necessary make executable chmod 755 HackIt.sh > Run the script bash HackIt.sh What if you want to \"hack\" another website ? The easiest way to do allow for this is to enable to give the url as an argument of the bash command so that's what we'll do. Reopen the file in gedit > Replace www.bits.vib.be by $1 #!/usr/bin/env bash #this program pretends to hack sites str=” 0 1 23 45 6 789” clear echo “hacking $1” sleep 2 echo “Server hacking module is loading” sleep 2 echo “Hack module is starting in 2 seconds” sleep 1 echo “1 second” sleep 1 ping -c 3 $1 sleep 2 netstat sleep 1 for i in {1..1000} do number1=$RANDOM let “number1 %= ${#str}” number2=$RANDOM let “number2 %=4” echo -n -e “\\033[1m${str:number1:1}\\033[0m” done echo “453572345763425834756376534” sleep 3 echo “$1 succesfully hacked!” echo “PASSWORD ACCEPTED: token is 453572345763425834756376534” > Save and run the script again now giving www.kuleuven.be as an argument bash HackIt.sh www.kuleuven.be $1 refers to the first argument of the command. If you have two arguments you use $1 and $2 to represent them. #### Writing and executing Perl scripts We are going to create and the perl script you find below: #This program predicts if a sequence is protein, nucleic acid or rubbish $seq = $ARGV[0]; if ($seq =~ /[JO]/) { print “is not a sequence, first illegal character is $&\\n”; } elsif ($seq =~ /[EFILPQZ]/) { print “is protein\\n”; } else { print “is nucleic acid\\n”; } Open gedit and paste the code. > Add a shebang line to the top of the script #!/usr/bin/env perl #This program predicts if a sequence is protein, nucleic acid or rubbish $seq = $ARGV[0]; if ($seq =~ /[JO]/) { … Save the script as SeqIt.pl > If necessary make executable chmod 755 SeqIt.pl > Run the script using your first name in capitals as an argument perl SeqIt.pl JANICK #### Writing and executing Python scripts We are going to make additions to the python script you find below: #This program counts the number of amino acids in a protein sequence !Define variable mySequence equal to “SFTMHGTPVVNQVKVLTESNRISHHKILAIVGTAESNSEHPLGTAITKYCKQELDTETLGTCIDFQVVPGCGI” !Create a set myUniqueAminoAcids out of mySequence for aaCode in myUniqueAminoAcids: !Print to screen, use format to fill in the values: “Amino acid {} occurs {} times.” Open gedit and paste the code. > Replace all lines that start with ! by the appropriate command #This program counts the number of amino acids in a protein sequence mySequence = “SFTMHGTPVVNQVKVLTESNRISHHKILAIVGTAESNSEHPLGTAITKYCKQELDTETLGTCIDFQVVPGCGI” myUniqueAminoAcids = set(mySequence) for aaCode in myUniqueAminoAcids: print(“Amino acid {} occurs {} times.”.format(aaCode,mySequence.count(aaCode))) > Add a shebang line to the top of the script #!/usr/bin/env python #This program counts the number of amino acids in a protein sequence mySequence = “SFTMHGTPVVNQVKVLTESNRISHHKILAIVGTAESNSEHPLGTAITKYCKQELDTETLGTCIDFQVVPGCGI” myUniqueAminoAcids = set(mySequence) … Save the script as CountIt.py > If necessary make executable chmod 755 CountIt.py > Run the script python CountIt.py What if you want to \"count\" another protein ? The easiest way to do allow for this is to enable to give the sequence as an argument of the python command so that's what we'll do. Reopen the file in gedit > Adjust the code to read the first argument of the python command using the sys library !#/usr/bin/env python #This program counts the number of amino acids in a protein sequence import sys mySequence = sys.argv[1] myUniqueAminoAcids = set(mySequence) for aaCode in myUniqueAminoAcids: print(“Amino acid {} occurs {} times.”.format(aaCode,mySequence.count(aaCode))) > Save and run the script again now giving QWEERTIPSDFFFGHKKKKLLLLLLLLLLLLLL as an argument python CountIt.py QWEERTIPSDFFFGHKKKKLLLLLLLLLLLLLL sys.argv[1] refers to the first argument of the command. If you have two arguments you use sys.argv[1] and sys.argv[2] to represent them. #### Installing and using Python tools Installing Python-based tools is not done with apt-get, instead the comand pip is used. If pip is not yet installed, the terminal will show an error message saying that pip is currently not installed. You can install pip using apt-get. As an example we will install Biopython, a Python library for bioinformatics. See [http://biopython.org/wiki/Download the documentation] for more details. > Install biopython You need superuser privileges for this sudo pip install biopython We will write a small python script to check if Biopython was successfully installed. In the folder /home/bits/Linux/ you find a file called [http://data.bits.vib.be/pub/trainingen/NGSIntro/sprot.fasta sprot.fasta] containing a set of protein sequences that we will use as input. Move to the folder containing the file. We will use SeqIO module of Biopython to parse the fasta file with the protein sequences. Check out [http://biopython.org/wiki/SeqIO the tutorial of the module]. !Import the SeqIO module of the Bio library !For every record in the sprot.fasta file do: !Print the id of the seq_record to the screen !Print the length of the sequence to the screen Open gedit and paste the code. > Replace all lines that start with ! by the appropriate command from Bio import SeqIO for seq_record in SeqIO.parse(“sprot.fasta”,”fasta”): print(seq_record.id) print(len(seq_record)) > Add a shebang line to the top of the script #!/usr/bin/env python from Bio import SeqIO for seq_record in SeqIO.parse(“sprot.fasta”,”fasta”): … Save the script as ParseIt.py in the folder that contains the input file. > If necessary make executable chmod 755 ParseIt.py > Run the script python ParseIt.py ### Compressing and decompressing files Some files or tools come in **.zip** format, how to decompress them ? In the **/usr/bin/tools** folder you can find the zipped version of the FastQC tool. To unzip it, you have to use the **unzip** command. The **/usr/bin/** folder belongs to the root user, not to the bits user. Therefore only root is allowed to do manipulations in this folder. Switch to root using the **su** command or type **sudo** in front of your commands. The system will ask for the password: bitstraining on the BITS laptops. > Decompress the FastQC tool with unzip. First look at the unzip manual to get an idea about the working of the command. ```man unzip``` To unzip the file you can use the simple command: ```unzip name_of_the_zip_file```. Remember to use tab autocompletion. This will generate a folder called FastQC in /usr/bin/tools. After decompression use **ls** and **cd** to take a look at the content of the newly created **FastQC** folder. You will see the fastqc command in this folder. > Make sure that you can read, write and execute the fastqc command and that other people can read and execute it. To see the current permissions of the command: ```ls -l``` The command that allows you to change the access permissions of files and directories is **chmod** (change mode). chmod has two mandatory arguments: - A three digit number representing the access permissions you want to set. Each digit refers to a different audience: - first digit refers to the owner of the file - second digit refers to the group the owner belongs to - third digit refers to all others The numbers themselves represent the permissions: - 7 full access: read, write and execute - 6 read and write - 5 read and execute - 4 read only - 3 write and execute - 2 write only - 1 execute only - 0 no access - The name of the file for which you want to change the access permissions As you can see **root** is the owner of the file. This is why you need to log on as superuser (= root) to be able to change root's files. ### Sorting files We want to sort the file exams.txt from highest to lowest score on maths. > Sort the file based on score on maths. Write results to a file called examssort1.txt You have to **sort** the lines in the file according to the maths score. So you want to sort the file based on the numbers in the second column: it means that you cannot use the default sort command (this will sort the lines based on the content of the first column) but you have to use an option that allows you to specify the column you wish to sort on. When you look in the manual you see that you can use the -k option for this: sort -k2 exams.txt This will sort the file according to the values in the second column, but it will overwrite the original file. To save the sorted list in a new file, examssort1.txt, use the **redirect operator: >** sort -k2 exams.txt > examssort1.txt > Use the head command to look at the sorted file. head examssort1.txt You can see that the sorting was not done correctly: it was done alphabetically, treating the numbers in the second column as characters, instead of numbers. This means that we are still missing an option that allows for numerical sorting. > Sort the file numerically based on score on maths. sort -k2 -n exams.txt > examssort1.txt head examssort1.txt This looks a lot better, but we still have to reverse the order since we want the scores from high to low. > Sort the file numerically from highest to lowest score on maths. For this we need to add a third option to the **sort** command. When you look in the manual you see that you can use the -r option for this: sort -k2 -n -r exams.txt > examssort1.txt head examssort1.txt > Show the results of the 10 students with the highest scores on the maths exam using a single line of commands. This means that you have to combine the **head** command and the **sort** command from the previous exercise into one single command. Remember that you can combine commands by writing them in the order they have to be performed, so in our case first **sort** then **head**, separated by the **pipe operator: |** sort -k2 -n -r exams.txt | head > Show only the names of the 10 students with the highest scores on the maths exam using a single line of commands. To leave out gender and scores you have to use the **cut** command. To specify which columns to cut you can use the -f option. Please note that the -f option specifies the column(s) that you want to retain ! As an argument you have to specify the name of the file you want to cut. In the manual you can see that TAB is the default delimiter for the cut command. So if you have a tab-delimited text file, as in our case, you do not need to specify the delimiter. Only if you use another delimiter you need to specify it. sort -k2 -n -r exams.txt | head | cut -f3 **The case of chromosomes and natural sorting.** 'sort' will sort chromosomes as text; adding few more parameters allows to get the sort you need. > Write a list of human chromosomes (values: 22 to 1 X Y MT) to the screen. Use {end..begin} to define a numerical range. Remember that you can use **echo** to print text to the screen, so to generate text. Try echo {22..1} X Y MT and see what happens... You don't want to numbers next to each other in one row, you want them in a column underneath each other. This means you want to replace the blanks by end-of-lines. > Replace blanks by end-of-lines. Use the sed command for this. Look up the command for replacing text in the slides. Blanks are represented by **\\ ** (back slash followed by a blank) and end-of-lines are represented by **\\n** (back slash followed by n). To replace all blanks by an end-of-line you need to add the **g** option (see [http://sed.sourceforge.net/sed1line.txt sed tutorial] for more info). So sed “s/\\ /\\n/g” should do the replacement. Of course you need to combine the two commands using the output of echo as input in sed. Look in the slides or the cheat sheet how to do this. However, you do not want to print the text to the screen you want to print the text to a file. Look in the slides or the cheat sheet how to do this and try to combine the three parts of the command. > Write chromosomes as a column to a file called chroms.txt The correct solution is: echo {22..1} X Y MT | sed “s/\\ /\\n/g” > chroms.txt The s in the sed argument refers to substitution: you want to substitute blanks by end-of-lines, it is followed by the character you want to replace (a blank or \"\\ \"), then the character you want to replace it with (an end-of-line or \"\\n\"), then you add g to use sed recursively, in other words to do the substitution more than once so each time a blank is encountered. It prints the chromosome numbers as a column to the file chroms.txt > Look at the file using the less command. less chroms.txt Remember to use q to leave a less page. > Sort the chromosome file by using a simple sort. Write results to chromssort.txt sort chroms.txt > chromssort.txt head chromssort.txt Not good! This is a tricky problem that always comes up when you are working with chromosome numbers e.g. when sorting bam/sam files, annotation files, vcf files... > Modify the sort command so that the sorting of the chromosomes is done in the correct way. Most people solve it by specifying that you want sort to do natural sorting using the -V option: sort -V chroms.txt > chromssort.txt head chromssort.txt Nice ! Now try with chr in front. > Create a file with values chr22 to chr1 chrX chrY chrMT into one column called chroms2.txt in one single command echo chr{22..1} chrX chrY chrMT | sed “s/\\ /\\n/g” > chroms2.txt head chroms2.txt > Sort the file into a new file called chromssort2.txt sort -V chroms2.txt > chromssort2.txthead chroms2.txt ### Getting files from the internet To download data via a link on the internet you can use the **wget** command. For NGS analysis you often need to download genomic sequence data from the internet. As an example we are going to download the E.coli genome sequence from the iGenomes website: ftp://igenome:G3nom3s4u@ussd-ftp.illumina.com/Escherichia_coli_K_12_MG1655/NCBI/2001-10-15/Escherichia_coli_K_12_MG1655_NCBI_2001-10-15.tar.gz Download this file into the folder NGS/ChIPSeq/ in your home directory. > Download the data into this folder. Go to this folder and use the wget command to download the data: cd /home/bits/NGS?ChIPSeq/ wget ftp://igenome:G3nom3s4u@ussd-ftp.illumina.com/Escherichia_coli_K_12_MG1655/NCBI/2001-10-15/Escherichia_coli_K_12_MG1655_NCBI_2001-10-15.tar.gz ll In the same way you can download NGS data from the internet. We are not going to actually do this because NGS data sets are enormous and can take hours to download. Interrupting the download is done with + C > Decompress the file. tar -xzvf Escherichia_coli_K_12_MG1655_NCBI_2001-10-15.tar.gz ll This creates a new folder called Escherichia_coli_K_12_MG1655. Go into this folder and look at the whole genome fasta sequence > Look at the fasta sequence. Use **cd** to navigate the folders and **head** to look at the file cd Escherichia_coli_K_12_MG1655 ll cd NCBI ll cd 2001-10-15 ll cd Sequence ll cd WholeGenomeFasta ll head genome.fa ### Installing tools The FastQC tool was installed by unzipping it. Most tools can be installed using the **make** command. There are many ways to install software on Linux: - via the software manager, an application with a very easy user friendly interface - via the **apt-get** command - software packages written in Python are installed via the **pip install** command These methods handle the installation and removal of software on Linux distribution in a simplified way. They fetch the software from software repositories on the internet. However, these repositories do not always contain the most up-to-date version of software packages, especially not for niche software like bioinformatics tools. So to be on the safe side, it is recommended that you download the latest version of a tool from its website (using wget) and use **make** to install it. In that way, you have full control over the version of the tool that you are installing. This is not true for pip. Pip does the difficult steps in the installation for you and accesses an up-to-date package repository, so Python programs can safely be installed using pip. Download and install all packages in the **tools** folder of the **/usr/bin/** folder. This is a folder owned by root so it is a good idea to switch to superuser again. #### Installing TopHat In the Introduction training we use RNA-Seq reads. Mapping RNA-Seq reads is done using the TopHat tool. So we need to install the [http://ccb.jhu.edu/software/tophat/tutorial.shtml TopHat tool]. We are going to do this in the /usr/bin/NGS/ folder so we need to be superuser for this. > Go to the TopHat website and fetch the download link. - Go to the [http://ccb.jhu.edu/software/tophat/tutorial.shtml TopHat website] - Right click the Linux download link - Select **Copy Link Location** > Download the file into the /usr/bin/NGS/ folder. - Go to the terimnal - Navigate to the /usr/bin/NGS/ folder - Type **wget ** - Press the Shift and Insert keys simultaneously to paste the url TopHat is downloaded as a .tar.gz file > Decompress the file For decompressing a .tar.gz file you need the following command: tar -xzvf tophat-2.1.1.Linux_x86_64.tar.gz Remember to use tab autocompletion ! This creates a new folder called tophat-2.1... Go into the tophat folder and type: ./tophat If this opens the help of tophat, it means the software has been installed correctly. It does not mean that you can use the software now. Well you can but you will always have to type the commands from inside the tophat folder like we do here or provide the full path to the tophat folder. The dot slash (./) in front of the command means use the tophat **that is located in this folder**. It tells the command line where it can find the script (./ = the current directory = /usr/bin/tools/tophat-2.1.1.Linux_x86_64/).To avoid this we can create a symbolic link for tophat2 (see later). #### Installing samtools When you navigate to the **tophat** folder in /usr/bin/NGS/ you see that samtools is automatically installed when TopHat was installed: If you see the samtools help page when you type ./samtools_0.1.18 it means that samtools is indeed installed [http://wiki.bits.vib.be/index.php/Introduction_to_ChIP-Seq_analysis Installing tools for the ChIP-Seq training] #### Installing cutadapt Cutadapt is a Python program that removes adapter sequences from NGS reads. It has already been installed on the bits laptops but if you need to install it, use [http://wiki.bits.vib.be/index.php/Installing_cutadapt these instructions]. ### Quality control of NGS data #### Checking the quality of the Introduction training data using FASTQC==== In the /home/bits/NGS/Intro directory you can find a file called SRR074262.fastq (the file containing Arabidopsis RNA-Seq reads), that was used in the exercises on FastQC in Windows. FastQC is a tool that checks the quality of fastq files, containing NGS data. We will now try to do the same FastQC analysis from command line in Linux. FastQC is a java-based tool that needs java to be able to run. > Check if the correct version of java is installed In command line you can check if java is installed on your laptop using the following command: java -version You should see something like: ava version “1.8.0_101” Java(TM) SE Runtime Environment (build 1.8.0_101-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode) If you get an error then you don't have java installed. If the version listed on the first line is less than 1.5 then you will have problems running FastQC and you need to update java on your laptop. > Run FastQC To run FastQC as a GUI just like in Windows type: fastqc This opens the FastQC GUI and you could load a fastq file via the GUI to get its quality report. However, you can also use fastqc as a command via the command line. > Open the file SRR074262.fastq to obtain the sequence of the contaminating adapter. Check [http://wiki.bits.vib.be/index.php/Quality_control_of_NGS_data#Exercise_1:_Quality_control_of_the_data_of_the_introduction_training the exercise on FastQC in Windows] for details on the quality report that is generated The big plus of running FastQC from command line is that command line allows you to combine and run a set of commands as a program by writing a command script. #### Automating FASTQC analyses If you have many FASTQ files to check you might prefer running FASTQC from command line so you can loop over your files and process the reports automatically. > View the help files of the fastqc command As for most commands the -h option nicely opens the help file: fastqc -h To run via command line you can simply specify a list of files to process: fastqc somefile.fastq someotherfile.fastq You can specify as many files as you like. If you don't specify any files the program will open the GUI. However, there are a few options that might be helpful to use. Since FASTQC can process FASTQ, SAM and BAM files, it is always safer to tell him upfront which format to expect. We will generate FASTQC reports for the two FASTQ files in the /home/bits/NGS/RNASeq/ folder. > Decompress the files First you have to decompress the fastq files. In the cheat sheet look up the command for decompressing a .gz file gunzip chr22_SRR1039509_1.fastq.gz gunzip chr22_SRR1039509_2.fastq.gz Decompression of the files results in two .fastq files that can be used as inputs generating the FASTQC reports. > Generate the FASTQC reports for the two fastq files. As you can see in the help file of fastqc, the -f option allows you to specify the format of the input file(s). fastqc -f fastq chr22_SRR1039509_1.fastq chr22_SRR1039509_2.fastq The two .html files contain the FASTQC reports and can be opened in a browser. > Open the first report in firefox via command line firefox chr22_SRR1039509_1_fastqc.html By default, FastQC will create an HTML report with embedded graphs, but also a zip file containing individual graphs and additional data files containing the raw data from which the plots were drawn. > Remove the .html and the .zip files rm *.zip rm *.html If you have many files you might want to use a for-loop instead of typing all file names into the command. > Write a for-loop to process the two FASTQ files. First go back to the folder that contains the fastqc command and make sure you are operating as superuser. Take a close look at the syntax of the for-loop that is described in the slides. We are going to use the syntax for looping over files in a folder. Don' t forget the ***** to loop over all fastq files in the specified folder: for file in /home/bits/NGS/RNASeq/*.fastq do fastqc -f fastq ${file} done Don't forget the **$** since file is just a variable that refers to the actual files in the folder. Write every line on a different line in the terminal. When you go to the /home/bits/NGS/RNASeq folder you should see the same html and zip files as in the previous exercise. The two .html files contain the FASTQC reports and can be opened in a browser. If you want to save your reports in a folder other than the folder which contains your fastq files you can specify an alternative location by using the **-o** option. > Create a new folder called FASTQCresults mkdir FASTQCresults > Create a variable output. Its value is the path to the newly created folder. output=/home/bits/NGS/RNASeq/FASTQCresults/ > Write a for-loop to analyze the quality of the fastq files and write the report to the new folder Adjust the code of the for-loop to write the results to the newly created folder for file in /home/bits/NGS/RNASeq/*.fastq do fastqc -f fastq -o ${output} ${file} done Don't forget the **$** since output and file are variables. Write every line on a different line in the terminal. When you go to the /home/bits/NGS/RNASeq/FASTQCresults folder you should see the same html and zip files as in the previous exercise. The two .html files contain the FASTQC reports and can be opened in a browser. In this way you can process hundreds of FASTQ files automatically. You can even write a script to process the reports and create a general overview of the quality of the complete experiment. In the **Templates** directory of the /usr/bin/tools/FastQC/ you will find a file called **header_template.html** which you can edit to change the look of the report. This file contains all information concerning the layout of the FASTQC reports like the header for the report, the CSS section... and you can alter this however you see fit. ===Improving the quality of the data=== In this exercise we go back to the data set of the Intro training in folder /home/bits/NGS/Intro. Almost all NGS data sets contain a high number of contaminating adapter sequences. You can remove these adapters using command line tools like [https://code.google.com/p/cutadapt/ cutadapt]. See [http://wiki.bits.vib.be/index.php/Installing_cutadapt installation instructions]. > Check the help file for the option that defines the number of mismatches you allow (= error rate). To open the cutadapt help files (it's not a regular bash command so it doesn't have a manual) type: cutadapt -h Scrolling down the help file shows that the **-e** option defines the maximum allowed error rate: the default is 0.1 meaning that it allows one mismatch every 10 nucleotides. Adapter sequences are identified by aligning each read to the adapter sequence: if the frequency of mismatches in the alignment is below the allowed error rate then the adapter sequence is trimmed from the read. > Check the option you need for defining the adapter sequence In the help file you see that you have multiple options: - **-a** to trim adapter sequences at the 3' end of the reads. In most cases this is the adapter that's causing the problems: when small RNA fragments are sequenced, the resulting reads can be longer than the RNA fragments. As a results they will contain (parts of) the 3’ adapter. In longer reads the adapter might even lie within the read: MYSEQUEN (no adapter contaimination) MYSEQUENCEADAP (part of adapter at 3’ end) MYSEQUENCEADAPTER (adapter at 3’ end) MYSEQUENCEADAPTERSOMETHINGELSE (adapter within the read) Cutadapt will cut the adapter (part) and all sequence following it resulting in: MYSEQUEN MYSEQUENCE MYSEQUENCE MYSEQUENCE - **-g** to trim adapter sequences ligated at the 5' end of the reads. These adapters are expected to appear at the start of a read (where they can be just partially there) or somewhere within the read: ADAPTERMYSEQUENCE (5’ end) DAPTERMYSEQUENCE (partial) TERMYSEQUENCE (partial) SOMETHINGADAPTERMYSEQUENCE (within) In all cases, the adapter itself and the sequence preceding it will be removed, leaving in all examples above: MYSEQUENCE - **-b** to trim adapters at the 3' or 5' end of the read. If there is at least one base before the adapter, then the adapter is trimmed as a 3’ adapter and the adapter itself and everything following it is removed. Otherwise, the adapter is trimmed as a 5’ adapter and it is removed from the read, but the sequence after it it remains: Before trimming After trimming MYSEQUENCEADAPTERSOMETHING MYSEQUENCE MYSEQUENCEADAPTER MYSEQUENCE MYSEQUENCEADAP MYSEQUENCE MADAPTER M ADAPTERMYSEQUENCE MYSEQUENCE PTERMYSEQUENCE MYSEQUENCE TERMYSEQUENCE MYSEQUENCE Since we probably have contaminating adapter at the 3' end we'll take the -a option At the top of the help file you see that the standard usage of the command is: cutadapt -a ADAPTER -o output.fastq input.fastq You can find the sequence of the adapter in the FastQC report of SRR074262.fastq > Trim the adapter sequence using the default error rate, store the trimmed sequences in a file SRR074262trim.fastq So in our case the command is: cutadapt -a GATCGGAAGAGCTCGTATGCCGTCTTCTGCTTGAAA -o SRR074262trim.fastq SRR074262.fastq Note that the default error rate means that you allow max. 10% mismatches in the alignment of adapter and read. > How many reads consisted solely of adapter sequence (and were consequently completely removed) ? The output of the cutadapt command is: This is cutadapt 1.8.1 with Python 2.7.6 Command line parameters: -a GATCGGAAGAGCTCGTATGCCGTCTTCTGCTTGAAA -o SRR074262trim.fastq SRR074262.fastq Trimming 1 adapter with at most 10.0% errors in single-end mode … Finished in 66.92 s (7 us/read; 8.62 M reads/minute). Summary Total reads processed: 9,619,406 Reads with adapters: 2,327,902 (24.2%) Reads written (passing filters): 9,619,406 (100.0%) Total basepairs processed: 346,298,616 bp Total written (filtered): 271,141,022 bp (78.3%) Adapter 1 Sequence: GATCGGAAGAGCTCGTATGCCGTCTTCTGCTTGAAA; Type: regular 3’; Length: 36; Trimmed: 2327902 times. No. of allowed errors: 0-9 bp: 0; 10-19 bp: 1; 20-29 bp: 2; 30-36 bp: 3 Bases preceding removed adapters: A: 6.1% C: 1.5% G: 1.8% T: 3.0% none/other: 87.5% Overview of removed sequences length count expect max.err error counts 3 156030 150303.2 0 156030 4 48693 37575.8 0 48693 5 12005 9394.0 0 12005 6 8702 2348.5 0 8702 7 6686 587.1 0 6686 8 5546 146.8 0 5546 9 5958 36.7 0 5484 474 10 5479 9.2 1 4539 940 11 4197 2.3 1 3737 460 12 4038 0.6 1 3713 325 13 3392 0.1 1 3158 234 14 2730 0.0 1 2531 199 15 2801 0.0 1 2625 176 16 2384 0.0 1 2221 163 17 1887 0.0 1 1759 128 18 1998 0.0 1 1848 150 19 1572 0.0 1 1447 123 2 20 1257 0.0 2 1079 107 71 21 1141 0.0 2 1029 90 22 22 730 0.0 2 671 46 13 23 504 0.0 2 471 21 12 24 549 0.0 2 499 37 13 25 495 0.0 2 441 39 15 26 587 0.0 2 538 35 14 27 657 0.0 2 585 53 19 28 711 0.0 2 633 40 26 12 29 764 0.0 2 687 49 24 4 30 889 0.0 3 760 85 33 11 31 887 0.0 3 739 94 42 12 32 579 0.0 3 466 65 37 11 33 438 0.0 3 347 36 38 17 34 700 0.0 3 541 85 53 21 35 5390 0.0 3 4652 507 171 60 36 2037526 0.0 3 1870684 129754 20094 16994 In the last line you see the number of reads with 36 bases aligned to the adapter sequence. Since that is the total of the read (the reads are 36bp long) it means that over 2 million reads only consist of adapter sequence, 1.870.684 being completely identical to the adapter, 129.754 containing 1 mismatch with the adapter... > Open the trimmed sequences in FastQC To open the FastQC GUI type the fastqc command fastqc You can compare the results with these of the original reads on [http://wiki.bits.vib.be/index.php/Quality_control_of_NGS_data the Quality control of NGS data wiki page]. > Are all the reads still 36 nt long after trimming ? In the **Basic statistics** tab you see that the length of the reads varies as was to be expected after trimming > Have the quality scores of the reads significantly changed after trimming ? The **Per base sequence quality** is similar to that of the untrimmed file, as is the **Per sequence quality**. The latter one just shows a lower number of sequences since the 2 million reads that consisted solely of adapter sequence are no longer taken into account. Quality scores have changed a bit of course since you removed bases and reads from the data set but you did not trim based on quality but based on similarity to an adapter sequence so the scores of the trimmed reads are similar to those of the untrimmed reads. If you had trimmed low quality bases, the quality scores would have been higher in the trimmed reads. > Has the per base sequence content improved as a result of the trimming ? The **Per base sequence content** - the tool to detect adapter contamination - plot has greatly improved allthough it is still not considered stable enough. > What are the bumps you see in the Sequence length distribution plot ? This question is related to the results of the trimming: Overview of removed sequences length count expect max.err error counts 3 156030 150303.2 0 156030 4 48693 37575.8 0 48693 5 12005 9394.0 0 12005 … 33 438 0.0 3 347 36 38 17 34 700 0.0 3 541 85 53 21 35 5390 0.0 3 4652 507 171 60 36 2037526 0.0 3 1870684 129754 20094 16994 As you can see here over 2 million reads corresponded to adapter over their entire length and as a result were trimmed to length zero. This is the large peak at length zero on the plot. Over 150000 reads contain 3 bases that belong to the adapter. These 3 bases have been cut leaving reads of 33 nt long: this is the small peak you see on the plot at length 33. All intermediate lengths of adapter contamination have been detected but in such a small fraction of reads that you cannot see the influence of the trimming on the plot. FASTQC calls a failure for this plot because it knows the file contains Illumina data and it expects the reads to have the same lengths. The software does not consider the fact that this is no longer true after trimming. > Are there any overrepresented sequences left ? The 2 million sequences that were initially detected as contaminating adapters are still in the list but now as sequences with zero length. The other contaminating sequences are of course still present but at very low counts. > Are there any overrepresented hexamers ? FASTQC still detects overrepresented hexamers although at much lower counts than before. These are probably parts of the remaining overrepresented sequences. ### Linking files #### Linking FastQC In the previous exercise you had to specify the path of the fastqc command, otherwise the operating system was not able to find (and thus execute) the command. You can avoid having to specify the path every time you want to execute a command by creating a link to the command using the **ln** command. You can soft or hard links, for what we want to achieve a soft link is fine. When you place a link to the command in /usr/local/bin you will be able to run the program from any location by just typing fastqc So the overall format of the command is as follows: ln -s (soft link) path_where_fastqc_is (source path) /usr/local/bin/fastqc (destination path) > What's the command you would need for creating this soft link ? When you look in the manual of **ln** you see that for creating a soft link you need the **-s** option. So you use the following command: ln -s /usr/bin/tools/FastQC/fastqc /usr/local/bin/fastqc Check if you can run the fastqc command from any location now. #### Linking Tophat2 If you don't create a symbolic link you have to specify the full path of the command when you want to run it, otherwise the operating system is not able to find (and thus execute) the command. You can avoid having to specify the full path every time you want to execute a command by creating a link to the command using the **ln** command. For creating symbolic links you need superuser powers! You can make soft or hard links, for what we want to achieve a soft link is fine. When you place a link to the command in /usr/local/bin/ you will be able to run the program from any location by just typing its name. So the overall format of the command is as follows: ln -s (soft link) path_where_command_is (source path) /usr/local/bin/name (destination path) > Create a symbolic link for tophat2 For creating the link you need the following command: sudo ln -s /usr/bin/NGS/tophat-2.1.1.Linux_x86_64/tophat2 /usr/local/bin/tophat2 Remember to use tab autocompletion ! Now type **tophat2**. If you see the help file, the link works. If you mess up the link you have to remove it before you can try again using the following command: sudo unlink /usr/local/bin/tophat2 #### Linking samtools We will also do the same for samtools to use samtools from anywhere in the file system. > Create a symbolic link for samtools Create a link using the **ln -s** command: sudo ln -s /usr/bin/NGS/tophat-2.1.1.Linux_x86_64/samtools_0.1.18 /usr/local/bin/samtools-0.1.18 Check if the command works. If you type samtools-0.1.18 view (one of the possible samtools commands) you should see the manual of the command. In many cases you will have several versions of samtools running on your laptop. That's why I don't call the tool samtools but I choose the full name including the version number. [http://wiki.bits.vib.be/index.php/Introduction_to_ChIP-Seq_analysis#Linking_tools Linking tools for the ChIP-Seq training] ### Mapping reads #### Mapping reads of the ChIP-Seq training with Bowtie ### Mapping reads with Bowtie *Exercise created by Morgane Thomas Chollier* #### Obtaining the reference genome If you are going to follow the ChIP-Seq training, skip this part: you are going to do these steps during the ChIP-Seq training. The fasta file containing the reference genome is called Escherichia_coli_K12.fasta and is stored in the /home/bits/NGS/ChIPSeq/ folder on the BITS laptops. Alternatively you can use the file that you downloaded via wget in exercise 3. If you are not going to follow the ChIP-Seq training, go on and see how to obtain the reference genome. Back to the ChIP-Seq data of *E. coli*. In this experiment we want to see which genomic regions are bound to transcription factor FNR. However, at this point what we have is a set of reads that are identified by their location of the flow cell. To answer our question we should link the reads to regions in the genome. To obtain their genomic coordinates, we will map each read on the reference genome sequence As said before, for Illumina reads the standard mappers are BWA and Bowtie (version 1 and 2). In this exercise we will use Bowtie version1. Check out the [http://wiki.bits.vib.be/index.php/Linux_command_line#Installing_Bowtie installation instructions for Bowtie]. Bowtie1 was installed and a symbolic link was created so the command should work from anywhere in the file system when you type bowtie-1.1.2 > What happens when you type the bowtie command ? This prints the help of the program. However, the help file is a bit difficult to read ! If you need to know more about the program, it's easier to directly check [http://bowtie-bio.sourceforge.net/manual.shtml the manual on the website] Bowtie needs a reference sequence to align each read on it. > Which *E. coli* strain was used in the experiment ? Go to [http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1003565 the paper] and check the part **Strains and growth conditions** in the **Materials and methods** section. There you see that the experiment was done using *E. coli* K-12 MG1655. So we need the genome sequence of *E. coli* K-12 MG1655 and it needs to be in a specific format (=index) for bowtie to be able to use it. Several pre-built indexes are available to download on [http://bowtie-bio.sourceforge.net/manual.shtml the bowtie webpages] or the [http://support.illumina.com/sequencing/sequencing_software/igenome.html iGenomes website]. Although the *E. coli* sequence is available we will not use it to show you how you should proceed if you don't find your reference sequence here. In that case you will need to make the index file yourself. If you can't find your reference on the iGenomes website you have to download it from: - [http://genome.ucsc.edu/ UCSC] - [http://www.ensembl.org/index.html Ensembl] - [http://www.ncbi.nlm.nih.gov/ NCBI] Since Ensembl focuses on higher eukaryotes, we are going to download the genome from NCBI. > Which reference sequence was used in the experiment ? Go to [http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1003565 the paper] and check the part **High-throughput RNA sequencing (RNA-seq) analysis**. There you see that the reads were mapped to an NCBI sequence with version number **U00096.2**. > Search for this sequence on NCBI ? Go to [http://www.ncbi.nlm.nih.gov/ the NCBI website], select the **Nucleotide** database, type **U00096.2** as a search term and click **Search**. In the record of this sequence you see that an updated version is available. Click the **See current version** link. This sequence is not a RefSeq sequence (the high quality part of NCBI Genbank). You can see that because the version number does not contain an underscore and all RefSeq version numbers contain an underscore. > Is there a RefSeq sequence available ? In [http://www.ncbi.nlm.nih.gov/nuccore/U00096.3 the record of the current version], scroll down to the **Related information** section in the right menu. There you see that a RefSeq sequence is available. Click the **Identical RefSeq** link. This brings us to a RefSeq record with version number NC_000913.3. Note that we will not take this lastest version but the previous one (NC_000913.2), because the available tools for visualization have not been updated yet to the latest version. This will not affect our results. > Download the sequence of the previous version of the RefSeq record in FASTA format Search the **Nucleotide** database for **NC_000913.2** - In the record expand the **Send to** section (red). - Select **File** as destination (green). This means that you download the data on your computer. - Select **FASTA** format (blue). - Click **Create File**. This creates a file called **sequence.fasta** in the **Downloads** folder in your **Home** folder. Copy the downloaded file to the folder where the fastq files are located (/home/bits/NGS/ChIPSeq on the BITS laptops) and rename it as **Escherichia_coli_K12.fasta**. #### Writing a bash script to map the reads to the reference genome Suppose that you expect to be doing many NGS experiments on *E. coli*. Each time we analyze a data set, we will have to map the reads against the *E. coli* genome. The best way to ensure that you can reuse commands during the next analysis, is to combine them into a script (= small program). Since the script will consist of command line (= bash) commands, the script is called a bash script. You cannot do the mapping directly on the .fasta file, you need to index the file first. Reference genomes from the Bowtie /iGenomes website are already indexed so when you get your reference there you can skip this step. Reference genomes downloaded from NCBI, Ensembl or UCSC need to be indexed using the bowtie-build command. Indexing a reference genome is a one-time effort: you do not have to repeat it each time you do a mapping. This is why we are not going to include the indexing in the script. > Create a variable called folder containing the path to the folder that contains the E. coli fasta file folder=/home/bits/NGS/ChIPSeq/ > Check out the manual of the bowtie-1.1.2-build command to see the arguments it takes Since we have created a soft link for the bowtie-1.1.2-build command, the command should work from any location in the Linux file system. To see to help file just type the command: bowtie-1.1.2-build In the help file you see that you need to specify the reference genome that you want to index as an input (in our case the E. coli fasta file) and that you have to specify the output file. Usage: bowtie-build [options]* reference_in comma-separated list of files with ref sequences ebwt_outfile_base write Ebwt data to files with this dir/basename We will give the output files the same name as our input file: Escherichia_coli_K12 > Prepare an indexed reference sequence for E. coli using the bowtie-build command, use the folder variable So as an input the command expects the name of the input and the output file. bowtie-1.1.2-build ${folder}Escherichia_coli_K12.fasta ${folder}Escherichia_coli_K12 bowtie-build will index the Escherichia_coli_K12.fasta generating a whole set of .ebwt files whose name all start with Escherichia_coli_K12. We will write a bash script to do the rest of the mapping. Writing a script can be done in any text editor. On the BITS laptops you can use gedit: - Click the **Menu** at the bottom left corner of the desktop - Type **gedit** in the text search box - Click the **Text Editor** button The first thing you do when you write a script is define all the variables you need. We need the following variables: - The **folder** that contains the reference genome. - The name of the **input** fastq file you want to map (if it's in the same folder as the reference as it is in our case). If the fastq file is in another folder you have to specify the full path to the file. > Create the required variables folder=/home/bits/NGS/ChIPSeq/ input=SRR576933 Make sure that the file containing the indexed reference genome and the fastq files containing the *E. coli* reads are located in the same folder. > Check the help file for bowtie-1.1.2 Go back to the terminal and type bowtie-1.1.2 > What is the first argument bowtie expects ? As first argument bowtie expects the path to the ebwt files (= the genome index files) so in our case that's Escherichia_coli_K_12 Usage: bowtie [options]* > What is the second argument bowtie expects ? As second argument bowtie expects the information of the input file containing the reads, in our case SRR576933.fastq Bowtie can be used to map single end reads as we have but also to map paired end reads. In the case of paired end reads you have two fastq files, one with the upstream reads and one with the downstream reads. That's why you can specify two input files m1 and m2. In our case it's just one file. Usage: bowtie [options]* {-1 -2 Comma-separated list of files containing upstream mates (or the sequences themselves, if -c is set) paired with mates in Comma-separated list of files containing downstream mates (or the sequences themselves if -c is set) paired with mates in ``` > What is the final argument bowtie expects ? As final argument bowtie expects the output file which is in our case SRR576933.sam ``` Usage: bowtie [options]* {-1 -2 | --12 | } [] File to write hits to (default: stdout) ``` You need to tell bowtie which type of file your input file is. > What is the option for doing this ? Via the option: -q indicates the input file is in FASTQ format. ``` Usage: Input: -q query input files are FASTQ .fq/.fastq (default) ``` Fastq is the default, so you don't have to explicitly set this option. If you don't specify it in your command bowtie will automatically assume your input is in fastq format. You need to tell bowtie the maximum number of mismatches you allow in the alignments of your reads to the reference. > What is the option for doing this ? Via the option: -v ``` Alignment: -v report end-to-end hits w/ What is the option for doing this ? Via the option: -3 ``` -3/--trim3 trim bases from 3' (right) end of reads ``` We want to set this option to trim the last base from the 3' ends of the reads before the alignment is done. We also want to specify that we only want reads that map specifically to one location in the genome in our output. > What is the option for doing this ? Via the option: -m ``` -m suppress all alignments if > exist (def: no limit) ``` Finally we want to specify that the output should be SAM format. > What is the option for doing this ? Via the option: -S ``` SAM: -S/--sam write hits in SAM format ``` > Write the error channel to a file called SRR576933.out Via the option: -S ``` 2> SRR576933.out ``` In the script you use the variables to you have created instead of the actual file name SRR576933 > Map the reads to the indexed reference sequence ? So the full script becomes: ``` folder=/home/bits/NGS/ChIPSeq/ input=SRR576933 bowtie-1.1.2 ${folder}Escherichia_coli_K12 -q ${folder}${input}.fastq -v 2 -m 1 -3 1 -S 2> ${folder}${input}.out > ${folder}${input}.sam ``` We asked the mapper to create a sam file with mapping results. In the same way we could create a bam file. While SAM files can be inspected using Linux commands (head, less, grep, ...), BAM format is compressed and requires a special parser to read the file. Samtools is used to view bam files but it can also be used to analyze sam files. Look at this [http://davetang.org/wiki/tiki-index.php?page=SAMTools very informative wiki on samtools] and the [http://samtools.sourceforge.net/samtools.shtml official manual of samtools]. The manual does not document some of the commands, so it is better to first look in the wiki to find the command you need and then look in the manual to have an overview of the options it uses. We will use samtools to get a rough idea of the quality of the mapping. Look at the samtools wiki to see which command you need for getting the basic statistics of a sam file. > Command to get the basic stats of the mapping file. On the [http://davetang.org/wiki/tiki-index.php?page=SAMTools samtools wiki] you can see that you need the **samtools flagstat** command for this. However samtools flagstat expects a bam file as input. So look at the samtools wiki to see which command you need for transforming a sam into a bam file. > Command to convert sam into bam files. On the [http://davetang.org/wiki/tiki-index.php?page=SAMTools samtools wiki] you can see that you need the **samtools view** command for this. For the exact command you need to know if the sam file contains a header. Let's assume that the sam file indeed contains a header (it does, I checked). The symbolic link for samtools is samtools-0.1.18 Notice that we include the version number of bowtie and samtools in the symbolic link because we have mutiple versions of bowtie and samtools installed on the laptops. > Add the command for transforming the sam into a bam file to your script ``` folder=/home/bits/NGS/ChIPSeq/ input=SRR576933 bowtie-1.1.2 ${folder}Escherichia_coli_K12 -q ${folder}${input}.fastq -v 2 -m 1 -3 1 -S 2> ${folder}${input}.out > ${folder}${input}.sam samtools-0.1.18 view -bS ${folder}${input}.sam > ${folder}${input}.bam ``` > Add the command for analyzing the bam file to your script ``` folder=/home/bits/NGS/ChIPSeq/ input=SRR576933 bowtie-1.1.2 ${folder}Escherichia_coli_K12 -q ${folder}${input}.fastq -v 2 -m 1 -3 1 -S 2> ${folder}${input}.out > ${folder}${input}.sam samtools-0.1.18 view -bS ${folder}${input}.sam > ${folder}${input}.bam samtools-0.1.18 flagstat ${folder}${input}.bam ``` Bash scripts all have one characteristic: the first line of a bash script is always the following: ``` #!/bin/bash ``` This tells the system which program should be used to interpret the script (in this case: /bin/bash) > Add this line to your script So the full script becomes: ``` #!/bin/bash folder=/home/bits/NGS/ChIPSeq/ input=SRR576933 bowtie-1.1.2 ${folder}Escherichia_coli_K12 -q ${folder}${input}.fastq -v 2 -m 1 -3 1 -S 2> ${folder}${input}.out > ${folder}${input}.sam samtools-0.1.18 view -bS ${folder}${input}.sam > ${folder}${input}.bam samtools-0.1.18 flagstat ${folder}${input}.bam ``` Save the script as \"my_mapping\" in the /home/bits/NGS folder. > Check permissions of the script and change them if needed. Go to the folder where you have saved the script: /home/bits/NGS and type ``` ll ``` The script is not executable: Make it executable by typing: ``` chmod 755 my_mapping ll ``` To run the script make sure you are in folder containing the script (/home/bits/NGS) and type: ``` ./my_mapping ``` The mapping should take few minutes as we work with a small genome. For the human genome, we would need either more time, or a dedicated server. The samtools flagstat command displays an overview of the alignment results on your screen. The results are not very informative because the data set comes from a single-end sequencing experiment. You just see that 62% of the reads were mapped. This may seem low but remember that we haven't done any cleaning on the file. According to FASTQC the file contains about 30% of adapter sequences that will not map. Repeat the analysis for the control sample SRR576938.fastq These two fastq files come from a ChIP-Seq experiment, the first contains the reads of the ChIP sample, the second of the control sample, which consists of fragmented genomic DNA. You need both to identify regions in the genome that are represented more in the ChIP reda than in the control (these are the regions that bind to the transcription factor). > Repeat the analysis for sample SRR576938.fastq ? Repeating the mapping is easy now the only thing you need to do is changing the value of the input variable in the script: - Reopen the script in gedit - Change the name of input file - Save the changes - In the terminal go to the folder containing the script (/home/bits/NGS) - Run the script by typing: ``` ./my_mapping ``` > How many reads of the control sample were mapped ? In the flagstat results, you see that 95% of the reads was mapped. This is of course ok but you expected a high percentage here since the control sample is nothing more than the reference genome cut up into small pieces. At this point, you have two sam and two bam files, one for the treated sample, one for the control sample. For paired-end data flagstat results are much more informative, see an example below: This overview deserves some explanation: - **nan** means **Not A Number** (e.g: divided by 0 ) - **paired in sequencing** means reads that belong to a pair regardless of the fact that they are really mapped as a pair - **read1** means forward reads - **read2** means reverse reads - **properly paired** means that both mates of a read pair map to the same chromosome, oriented towards each other, and with a sensible insert size - **with itself and mate mapped** means that both reads of a pair map to the genome but they are not necessarily properly paired, they just map somewhere on the genome - **singletons** means that one of the reads of a pair is unmapped while its mate is mapped - **with mate mapped to a different chr** means reads with a mate mapped on a different chromosome - **with mate mapped to a different chr (mapQ >= 5)** means reads with a mate mapped on a different chromosome having a mapping quality greater than 5 > Compare the number of forward and reverse reads in the paired-end experiment. the counts of forward and reverse reads are to be found on the lines ending with read1 and read2 respectively. As you see the number of reverse reads exceeds the number of forward reads by 439. > How many reads were mapped as a pair in the paired-end experiment? 12.911.388 reads were properly mapped as a pair, that's 85,68% of the total number of reads You can find similar info in the SRR576933.out file in the ChIPSeq folder (using the **less** command), which also contains some statistics about the mapping. > How many reads were mapped according to this file ? You see that 62% of the reads was mapped, which is good considering 30% of the reads contained adapter sequences. Type **q** to leave the less editor. This result is in agreement with the result of the samtools flagstat command. #### Visualize mapping in IGV IGV is installed on the bits laptops and can be run using the **igv** command. ``` igv ``` This opens the graphical user interface of the tool (similar to what we have with firefox during the class). Be patient, it might take a few minutes for the program to start. We open the bam file that was generated by the Picard modules in IGV. The bam file contains Arabidopsis reads. This means we have to visualize them on the Arabidopsis genome. Change the genome in IGV from Human hg19 to A. thaliana (TAIR10). This should display the Arabidopsis genome in the top and the bottom view. Now it's time to load the mapped reads via **File** in the top menu and **Load from File**. Select the .bam file to open. You don't need to load the .bai file, it's suffcient that it is present in the same folder as the .bam file. This loads the data into the center view. At this point, you can't see the reads, you have to zoom in to view them. According to the [http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0025279#s3 supplemental material] accompanying the paper describing this data set, AT1G02930 is highly expressed in all samples and differentially expreesed during the defense response in ''A. thaliana''. So we will zoom in on this gene. You can do this by typing the accession number in the top toolbar and clicking **Go**: The reads for this gene are now visualized in the center view. You can zoom in even more using the zoom bar in the top toolbar: Zoom in until you see the nucleotides of the reference sequence. The reads are represented by grey arrows, the arrow indicating the orietation of the mapping. Hovering your mouse over a read gives additional info on the mapping. The colored nucleotides indicate mismatches between the read and the reference. Alignments that are displayed with light gray borders and white fill, have a mapping quality equal to zero. Interpretation of this mapping quality depends on the mapper as some commonly used mappers use this convention to mark a read with multiple alignments. In such a case, the read also maps to another location with equally good placement. It is also possible the read could not be uniquely placed but the other placements do not necessarily give equally good quality hits. By default IGV calculates and displays the coverage track (red) for an alignment file. When IGV is zoomed to the alignment read visibility threshold (by default 30 KB), the coverage track displays the depth of the reads displayed at each locus as a gray bar chart. If a nucleotide differs from the reference sequence in greater than 20% of quality weighted reads, IGV colors the bar in proportion to the read count of each base (A, C, G, T). You can view count details by hovering the mouse over a coverage bar:"},{"title":"01 Fundamental Image Aspects","url":"/topics/gimp-inkscape/tutorials/fundamental-image-aspects/tutorial.html","tags":[],"body":"Bitmap vs Vector images Bitmap Pixels in a grid/map Resolution dependent Restricted to rectangle Resizing reduces visual quality Easily converted Minimal support for transparency Popular file formats: BMP, GIF, JPEG, JPG, PNG, TIFF Bit depth or color depth is the amount of data assigned to every pixel (e.g. 1-bit = black/white, 4-bit = 16 colors/shades of grey, etc.) The more data, the more realistic your image will be. More data per pixel also means larger files. Vector Scalable Resolution independent No background Inappropriate for photo-realistic images XML based text format Popular file formats: SVG, AI, CGM, DXF, WMF, EMF Pixels Resolution = number of pixels = how much detail an image holds PPI: pixel per inch Screen pixel density (monitor/smartphone) Tells you how large an image is DPI: dots per inch Print-out dots density (inkjet/laser printer) Printer settings An image at 300 PPI will look fine on a monitor, but printing is another matter! Print it on paper and you will notice the difference between 72 DPI and 300 DPI File formats and compression JPG/JPEG Supports 26 million colours (24 bit) Lossy compression (information is lost from original file) Small file size (compressed) Photographs BMP Supports 8/16/24-bit Uncompressed file format Large file size TIFF Tagged Image File Format All colour and data information is stored Uncompressed (lossy and lossless compression is possible) Very large file size GIF Graphics Interchange Format Only 256 colours possible (8-bit) Replace multiple occuring patterns into one Small file size Animation PNG Portable Network Graphics 256 / 16M colours 8-bit transparancy Lossless compression SVG Scalable Vector Graphics XML-based format Lossless data compression Creatable and editable with a text editor Can contain both bitmap and vector data PDF Portable Document Format Can contain both bitmap and vector data RAW/DNG Digital Negative (DNG) is a universal RAW file format Raw image file (without white balance, color saturation, contrast settings, …) RAW files can be camera brand specific Large file size Multiple options without taking the picture again Publication vs Presentation Key features for publications: Raw/uncompressed image file (e.g. TIFF) High quality image (300 PPI) and resolution Lossless compression (e.g. PNG) Compression is sometimes allowed (check journal website!) Key features for presentation: Normal quality image (72 PPI) and smaller resolution (max width: 1920 pixels) Compression is allowed (e.g. JPEG) Smaller file size Guidelines on image editing Scientific accepted image manipulations are described in guidelines. VIB also has a document to guide you in what is and what isn’t acceptible when adjusting your images. Some examples are: No specific feature within an image may be enhanced, obscured, moved, removed or introduced Adjustments of brightness, contrast or color balance are acceptable if they are applies to the whole image as long as they do not misrepresent information in the original Grouping of images from different parts of the same or different gel, fields or exposures must be made explicit by the arrangement of the figure (dividing lines) The original data must be available by the author when asked to provide it, otherwise acceptance of the publications may be revoked you can find all the VIB guidelines here."},{"title":"02 Inkscape","url":"/topics/gimp-inkscape/tutorials/inkscape/tutorial.html","tags":[],"body":"What is Inkscape? Inkscape is professional quality vector graphics software which runs on Windows, Mac OS X and GNU/Linux. It is used by design professionals and hobbyists worldwide, for creating a wide variety of graphics such as illustrations, icons, logos, diagrams, maps and web graphics. Inkscape uses the W3C open standard SVG (Scalable Vector Graphics) as its native format, and is free and open-source software. During this training we will use Inkscape 0.92 on Windows. To download the most recent version, browse to the Inkscape Download page. For Windows 10 S: the Inkscape app is also available in the Microsoft Store. External training material Online Inkscape tutorials. Nick Saporito Inkscape tutorials for beginners Nick Saporito Inkscape intermediate/advanced tutorials User Interface Inkscape is a single-window program. Drawing tools are on the left hand side, option docks are on the right. In the central window, you have the drawing area with default an A4 page as document layout. To select another format for e.g. posters, go to File - Document Properties. Next to the document size, you can adjust the background colour (default: transparant). Import Images You can import scalable vector graphic files (.svg) and also GraphPad Prism graphs (.emf or .pdf format). Inkscape is not used for editing images like GIMP. If you import bitmap images, note that they are not scalable like vector objects! Drawing lines and objects You can draw a line with the Draw Bezier tool. You can make your own shape or just draw a line or path. On top of your drawing area you can select the Mode: Regular Bezier curves, Spiro paths, straight line segments and paraxial line segments. When selecting the straight line mode, you can hold the Ctrl button to make your line snap every 15 degrees around your first/previous point. You can draw shapes by using the Rectangle tool, Ellipse tool and the Create Stars and Polygons tool. On top of the drawing area you can specify your polygon and star properties, size and lock aspect ration. Here is the Crtl key useful as well for creating squares, circles or specify the position of your object. When you have an object (polygon or others) you can select a color for the stroke and inside of the object. Selecting an object using the Selection tool will give you more options on top of the view area. You have the option to rotate, flip, change dimensions and XY position (in different units). You can change the position of the selected object compared to others (move up/down). Paths A path consist of lines and nodes. These lines can be straight or curved and you can make an object using paths ( closed path). When in Path mode you have several options; add or remove a node, joining or breaking nodes apart and changing the node properties. You can also change the segment (line between nodes) properties with the options on top of the screen. You can convert an object into a path to gain more flexibility by selecting the object and go to Path – Object to path. Afterwards you can use the object tool or the path tool to manipulate the object. Fill and stroke Paths, lines and objects can be given a plain color, patterns, gradient color or left blank/transparent. You can also configure the stroke style and color. Click Object – Fill and Stroke to see all the options. Paths/lines can be transformed into arrows using the Stroke style option Markers. Text At the left there is also a Text tool available. With this tool you can create and change text, it’s colour, font, style and size. After entering text, you’re able to manipulate it like an object. You can also attach text into a frame by selecting both objects and click on Text – Flow into Frame. You can also align text to a path. Select both text and path and click Text – Put on Path. Once the text in aligned to the path it stays adaptable and can be removed from the path; Text - Remove from Path. Text is an object at first. When you select Path - Object to path you can modify your text like any other object that is converted into a path. Grouping, aligning and arranging object/paths To group several object you must select them all (hold Shift) and select Object – Group. To unite several paths you must select Path – Combine. Both options are the same and allow you to manipulate objects/paths as one. Both actions can be reversed (Ungroup / Break Apart). Several object must be aligned before you group them, think of text inside a box. To display the options, go to Object - Align and Distribute. When multiple objects are selected, you can align the top, bottom, left and right edges of the objects. Aligning on the central axes is also possible, this in both horizontal as vertical direction. The aligned objects always need an anchor, this can be changed in the box on top of the toolbox (Relative to:). This anchor can be an object (first, last, smallest or biggest) or the page, a selection or the complete drawing. Distributing objects works in a similar way, but manages the space between objects. For paths you can only align the nodes. Aligning or distributing objects allows you to manipulate the X and Y position of your objects. There is also a virtual Z axis. When you have multiple objects with different colours, you can move the one above the other. Every new object you draw will be on top of all the rest. To raise an object one step or to the top, you can use the buttons on top of your screen. The same can be done to lower an object one step or to the bottom. Path Effects and operations When you want to distribute/multiply an object along a guideline, there is a tool called Path Effects. First draw and select the object or group of objects and past it in the clipboard (Ctrl + C). Draw or select your path (guideline) and select Path – Path Effects. Click on the ‘+’ sign and select the effect Pattern Along Path. In the new box on the right: select ‘Repeated’ on the option Pattern copies. Now click on ‘Paste path’ to paste the object you want to multiply. Note that only the shape is pasted, not the color. When adjusting the color, it will affect the entire path. To copy the colour, use Crtl+C again on your original, select your path of objects and go to Edit - Paste Style - Paste Style. There are also standard patterns to distribute along a path. When clicking on the ‘+’ sign to add an effect, select ‘Gears’ or ‘Hatches (rough)’. Each of these effects have their own options to create an effect and to adjust the pattern. When it comes to paths, you can do much more than combining them. When you want to cut one shape out of another shape, you can use the options in the Path menu; Union, Difference, Intersection, Exclusion, Division and Cut Path. Diagrams To make a diagram with objects (circles, rectangles, stars, etc.) connected by lines, there is the Diagram connector tool. First you must draw and align the objects to create your diagram. Then select the Diagram connector tool. Every object can be selected by clicking in the white box in the middle of the object. Once connected the lines will follow the object if you move it to another place. The lines can be used as a path, therefore you can also modify them to e.g. dashed lines, arrows, etc. Exercises hands_on Hands-on: Exercise 1 Image 1 PNG: Image 1 Image 1 SVG: Image 1 SVG Task: Reproduce the top strand. Afterwards, reproduce the bottom strand using the first one. hands_on Hands-on: Exercise 2 Image 2 PNG: Image 2 Image 2 SVG: Image 2 SVG Task: Reproduce one of the sets of this image. Afterwards, reproduce the others using the first set. hands_on Hands-on: Exercise 3 Image infographic 1: Image 1 Image infographic 2: Image 2 Image infographic 3: Image 3 Task: Try to reproduce one of these images using the video tutorial series from Nick (see top of this page)."},{"title":"05 Peak calling with MACS","url":"/topics/chip-seq/tutorials/peak-calling-macs/tutorial.html","tags":[],"body":"Using the mapping results we can define the peaks, the regions with a high density of reads in the ChIP sample, where the transcription factor was bound. There are multiple programs to perform the peak calling. Some are more directed towards histone marks (broad peaks) while others are specific to narrow peaks (transcription factors). Here we will use MACS because it’s known to produce generally good results, and it is well-maintained by the developer. MACS is installed on GenePattern. Check the documentation on GenePattern or read the manual on the MACS github. Let’s see the parameters of MACS before launching the peak calling. How to define the input files? treatment and control: the treatment mapped read file (SRR576933.bam) and the control mapped read file (SRR576938.bam) Note that the bam files need to be sorted according to genomic location. At this point they are not, the reads are in the same order as they were in the fastq file, according to position on the flow cell. Which Picard tool can we use to sort the files? You can use Picard.SortSam for this. Sort the bam files so that they can be used as input for MACS. Use the default parameter settings. Let’s go over the different parameters of MACS: The effective genome size is the size of the genome considered “usable” for peak calling. This value is given by the MACS developers on their website. It is smaller than the complete genome because many regions are excluded (telomeres, highly repeated regions…). The default value is for human (2700000000), so we need to change it. As the value for E. coli is not provided, we will take the complete genome size 4639675. MACS needs the length of the fragments, which are longer than the read length, because the sequencer sequences only parts starting from the end of the fragments. MACS2 does this by making a model of enrichment of reads in the ChIP sample versus the background, searching pairs of peaks within a bandwidth of 300 bases with an enrichment ratio between 5 and 50. If there are not enough pairs of peaks, as is the case in our data, you can fall back on using a preset fragment length by setting the model parameter to no. The default of shift 0 extsize 200 is adequate for ChIPSeq. It means that reads are extended to a length of 200 bases before they are counted. The duplicates specifies how MACS should treat the reads that are mapped to the exact same location (duplicates). The manual specifies that keeping only 1 representative of these “stacks” of reads is giving the best results. The make BedGraph parameter will output a file in BEDGRAPH format to visualize the peak profiles in a genome browser. There will be one file for the treatment, and one for the control. FDR and FDR for broad peaks indicates that MACS will report peaks if their associated p-value is lower than the value specified here. Use a relaxed threshold as you want to keep a high number of peaks (even if some of them are false positives). Perform peak calling on the sorted bam files. Set the parameters as described above: Load sorted bam files for treatment and control Set effective genome size to 4639675 Don’t use a model Make a bedgraph file Look at the files that were created by MACS. Which files contains which information? macs_summits.bed: location of the summit base for each peak (BED format).If you want to find the motifs at the binding sites, this file is recommended. The file can be loaded directly to the UCSC genome browser. Remove the beginning track line if you want to analyze it by other tools. macs_peaks.xls: peak coordinates with more information, to be opened with Excel. Information include: chromosome name start position of peak end position of peak length of peak region absolute peak summit position pileup height at peak summit -log10(pvalue) for the peak summit (e.g. pvalue =1e-10, then this value should be 10) fold enrichment for this peak summit against random Poisson distribution with local lambda -log10(qvalue) at peak summit Coordinates in XLS is 1-based which is different from BED format. MACS_peaks.narrowPeak is a BED file which contains the peak locations together with peak summit, p-value, and q-value. You can load it to the UCSC genome browser. Definition of some specific columns are: 5th: integer score for display. It’s calculated as int(-10log10pvalue) or int(-10log10qvalue) depending on whether -p (pvalue) or -q (qvalue) is used as score cutoff. 7th: fold-change at peak summit 8th: -log10pvalue at peak summit 9th: -log10qvalue at peak summit 10th: relative summit position to peak start The file can be loaded directly to the UCSC genome browser. Remove the beginning track line if you want to analyze it by other tools. The MACS_treat_pileup.bdg and MACS_control_lambda.bdg files are in bedGraph format which can be imported to the UCSC genome browser or be converted into even smaller bigWig files. The MACS_treat_pileup.bdg contains the pileup signals (normalized) from the ChIP sample. The MACS_control_lambda.bdg contains local biases estimated for each genomic location from the control sample."},{"title":"01 GenPattern tutorial","url":"/topics/chip-seq/tutorials/genepattern-tutorial/tutorial.html","tags":[],"body":"Introduction to GenePattern Access GenePattern You can work on our BITS Genepattern server. Ask the trainer for login details. The GenePattern user interface Logging in brings you to the GenePattern homepage: Click the GenePattern icon at the top of the page (red) to return to this home page at any time. The upper right corner shows your user name (green). The navigation tabs (blue) provide access to other pages. We’ll zoom in on the navigation tabs: The Modules tab gives access to the tools that you can run. Enter the first few characters of a module in the search box to locate a tool. Click the Browse modules button to list the tools. The Jobs tab shows an overview of the analyses that you have done by showing the tools that you have run, together with a list of output files that were generated. The Files tab shows a list of files you can use as input for the tools. These are files that you have uploaded from your hard drive or files that were generated as the output of a tool and that were saved to the Files tab. In your case the Files tab contains a folder uploads. Searching a tool in GenePattern You can find a module by typing its name into the search box on the Modules tab: Searching a tool makes its name appear in the main window. Running tools in GenePattern Clicking the name of the tool will open its parameter form in the main window. Fill in the parameters and click Run to start the analysis. As long as the tool is running you see an arched arrow in the top right corner: When the tool has finished the arched arrow is replaced by a checkmark and the file(s) containing the results appear at the bottom: Note that apart from the file containing the results, other files are generated e.g. stdout.txt containing the error log of the tool. You can consult the error log in case of problems. Check the documentation To obtain a description of the parameters of a tool and their default values click the Documentation link at the top of the page. Define input files Many input files are located in the shared data folder. In the parameter form of a tool, you will find the Add Paths or URLs button in the input files section: Click the button and expand BITS trainingdata Chipseq: Store the output of a tool in GenePattern Copy the file in the uploads folder on the Files tab to store it permanently and allow to use it as input for other tools. Output files that are not saved in the uploads folder are stored 7 days on the server and are visible via the Jobs tab. When a tool has finished output files are generated at the bottom of the page. Click the name of the output file. Select Copy to Files Tab"},{"title":"04 Mapping reads with Bowtie","url":"/topics/chip-seq/tutorials/mapping-reads-bowtie/tutorial.html","tags":[],"body":"Mapping reads with Bowtie Exercise created by Morgane Thomas Chollier Obtaining the reference genome In the ChIP-Seq experiment of E. coli we want to see which genomic regions are bound to transcription factor FNR. However, at this point what we have is a set of reads that are identified by their location on the flow cell. To answer our question we should link the reads to regions in the genome to obtain their genomic coordinates. This process is called mapping. For Illumina reads the standard mappers are BWA and Bowtie (version 1 and 2). Which version of Bowtie are we going to use? We will use Bowtie version 1 as this version was designed for mapping short reads ( the message is “sequence.fasta. This creates a file called sequence.fasta in the Downloads folder of your computer. Upload the downloaded file to your Uploads folder in GenePattern. Go to the Files tab in GenePattern. Drag and drop the file onto the Drag Files Here section. Select the Uploads folder and click Select If all goes well you should see the following message If the upload takes too long use the fasta file from the SHARED_DATA folder in GenePattern. Indexing the reference genome You cannot do the mapping directly on the .fasta file, you need to index the file first. Reference genomes from the Bowtie/iGenomes website are already indexed so when you get your reference there you can skip this step. Reference genomes downloaded from NCBI, Ensembl or UCSC need to be indexed using the Bowtie_1 indexer tool. Indexing a reference genome is a one-time effort: you do not have to repeat it each time you do a mapping. Check the documentation of the Bowtie_1 indexer to see the parameters it takes. The documentation shows that you need to specify: the reference genome that you want to index as an input (in our case the E. coli fasta file) the name of the indexed output file Give the output file the same name as the input file: Escherichia_coli_K12. The Bowtie indexer will generate a zip file containing a whole set of .ebwt files whose name all start with Escherichia_coli_K12. Copy the zip-file to your Uploads folder. Mapping the reads Open the Bowtie_1 aligner parameter form. Use the indexed E.coli genome for mapping The first parameter of the Bowtie 1 aligner parameter form are the genome index files (= the zipped ebwt files in your Uploads folder). Go to the Files tab Click the Upload your own file button in the bowtie1 index section of the bowtie 1 parameter form Drag and drop the zip file to the Drag your files here section How to define the input file(s) ? Bowtie needs an input file containing the reads (in our case SRR576933.fastq). Bowtie can map single end reads like we have but also paired end reads. In the case of paired end reads you have two fastq files, one with the upstream reads and one with the downstream reads. That’s why you can specify two input files: reads pair 1 and reads pair 2. We just select SRR576933.fastq from the SHARED_DATA folder as input for reads pair 1. You need to tell bowtie what type of file your input file is. What is the parameter for doing this ? Via the parameter called input format you can specify that the input file is in fastQ format. FastQ is the default, so you don’t have to explicitly set this option. Bowtie has two modes of mapping. The simplest strategy is called v-mode alignment: you align complete reads (from the first to the last base aka end-to-end) to the reference and you count the number of mismatches in this alignment. In this mode quality values are ignored and you need to tell bowtie the maximum number of mismatches you allow. Do a v-mode mapping allowing 2 mismatches in the alignments. Expand the advanced customization of run parameters Set alignment mode to v-mode Set max mismatches for vmode alignment to 2 it means that bowtie will allow two mismatches anywhere in the alignments. The value for this parameter must be a number from 0 through 3. Remember because the base quality at the 3’end of the reads is lower, base calls at the 3’ends are often incorrect. This will inevitably lead to mismatches in the alignments. Reads with more than 2 mismatches will not be reported. To avoid losing too many reads during the mapping we can either trim low quality bases from the 3’ ends of the reads before the alignment is done or use a mapping strategy that takes into account the quality scores of the bases. This strategy is called n-mode alignment. It’s the default mode. It aligns seeds, the first N bases of the reads at the high quality 5’end, to the reference. You have to set the length of the reads and the maximum number of mismatches allowed in the seed alignment. Additionally the sum of the quality scores at all mismatched positions (not just in the seed) is calculated and you can set a maximum for this parameter. In this way, reads with mismatches with high quality scores will not be reported whereas mismatches with low scores are more or less ignored. The FASTQC report showed that the last base is of low quality. Since the reads are 36 bases ling we could use seeds of 35 bases for the mapping. Do an n-mode mapping with seeds of 35 bases allowing 2 mismatches in the seeds. Expand the advanced customization of run parameters Set alignment mode to n-mode Set seed length for nmode alignment to 35 Set max mismatches in seed to 2 it means that bowtie will allow two mismatches in the alignments of the seeds (the first 35 bases of the reads) to the reference. The value for this parameter must be a number from 0 through 3. We also need to specify that we only want to report reads that map specifically to one location in the reference. What is the parameter for doing this ? Via the parameter called report alignments you can specify that the output file should contain reads only mapping at unique location. By default, bowtie will include unmapped reads in the output file. That’s unnecessary since no one uses these unmapped reads. How to exclude unmapped reads from the output file? Via the parameter called include unaligned in the output section you can specify not to include unmapped reads in the output file. We want to get a rough idea of the quality of the mapping. Look at the stdout.txt file that was generated by bowtie to get the basic statistics of the mapping. ![](../../images/GPBowtie5.png 300px}} You see that 62% of the reads were mapped. This may seem low but remember that we haven’t done any cleaning on the file. According to FASTQC the file contains about 30% of adapter sequences that will not map. How many multi-mapped reads were originally present in the sample? Multimappers are reported as reads with alignments suppressed due to -m. Behind the scenes the report alignments parameter in the form is translated into a bowtie -m option that is run at command line. This option is used to guarantee that reported alignments are unique. Specifying -m 1 instructs bowtie to refrain from reporting any alignments for reads having more than 1 reportable alignment. The output of Bowtie is a sam file. The SAM format corresponds to large text files, that can be compressed (“zipped”) into .bam files that take up to 4 times less disk space and are usually sorted and indexed for fast access to the data they contain. The index of a .bam file is named .bai aand some tools require these index files to process the .bam files. So we need to transform the .sam file with our mapping results to a .bam file. You can use one of the tools from the Picard toolbox for this. Convert the sam to a bam file. You can use the tool Picard.SamToBam for this. Repeat the analysis for the control sample SRR576938.fastq These two fastq files come from a ChIP-Seq experiment, the first contains the reads of the ChIP sample, the second of the control sample, which consists of fragmented genomic DNA. You need both to identify regions in the genome that are represented more in the ChIP reads than in the control (these are the regions that bind to the transcription factor). Suppose that you have many fastq files that you need to map to the E. coli genome. The best way to ensure that you can reuse tools and parameter settings during the next analysis, is to combine them into a pipeline. Create a pipeline to map ChIPSeq data set? In the top menu select Modules & Pipelines Click New Pipeline ![](../../images/GPPL.png 100px}} In the Search Modules section search for the modules you need: first Bowtie_1_aligner and then Picard.SamToBam Click a tool to open its parameter form in the right pane of the pipeline editor. You can set values for the parameters or you can allow users to give values for the parameters when they use the pipeline. For the Bowtie_1_aligner allow users to define the index and the input fastq file by checking the boxes in front of these parameters. After you have done this you should see a user icon appearing in front of these parameters in the middle pane of the pipeline editor. ![](../../images/GPPL2.png 750px}} Use the same settings for the remaining parameters as you used for mapping SRR576933.fastq Connect the sam output of bowtie as input file in Picard. Click the Properties button at the top to open the Editing pipeline parameters in the right pane. Type a name for the pipeline and hit Enter Click the Save button at the top. The pipeline has now become a module that you can search for and run in GenePattern. Exit the pipeline editor by clicking the GenePattern button at the top. ![](../../images/GPPL3.png 150px}} Now you use the pipeline as a regular module. Repeat the analysis for sample SRR576938.fastq use the ChIPSeqMapping pipeline. Repeating the mapping is easy, the only thing you need to do is define the index and the input file: Open the parameter form of the ChIPSeqMapping pipeline Drag and drop the zip file with the indexed genome to the Drag your files here section Use SRR576938.fastq from the SHARED_DATA folder as input file Run the pipeline How many reads of the control sample were mapped ? In the stdout.txt file generated by bowtie, you see that 95% of the reads was mapped. This is of course ok but you expected a high percentage here since the control sample is nothing more than the reference genome cut up into small pieces. At this point, you have two sam and two bam files, one for the treated sample, one for the control sample."},{"title":"06 Visualization with deepTools","url":"/topics/chip-seq/tutorials/visualization-deeptools/tutorial.html","tags":[],"body":"For visualization with deepTools we need a bam file in which the order of the reads is determined by genomic location. We have created such a bam file in the peak calling step using the SortSam tool from the Picard suite. The bam file still contains duplicate reads (=reads that map to exactly the same position in the genome). Such reads represent technical duplicates often caused by biased PCR amplification during the library prep or by fragments coming from repetitive elements in the genome… Since we are going to quantify the reads (we look for regions that are enriched in the ChIP sample) these technical duplicates will distort the quantifications. So they should be removed from the .bam file Additionally an index file should be created to allow for fast and easy access to the sorted and processed .bam file. Which tool from the Picard suite can be used to mark/remove duplicates? Picard MarkDuplicates can be used to remove duplicates. Remove duplicates and index the .bam files? Use the sorted .bam files as input files Indicate that the files are sorted according to coordinates Remove the sequencing duplicates, duplicates generated by PCR Create an index file MarkDuplicates generates an error but you can ignore the error. Open the metrics.txt file that is generated by MarkDuplicates. How many duplicates were found in the ChIP sample? Now we will plot a Lorenz curve with DeepTools to assess the quality of the ChIP. It answers the question: “Did my ChIP work?” Did the antibody-treatment enrich sufficiently so that the ChIP signal can be differentiated from the background signal in the control sample? This is a valid question since around 90% of all fragments in a ChIP experiment will represent the genomic background. For factors that enrich well-defined, narrow regions, the plot can be used to assess the strength of the ChIP, but the broader the enrichments, the less clear the plot will be. Vice versa, if you do not know what kind of signal to expect, the plot will give you an indication of how careful you have to be during downstream analyses to separate biological noise from meaningful signal. The tool randomly samples genome regions (bins) of a specific legth in indexed BAM files, calculates the sum of all reads that map in a bin. These sums are sorted according to their rank and a profile of cumulative sums is plotted. Which tool from the DeepTools toolset are you going to use for this? Run DeepTools plotFingerprint to draw the Lorenz curve. Create a Lorenz curve for the ChIP sample You have to provide both the .bam and the .bai file as input! The nsamples parameter represent the number of bins that is sampled from the genome. It has to be smaller than the genome size divided by the size of the bins (default 500nt). The size of the E. coli genome is 4639675 nt. So we will set this parameter to 9000. Other parameters can be kept at default settings An experiment with perfect uniform distribution of reads along the genome (without enrichment) and infinite sequencing coverage should generate a straight diagonal line. A very specific and strong ChIP enrichment will be indicated by a prominent and steep rise of the curve towards the highest rank. This means that a big chunk of reads from the ChIP sample is located in few bins. Below you see a few examples on how to interpret this curve: What do you think about the fingerprint plot that was generated on the E. coli data?"},{"title":"01 Download the data","url":"/topics/chip-seq/tutorials/download-data/tutorial.html","tags":[],"body":"Downloading a data set for the ChIP-Seq training Download the data from GEO For the ChIP-Seq training, we are going to use the data set that is described in the article of Myers et al., 2013 [6]. The data consists of reads from ChIP enriched genomic DNA fragments that interact with FNR, a well-studied global transcription regulator of anaerobiosis. As a control, reads from fragmented genomic DNA were used. NGS datasets are (usually) made freely accessible, by depositing them into specialized databases. Sequence Read Archive (SRA) located in USA and hosted by NCBI, and its European equivalent European Nucleotide Archive (ENA) located in England hosted by EBI both contains raw, unprocessed reads. Processed reads from functional genomics datasets (transcriptomics, genome-wide binding such as ChIPSeq,…) are deposited in Gene Expression Omnibus (GEO) or its European equivalent ArrayExpress. The article contains the following sentence at the end of the Materials and Methods section: “All genome-wide data from this publication have been deposited in NCBI’s Gene Expression Omnibus (GSE41195).” In this case GSE41195 is the identifier that allows you to retrieve the dataset from the NCBI GEO (Gene Expression Omnibus) database. GEO hosts processed data files from experiments related to gene expression studies, based on NGS or microarrays. The files of NGS experiments can include alignments, peaks and/or counts. Go to the GEO page hands_on Download the data of the experiment with GEO ID GSE41195 Type the ID in the search box on the GEO home page Click Search This redirects you to the GEO record of the full experiment consisting of microarrays, tiling arrays and a ChIP-Seq experiment. In the Experiment type section you can see that this GEO record indeed reports a mixture of expression analysis and ChIP-Seq experiments. Scroll to the bottom of the page: You can see that the ChIP-Seq data have their own GEO ID: GSE41187 Click the ChIP-Seq data ID: GSE41187. This brings us on the GEO record of the ChIP-Seq experiment. In the GEO record scroll down to the Samples section: For time’s sake, we will focus in the training on a single sample: FNR IP ChIP-seq Anaerobic A Click the GEO ID GSM1010219 of the sample that we will use in the training This brings us to the GEO record of the sample. Scroll to the bottom of GEO record of the sample to the Relations section: GEO only contains processed data, no raw data. The raw data is stored in the SRA database. In the Relations section you can find the SRA identifier of this data set. For the training we would like to have a fastq file containing the raw data. Copy the SRA identifier Download the data from ENA at EBI Although direct access to the SRA database at the NCBI is doable, SRA does not store sequences in a FASTQ format. So, in practice, it’s simpler (and quicker!!) to download datasets from the ENA database (European Nucleotide Archive) hosted by EBI (European Bioinformatics Institute) in UK. ENA encompasses the data from SRA. SRA identifiers are also recognized by ENA so we can download the file from EBI. Go to the ENA website at EBI hands_on Download the data with SRA ID SRX189773 Type the ID in the search box on the EBI home page Click the search icon This returns two results: a link to the record of the experiment and a link to the record of the run: Click the first result (red) The table at the bottom of the page contains a column called Fastq files (ftp) Click the link in this column to download the data in fastq format For the training you do not have to download the data, it’s already on the GenePattern server. To download the replicate and the control data set, we should redo the same steps starting from the GEO web page of the ChIP-Seq experiment (click the sample ID of the FNR IP ChIP-seq Anaerobic B and the anaerobic INPUT DNA sample). The fastq file of the control sample is also available on the GenePattern server."},{"title":"07 Visualizing the peaks in a genome browser","url":"/topics/chip-seq/tutorials/visualizing-peaks-genome-browser/tutorial.html","tags":[],"body":"Choosing a genome browser There are several options for genome browsers, divided between the local browsers (need to install the program, eg. IGV) and the online web browsers (eg. UCSC genome browser, Ensembl). We often use both types, depending on the aim and the localisation of the data. Note that if you’re working on a non-model organism, the local viewer will be the only choice. If the aim is to share the results with your collaborators, view many tracks in the context of many existing annotations, then the online genome browsers are more suitable. Viewing the aligned reads in IGV Open IGV. Be patient, it might take a few minutes for the program to start. Change the genome in IGV from ‘'’Human hg19’’’ to the one you used in the mapping. Load the desired genomed. Load the E. coli genome as reference (from the file Escherichia_coli_K_12_MG1655.fasta, downloaded to build the bowtie index). Top menu: Genome -> Load Genome from File The loaded genome appears in the top left panel: You can also visualize the annotation (genes) in IGV. You can obtain a file with annotations from the Refseq record. Download the annotations from RefSeq in GFF3 format. Go to the RefSeq record of the E. coli genome. Expand the Send to section at the top of the page. Choose File as destination. Select GFF3 format. You can also download the GFF3 file from our website. If you want to load the .gff3 file and visualize the annotation properly in IGV, it’s necessary to comment (or remove) the third line: ##sequence-region NC_000913.3 1 4641652 ##species https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=511145 ## NC_000913.3 RefSeq region 1 4641652 . + . ID=NC_000913.3:1..4641652;Dbxref=taxon:511145;Is_circular=... NC_000913.3 RefSeq gene 190 255 . + . ID=gene-b0001;Dbxref=ASAP:ABE-0000006,ECOCYC:EG11277... You can visualize reads in IGV as long as they are sorted according to genomic location. Download the two sorted and indexed bam files (for SRR576933 and SRR576938) from GenePattern to your computer and load them in IGV. Load the annotation and the bam files of the ChIP and the control sample. Top menu: File -> Load from File: You should see the track now. Do the same for the .bam files. Note that you have to download the .bai files too and store them in the same folder as the .bam files. You do not have to explicitly open the .bai files in IGV but they have to be in the same folder as the .bam files or IGV will throw an error. Zoom in u8ntil you see the reads Browse around in the genome. Do you see peaks? Go to the following gene: pepT. Type pepT in the box at the top (red) and click Go: Do the same for gene ycfP. Looking at .bam files does not allow to directly compare the two samples as data are not normalized. To generate normalized data for visualization you can use bamCoverage from deepTools (it’s available in GenePattern). It generates BigWig files out of .bam files. Create a BigWig file from the sorted and indexed .bam file of the ChIP sample The bamCoverage tool has the following parameters: input file is the sorted and indexed .bam file to process index is the accompanying .bai file output format is the output file type, we want to generate a BigWig file genomeSize 4639675 nt for E. coli normalize: different overall normalization methods; we will use the RPGC method corresponding to 1x average coverage skip noncovered: skip non-covered regions (without mapped reads) in the genome? Set to yes. extend reads: extend reads to fragment size, in our case 200 nt. ignore duplicates: reads that map to the same location in the genome will be considered only once. Set this to yes. Repeat for the control (again you see the benefit of creating a pipeline for repeating the same steps on multiple samples). Download the BigWig files, start a new session in IGV and load the BigWig files in IGV. Create a BigWig file from the sorted and indexed .bam file of the ChIP sample Top menu: File -> New session Top menu: File -> Load from File. Load the two BigWigs and the .ggf3 with the annotation. Right click the names of the BigWig tracks and select Autoscale. Go back to the genes we looked at earlier: pepT, ycfP. Look at the shape of the signal. Viewing the peaks in IGV Download the bdg files generated by MACS from GenePattern to your computer and rename them with the extension .bedgraph. Dowload the bdg files. Click the names of the bdg files in the Files tab and select Save File Replace .bdg by .bedgraph otherwise the file will not be recognized by IGV. Open a new session in IGV. Reload the .ggf3 file with the annotation. View the bedgraph files. Load the control bedgraph file: Top menu: File -> Load from File: You might get a warning that the file is big. Simply click on the button continue. You should see the track (in blue): Repeat this step to load the treatment bedgraph file. You should now see the 2 tracks (in blue): Download and view the BED file containing the peak locations. View the bed file with the peak locations. Save the file from GenePattern to your computer and load the bed file into IGV. A new track with discrete positions appears at the bottom: The end result should look like this: 3 tracks with data (the bedgraph files of the 2 samples and the peaks file) and 1 track with annotation: Go back again to the genes we looked at earlier: pepT, ycfP. Do you see peaks?"},{"title":"08 Motif analysis","url":"/topics/chip-seq/tutorials/motif-analysis/tutorial.html","tags":[],"body":"Motif analysis For the motif analysis, you first need to extract the sequences corresponding to the peaks. There are several ways to do this (as usual…). If you work on a UCSC-supported organism, the easiest is to use RSAT fetch-sequences. Here, we will use Bedtools, as we have the genome of interest at our disposal (Escherichia_coli_K12.fasta). However, we have to index the fasta file first to make it easy to access. Which tool can be used to index the fasta file ? When you search for modules containing the word fasta you find a tool called SAMtools.FastaIndex that can index a reference sequence in fasta format and this is exactly what we need. Use this tool to index the E. coli genome and copy the resulting .fai file to the Files tab (in the same folder as the fasta file). How to extract sequences corresponding to the peaks ? Use the BEDTools.fastaFromBed module for this. The input file is the fasta file of the E. coli genome that you uploaded to the server. The bed file is the bed file with the peaks that was generated by MACS (narrowPeak) Save the resulting .fa file to your computer. To detect transcription factor motifs, you will use the Regulatory Sequence Analysis Tools. It has a specific teaching server recommended for trainings: http://pedagogix-tagc.univ-mrs.fr/rsat/ You will use the program peak-motifs. How to find the peak-motifs program In the left menu, click on NGS ChIP-seq and then click on peak-motifs. A new page opens, with a form The default peak-motifs web form only displays the essential options. There are only two mandatory parameters. Fill the mandatory options The title box, which you will set as FNR Anaerobic . The sequences, that you will upload from your computer, by clicking on the button Choose file, and select the file FNR_anaerobic_combined_peaks.fa from your computer. We will now modify some of the advanced options in order to fine-tune the analysis according to your data set. Fill the advanced options Open the “Reduce peak sequences” title, and make sure the Cut peak sequences: +/- ** option is set to **0 (we wish to analyze our full dataset) Open the “Motif Discovery parameters” title, and check the oligomer sizes 6 and 7 (but not 8). Check “Discover over-represented spaced word pairs [dyad-analysis]” Under “Compare discovered motifs with databases”, remove “JASPAR core vertebrates” and add RegulonDB prokaryotes (2015_08) as the studied organism is the bacteria E. coli. Launch the analysis You can indicate your email address in order to receive notification of the task submission and completion. This is particularly useful because the full analysis may take some time for very large datasets. Click on the button “GO”. As soon as the query has been launched, you should receive an email indicating confirming the task submission, and providing a link to the future result page. The Web page also displays a link, You can already click on this link. The report will be progressively updated during the processing of the workflow."},{"title":"03 Quality control of the data of the ChIP-Seq training","url":"/topics/chip-seq/tutorials/quality-control-chip-seq/tutorial.html","tags":[],"body":"Quality control of the data of the ChIP-Seq training Use FASTQC inside GenePattern to get basic information on the data (read length, number of reads, global quality of the datasets). Read the GenePattern tutorial for more details on how to use GenePattern. The data is already present on the GenePattern server. When you open a tool in GenePattern, you will find the Add Paths or URLs button in the input files section: Click the button and expand BITS trainingdata Chipseq: The fastq file of the control data set is also available in the shared data folder (SRR576938.2.fastq) hands_on Generate and view the FASTQC report of SRR576933.2.fastq in GenePattern Search for FASTQC in the Modules section and open the parameter form. Use the fastq file from the Shared Data folder as input file. Leave the other parameters at their default values. Run FASTQC FASTQC will generate a zip file and a html file. You can open the HTML report in your browser: Click the name of the output file at the bottom of the page. Select Open Link The only parameter you might want to change in if you work on your own data is the contaminants file. It contains a long list of known adapter sequences (see the Documentation in GenePattern). If for some reason the adapters you used are not in the list, you have to provide them as a fasta file. question How many reads does the file contain? Check the answer. This is one of the results of the Basic statistics module in FASTQC (red): Knowing that it is recommended for ChIPSeq to have around 30 million reads, the number of reads in this fastq file seems very low. question Should we be concerned about the low number of reads in the sample? Check the answer. No it’s not a problem because the sample comes from E. coli. This bacterium has a very small genome so 3 million reads will still generate high coverage. However, if this was a human or mouse sample the number of reads would be way too low and we would indeed be concerned. question What is the length of the reads? Check the answer. This is one of the results of the Basic statistics module in FASTQC (green): Again, you see that the data set consists of very short reads although this data set is very recent. This is because it has been shown that elongating the reads does not improve your results in ChIP-Seq analysis. It will just cost you more money. question Are there any positions with low sequence quality? Check the answer. This is shown in the Per base sequence quality module in FASTQC: The overall sequence quality is good, although it drops sharply at the last position, but this is normal in Illumina data, so this feature is not raising hard concerns. question What could be the cause of the failure in the per base sequence content plot? Check the answer. The content of the 4 nucleotides is far from constant over all positions: This typically point the presence of adapter or other contaminating sequences in your reads. question Which FASTQC module allows you to confirm this suspicion? Check the answer. The Overrepresented sequences module will show if your read file is enriched in known contaminants. question What does this module tell you? Check the answer. The Overrepresented sequences module shows a high percentage of adapter sequencess (29% !). Again you see that adapter contamination is a frequently occurring problem of Illumina NGS data. question What about sequence duplication levels? Check the answer. There is sequence duplication. Adapter contamination will be partly responsible for the high duplication levels (the blue peaks at the far right of the plot) but the main cause lies in the technique itself. Typically, after ChIP, you end up with a very small initial amount of DNA (antibodies are not that effective, many cleanup steps in the protocol,…) and you have to do PCR to get your library up to a proper size for sequencing. So naturally, you expect many clones of the same DNA fragment due to the small initial size of the library. Now do the same for the control data set: SRR576938.2.fastq. In theory one expects that regions with high read count in the ChIP sample represent the regions that were enriched by the immunoprecipitation, i.e. the regions that were bound to the protein. However many studies have shown that the read count is affected by many factors, including GC content, mappability, chromatin structure, copy number variations… To account for these biases, a control sample is used consisting of fragmented genomic DNA that was not subjected to immunoprecipitation or that was precipitated using a non-specific antibody. question How many reads does the control data set contain? Check the answer. This is one of the results of the Basic statistics module in FASTQC. You see that the control data set contains the double amount of reads as the ChIP data set. The ChIP and control samples are usually sequenced at different depths, generating files with different total number of reads. This means that these two samples have to be made comparable later on in the analysis by normalization (see ChIP-Seq training). question What is the length of the reads in the control data set? Check the answer. This is one of the results of the Basic statistics module in FASTQC. You see that the control data set contains reads of 36 bases just like the ChIP data set. question Are there any positions with low sequence quality? Check the answer. This is shown in the Per base sequence quality module in FASTQC: The overall sequence quality is good, although it drops sharply at the last position, but this is normal in Illumina data, so this feature is not raising hard concerns. question Why did the per base sequence quality plot raise a failure in the ChIP sample and not in the control? Check the answer. In the slides you can see that the thresholds for a warning are: end of box < 10 median < 25 On the figure you see that the culprit is the median: In the ChIP sample the median Phred score of the last position is 21 (so below 25) raising a failure In the control sample the median Phred score of the last position is 26 (so above 25) question Which FASTQC module gives a failure? Check the answer. The Per tile sequence quality module. The quality of one of the tiles is consistently different from the rest of the tiles question Is this also the case in the ChIP sample? Check the answer. Yes, you see exactly the same problem in the ChIP sample. Since both samples were probably loaded on the same lane, it seems normal that you see the same problem in the ChIP sample. question Why does the Sequence duplication levels modules give a failure in the control sample? Check the answer. The duplication levels in the control data set are high. There are a high number of sequences with low duplication levels which could be due to high coverage. Remember that you are working with E. coli which has a small genome. Estimation of coverage Knowing your organism size is important to evaluate if your data set has sufficient coverage to continue your analyses, e.g. for the human genome (3 Gb), 10 million reads are considered sufficient. question What is the size of the genome of the E. coli K-12 strain substr. MG1655?? Check the answer. Go to the NCBI website Select the Genome database to search in Type Escherichia coli in the search term box Click Search The genome is 4.64 Mbase. The FASTQC report has shown that the fastq files of the ChIP and control sample contain 3.6 and 6.7 million reads respectively. As you aim for 10 million reads for 3 Gb in human, we can assume that these data sets contain enough reads for proper analysis."},{"title":"3 First commit","url":"/topics/git-introduction/tutorials/3_first_commit/tutorial.html","tags":[],"body":"1. Routine usage As mentioned in the first chapter, there are three conceptual areas in Git: the development area, the staging area and the commit repository. The routine usage is depicted in the figure below. When we want to save a file from the development area on our computer to the commit repository, we’ll always have to add it to the staging area first, before we can commit it. The usual routine looks like this: These commands will subsequently add the file to the staging area and then commit it to the commit repository. If we wouldn’t pass along the -m-message parameter, Git would have opened the editor asking to write the commit message there. It’s good practice to write a short, but powerful commit message that helps your future self to determine what has changed in this commit. The last step is to take these commits, essentially representing the folder with all the committed files, and push them to GitHub. Uptil now we kept track of our code locally on our computer. Why do we want to store this project and all of its files on GitHub? Imagine that you lose your computer now, you’ve also lost your project (and all the files in it). A bit less drastical, if you would just like to show your project to your colleagues or with the whole world, we need to publish it somewhere on the internet. And that is exactly what GitHub does for us. Here’s how it looks like (once everything is set) when we would use the appropriate commands on GitHub. git add git commit -m \"some text that explains what has changed\" git push That’s all you need to know: add-commit-push x repeat. This repetition represent 90% of how we interact with Git & GitHub. Before we can start adding, committing and pushing, we have to start a version controlled project/repository. There are two ways of initializing a new Git repository which only has to be performed once right at the start: Clone a GitHub repository (from GitHub): see Section 2 Initialize Git on a folder on your computer: see Section 4 Both options will work just fine and it depends on your preferences or maybe the situation of the project which one is preferable. The first option can be used if you’re about to start a new project, the second option can be used when you already have some files in a project which you now want to start version controlling. 2. Create a new repository from GitHub Go to your GitHub homepage and click on the ‘+’ icon in the upper right corner and select ‘New repository’. The following screen will pop up. We already filled in a repository name and an optional description. You can choose to already publish your repository, however as this is a meaningless repository, we will choose not to. When you’re about to start a new project, there are three things to consider: For a new repository, it’s a good practice to initialize the repository with a README file. This file will eventually include a (general) description about the project, what others can expect to find in the project and how they can use it. Adding an .ignore file is something we will cover later, however for now it suffices to know that the .ignore file will contain some code which tells git to exclude certain files from tracking and avoids uploading them to GitHub. Adding a license makes sense when your project becomes public. It defines under which license the content is made available. More information on licenses is available here. In our case, we will initialize the repository with a README file and click ‘Create repository’, which will then look like this: This is the home page of our GitHub repository. From here we can already do a lot, like changing or uploading files. We initialized a GitHub repository with a README file and we can see that we have only one file in this repository: a README.md file. By default the text in this README file is the title of the repository and the description that we created here above. Notice that it’s a Markdown-file as we can see by the .md extension, similar to an ordinary text file on your computer with a .txt extension. Markdown is enriched text allowing us to create formatted text using plain-text. More information related to markdown can be accessed from the Github guides here. Now that we created the repository in GitHub, we want to work on it on our computer. Therefore we need to download it, i.e. we have to clone it to our computer. Click on the green button ‘Clone’ and choose any of the options: Clone: with https link or with SSH. This will download the repository and all its contents, keeping the link to the GitHub repository. Open with GitHub Desktop (this might be interesting for you at a later stage). Download: will download all of the contents in a zipped file, however loses the connection to the repository. With the Git Bash (or Terminal), navigate with cd to the folder where you want to keep your project folder and type the following: git clone with being the link from GitHub that will look something like this for SSH: git@github.com:username/repository-name.git. This command is only used once in the beginning and creates a new folder on your computer with all the contents from GitHub (the README file). hands_on Exercise 1 Create a new GitHub repository, give it a name and initialize it with a README-file. Upload this file to the repository on GitHub. What is GitHub asking you to do? Which stage is omitted when uploading a file directly to GitHub? Clone the repository to your computer. How many files are there in your local repository? solution Solution Click on upload files and drag the file into the screen. GitHub is asking to add a commit message which defines the changes that you’ll do to your repository. In this case we’ll add the very brief Upload R script message. Notice that there is no staging area when you upload a file directly on GitHub. Click on ‘Commit changes’ and find the two files: README.md and example.R in your repository. Now, we can find the clone link via the green ‘Clone’ button. In our Terminal we type the following command to start using the repository locally on our computer: git clone in which you change to the link that you copied from GitHub. There should be two files in your local repository as well. On a Windows computer we have a folder that contains the following files: 3. Our first commit Our local copy (clone) of the GitHub repository is now able to communicate with the GitHub repository. Every change within this repository is traceable, whether it is a new file or changes to a file. When we make changes in our local repository (e.g. create a new file), you have to add the file to the staging area first (git add) and then commit it (git commit) before pushing it (git push) to GitHub. 3.1 Staging Let’s add a new file to our folder on our computer locally. Download this file and add it in the folder where also the plot1.R file is located. It contains some R code for plotting a new figure. The first thing we will have to do now, is to stage the file into the staging area. Remember that this is an intermediate area before committing the file to the repository. In a next section we will learn why this staging area can be useful. Now we have two options, depending on the situation: git add : will add a specific file to the staging area git add . : will add all the changed or new files to the staging area In this case, we can choose either of both options as we have only added one file. As this is a new file, git add will not only add it to the staging area, but it will also tell Git that it needs to keep track of changes that happen in this file. 3.2 Committing Our new file is now in the staging area, ready to be committed. For this, we have to use the following command: git commit -m \"some descriptive yet short message\" We added a parameter -m (message) to the command followed by a descriptive text. This text informs our future selves or our colleagues of what changes were done. In this case it could be: “added plot2.R script”. We make this message as explanatory as possible, yet as short as possible. Some tips and general best practices in writing commit messages are described in this link. question Question Which of the following commit messages would be most appropriate for a hypothetical commit made to our README.md file? “Update README file” “Added line ‘We use this repository as an example’ to README.md” “Added purpose description to the README file” solution Solution One can argue on the appropriatness of commit messages as it is subjective. In this case however, the third options seems most ideal. It’s both not too generic and not too specific. question Question What has happened after committing? We saved a version of the file which is now visible on GitHub.com We saved a version of the file which is now stored in our commit repository solution Solution We’ve been working locally uptil now and didn’t push the commits to the GitHub repository, hence it’s still in our commit repository. question Question What would have happened if we forgot about the message argument when committing a file (-m) solution Solution If the -m parameter was not added, git will launch a text editor and ask to write a message. We can not make a commit without providing a message. 3.3 Push commits to GitHub Recall that when we added the first file on GitHub (exercise 1), it was immediately committed and showed up right away in the GitHub repository. When we change or add files on our computer and commit them, GitHub doesn’t know this yet. Hence, we have to do one final step: git push Have a look in the GitHub repository and verify that the new file is now in our repository. 3.4 Stage-commit-push We’ve learned how to make a GitHub repository, clone it to our computer, add a file, commit it and push it back to GitHub. This is everything you need to know for a routine usage of Git(Hub) on one of your projects. In order to grasp this concept a bit better, we’ll repeat it by making changes on both files in the next exercise. hands_on Exercise 2 Add a title to both files (“# Title plot 1” and “# Title plot 2”). You can choose how you do this: e.g. open the files in a text editor and add the line on top of the file. Follow the routine steps to push your changes to our GitHub repository, however to make it a bit more difficult, you need to store the changes of both files in separate commits. solution Solution After adding the titles, use the following commands git add plot1.R git commit -m \"Added a title to plot1.R files\" git add plot2.R git commit -m \"Added a title to plot2.R files\" git push We first added the changes of plot1.R in the staging area, then we commit those changes in a given commit. Afterwards, we add the changes of plot2.R in the staging area and subsequently commit them. Finally, we use push to push all the latest commits together towards GitHub. 3.5 Commit all tracked files at once One thing we haven’t really said until now is that Git actually keeps track of the changes that you make to files as soon as you have told Git to do so. The first thing you have to do when you add a new file, is to tell Git to keep track of changes made in this file. If you do not do this, Git will know that there is a new file, but it will classify it as untracked. After adding it to the staging area a first time, it will always keep track of the changes in this file. On the premise that Git is already keeping track of the files, you can simply do git commit -a -m \"some informative text\" in which -a stands for add all changes in all files to the staging area and commit them at once. 4. Create a new repository from your computer As discussed here above, you can also create a Git repository from your computer. This is especially useful when we already have a project with a bunch of files which we now want to start version controlling. The first thing that we will do is initialize Git on this folder. Alternatively, make a new folder which will contain the files of an imaginary project in case you don’t have one yet. In Git Bash (Windows) or in your Terminal (Mac, Linux), move to the project folder with cd and use the following command: git init Unfortunately, it is not possible to create a GitHub repository from our computer. Hence, we need to open GitHub and create a new repository and DO NOT initialize it with a README.md, .gitignore or a license. It is important that it is empty in the beginning. We can add those files later. Once created, GitHub will seggest commands that you might want to use on the Terminal to push our first changes to this GitHub repository. We already initialized Git in our folder, so we can skip this step: git init THe following steps basically ask us to commit our first changes. Given that we edited the README file: git add README.md git commit -m \"first commit\" Here comes the tricky part. We will learn about branches in Chapter 5, however it suffises for now to understand that each branch carries a name and the default one is now called main where it earlier was called master. The following command will overwrite the name of the branch to main. git branch -M main Then, we need to link the repository on our computer to the one on GitHub with: git remote add origin git@github.com:tmuylder/testtt.git And finally push our commit to GitHub. The argument -u or --set-upstream will set the remote as upstream (see later): git push -u origin main question Question What if we want to create a new folder inside the folder which we are using for version controlling? Do we need to initialize Git inside this subfolder as well? solution Solution It is important to note that git init will keep track of all the subdirectories and their files that are in the folder. Thus, you don’t need to create a git repository for each folder or subdirectory. Git repositories can interfere with each other if they are “nested”: the outer repository will try to version-control the inner repository. This will lead to errors. question Question How can we know whether a folder is already initialized with Git, meaning that we are already version controlling the project? solution Solution If we use ls -al we get a list of all files and directories, including the hidden ones. A .git folder is present when the project is being version controlled. Git uses this special directory to store all the information about the project like the history of all commits. If we ever delete the .git sub-directory, we will lose the project’s history. Another possibility is to use the git status command which results in fatal: not a git repository… if the project is not being version controlled. Before starting with the next exercise we also want to stress the importance of not uploading data to GitHub. It’s good practice to have links to data, however not the data itself. GitHub is not your next cloud storage instance. hands_on Exercise 3 Find a folder on your computer with some files that you want to version control, initialize Git on that folder and make it (privately) available on GitHub. solution Solution See the steps in Section 4. 5. The strength of the staging area Now you’re probably wondering why it’s useful to have that many steps to save a file (add, commit, push). We will give a practical example based on the figure below: Imagine that you’re working on a project with multiple Python scripts and you’re working on all of them. In this case your folder in your development area contains the files scriptA.py, scriptB.py and scriptC.py. The changes that you made in script A and script C are somehow related, but script B is not. It’s good practice to make commits in which changes that are related to each other are bundled. Hence, in this case we want to make one commit with the changes from file A and C. Now we can simply add scripts A and C to the staging area and commit it. The changes in script B will remain unsaved until we commit the changes in a separate commit. It’s always better to have more commits; in case you want to remove part of your work in a later stage, or you want to start your work again from a specific commit. 6. Pull Imagine that you change something in a file on GitHub, or upload a new file online via GitHub. We would want to include these changes or that file in the folder on our computer as well. For this we need to use the pull command to pull in the changes from GitHub. Let’s go back to our repository on GitHub. We will make a change in the repository on GitHub and then pull these changes back into the repository on our computer (i.e. the project folder on our computer). Click on the README.md file in the list of files and click the pencil icon on the upper right. The file will open in an editor mode and we can change the title from introduction-github to Introduction GitHub or we can add some more descriptive text. Note that a README file is by default a markdown-file. Markdown is a text file with lay-outing options. If you haven’t heard of it before, it’s worth some further reading. Save the changes by committing them as depicted here below: GitHub is now one commit ahead of our local repository. Hence, we have to pull this commit into our local repository. We can do this by using the following command: git pull Open the file README.md and check whether the changes have merged in. Let’s go to the next session!"},{"title":"02 GIMP","url":"/topics/gimp-inkscape/tutorials/gimp/tutorial.html","tags":[],"body":"What is GIMP? GIMP is short for GNU Image Manipulation Program. It is a free and Open-source, cross-platform image editor available for GNU/Linux, MacOS and Windows operating systems. During this training we will use GIMP 2.10 on Windows. To download the most recent version for your OS, browse to the GIMP Download page. External training material GIMP Manual page. GIMP 2.10 Basics on YouTube Nick Saporito GIMP Tutorials User Interface GIMP has a ‘Single-window’ mode, this allows you to switch from multiple windows (for e.g. multiple monitors) to a single window. When the ‘Single-window’ mode is disabled, you have separate windows for toolboxes, view area and dockable dialogs. When enabled you have one window with all tools, options and dockable dialogs attached to the central view area. For beginners, we would advise the ‘Single-window’ enabled. On the left panel you have the ‘Toolbox’ (if not present: Windows - Toolbox or press Ctrl + B) and underneath the ‘Tool Options’ dialog. Selecting a tool will result in a different Tool Option bar. Every tool has his own set of parameters and functions, best to keep them close to each other. On the right-hand panel you can find other ‘dockable dialogs’. These are easy to move, remove and re-introduce if necessary. To get a list of all ‘dockable dialog’ go to Windows – Dockable Dialogs - … . If you want a full screen view of your image select Windows – Hide Docks. Import data and image properties To import an image: File – Open When you select an image (any file type) in the import window, you get a preview and information on the right side. Click Open and the image(s) will be displayed in the middle box at zoom level 100% (1 pixel image = 1 pixel screen) or fitted to your windows. To zoom use Ctrl + mouse scroll up or down. Multiple images in GIMP are displayed in different tabs on top of the View Area. Before you export your image, make sure it has the right resolution and pixel density. Image - Image Properties will give you all the information your image holds. This information can be very useful when you open an image from an unknown source. Selection Rectangular selection has several options and shortcut keys. The first icons in the tool options are the selection modes: add to selection (Shift), subtract from selection (Ctrl) and intersect with selection (Shift+Ctrl). More options are: feathering edges, rounding of the corners, expand from center, lock aspect ratio, size and position and if necessary to highlight the selection). The Ellipse selection tool has more or less the same options. There are other selection tools available: Free Selection, Select by Color, Fuzzy Selection, Scissor Selection, Foreground Selection. Those tools have different tool options and are only used in specific cases. Transforming There are several ways to transform your image or selection; rotating, scaling, shearing and flipping. You can transform a selection, a layer or the image. When using the rotation tool, you have several options in the dockable dialog below. An important option is “Clipping” this will change the aspect ratio of your image after rotating. Another way of rotating an entire image is: Image – Transform – … then you have the option to flip (horizontal/vertical) or rotate (90°/180°). The entire image will be rotated including the selection and image orientation. Layers Make sure you have the dockable dialog ‘Layers’ in your window. All options for layers can be found in the menu bar “Layer”. You can make a new blank layer or duplicate the current layer (e.g. copy of original image to compare or as back-up). In the dockable dialog you can hide or show a layer (eye button), rename them or move them up and down in the layer stack. If you want to link/connect two or more layers, you can use the chain button (next to the eye button). To copy a selection to a new layer, perform a regular copy/past action of that selection (Ctrl+C and then Ctrl+V) and select Layer - To New Layer If you want to merge all layers into one layer you can select Image – Merge Visible Layers. Brightness and contrast In the menu bar you can find Colors . This menu has multiple option to manipulate your image; Color Balance will change the cyan, magenta and yellow color levels of your image Brightness and Contrast will change brightness and contrast and you can save these settings as a favorite Threshold will reduce your image to two colors by using a threshold value Adjust color curve will change the gamma setting of your image Posterize will change the number of colors (2-256) Guides and cropping You can split your image in different sub-images. This can be done by using ‘Guides’. To create such a break-line, go to Image - Guides - New Guide… or (by Percent)…. You can create a horizontal or vertical guide at the value/percentage you enter. A guide will be displayed as a blue dashed line. To chop your image in multiple parts, go to Filters- Web- Slice (Older versions: Image - Transform - Guillotine). The sub-images will be generates in the folder you selected. If you only want a selection of your image without all the rest you can crop by clicking Image – Crop to Selection or use the Crop tool from the Toolbox. Scaling and print size When you want to scale your image to a smaller resolution you can select Image – Scale Image. There you can scale in pixels (or another unit) and you can lock the aspect ratio (chain symbols). If you want to change the print size to make your image suitable for publication you can select Image - Print Size…. There you can change the dimension/resolution and pixel density of your image. Remove background color If you download an image of a company or university logo, it might have a white (or any other color) background. This can be very annoying when the destination background is different. In order to remove the background color, we first have to add an alpha channel to this image: Layer - Transparency - Add Alpha Channel - If the Alpha channel is already present, skip this step. Now you’re able to get a transparent background using the option: Image - Color to Alpha. In the new window you can select the color which you would like to convert to transparent pixels. You can either select by clicking the color bar or use the color picker icon. Exporting Select File – Export as… If you click on the ‘+’ next to Select File Type, you have a list of all possible extensions in which you can export your image. Each of those file formats has different compression options. Exercises on image manipulations in GIMP hands_on Hands-on: Exercise 1 Source file: [http://data.bits.vib.be/pub/trainingen/GIMP_Inkscape/original_file.tif Image 1] Task: Split this image in 2 parts, one for each gel. Make sure the band are horizontal and export the 2 new images in the same file format as the original. You can adjust brightness and contrast to make all the band more visible. hands_on Hands-on: Exercise 2 Source file: [http://data.bits.vib.be/pub/trainingen/GIMP_Inkscape/Exercise1.1.jpg Image 2] Task: Rotate this image 45 degrees and crop an image of 500x500 pixels out of the original. Make sure the printing resolution is set to 300 ppi and export this image as a PNG file. Adjust brightness and contrast to make this image look better. hands_on Hands-on: Exercise 3 Source file: [http://data.bits.vib.be/pub/trainingen/GIMP_Inkscape/Exercise1.2.jpg Image 3] Task: Cut this image in 4 equal parts. Know that the printing width is 150 mm and the journal demands a minimum op 300 ppi for all 4 images. Also export each of them in a different file formats without losing image quality. Adjust brightness and contrast to your own opinion. hands_on Hands-on: Exercise 4 Source file: [http://data.bits.vib.be/pub/trainingen/GIMP_Inkscape/Exercise1.3.jpg Image 4] Task: Adjust brightness and contract of this images and export it in a way to make the file as small as possible. Use preferably lossless compression (try lossy compression to compare file size), there is no restriction on file formats. Be sure your image is exported with at least 300 ppi. hands_on Hands-on: Exercise 5 Source file: select from the internet Task: Download an image from your most favorite brand and remove the white (or other color) background. Export this new image in a format that support transparent pixels."},{"title":"06 Protein-ligand interaction - from small molecule to protein","url":"/topics/protein-structure-analysis/tutorials/protein-ligand-interaction/tutorial.html","tags":[],"body":"Introduction The goal of this exercise is appreciate how protein interactions can be studied through visual inspection and other software tools. Protein interactions can be classified into different groups regarding the molecular properties and functions of the interacting partners. (These groups are intertwined in several cases.) Some examples include: the interactions of proteins with other proteins, small molecules, carbohydrates, lipids or nucleic acids; Receptor-ligand interactions; Antigen-antibody interactions; Enzymatic interactions, enzyme-inhibitor interactions. Exploring the structure of a nanobody-stabilized active state of the β2 adrenoceptor - the ligand We will start with exploring one crystal structure of the β2 adrenoceptor. Together with the Steyaert lab from VIB, Kobilka published several crystal structures of the β2 adrenoceptor in its various activation states (Rasmussen et al. Nature 2011, 477) hands_on Get the structure Download the crystal structure 3P0G from the PDB into YASARA. File - Load - PDB file from internet As you can immediately appreciate, it is a bigger crystal structure with more than one molecule. question Questions How many molecules are present in the crystallized structures? And how many chain identifiers are used? solution Solution There are three molecules, chain A Beta-2 adrenergic receptor; Endolysin, chain B Camelid Antibody Fragment, and a small molecule ligand. Also have a look at PDBe 3P0G which gives a very nice overview of the structure and its composition. Only two chain identifiers A and B. Sometimes, this leads to issues depending on the software you might want to use for downstream processing. Some software routines need seperate chain identifiers for molecular entities to work correctly, so we suggest to rename the small molecule to chain L. hands_on Activate the Head-up display Select Rename Enter ‘L’ to proceed with the renaming. We first have a look whether we can find out if there are specific interactions of the small molecule ligand with the adrenoreceptor. In order to do so, we first have to add Hydrogens to all present molecules. hands_on Edit - Add - hydrogens to : All Change the display of the ligand to Sticks Select the amino acids of the binding pocket i.e. a sphere of 10 Angstrom around the ligand Select – in sphere around – Residue and drag with the mouse until the display says 10 Å View – show interactions – hydrogen bonds of - Residues select ‘Selected’ in the panel Belongs to or has and press OK in the subsequent window Given that hydrogen bonding is dependent on the definition of a hydrogen bond in the program, it is not a bad idea to use other tools to compare the analysis. There are many options to do this online if you look at published crystal structures. Next to the tools which are directly linked out from the web site of the crystal structure at the PDB database you can use the ProteinPlus server Go to the web site of ProteinPlus and enter the PDB code 3P0G into the search box. After clicking on Go, you should be presented with on overview of tools the ProteinPlus server provides. We do not go into great detail on all the tools but only mention PoseView. With this tool, you can prepare an automatic sketch of the small molecule-protein interactions. Figure 1: Overview of 3P0G Figure 2: Zoom on ligand co-crystallized with 3P0G question Questions Between which amino acids and the ligand do you see hydrogen bonds using YASARA? According to PoseView, between which amino acids and the ligand do you see hydrogen bonds? What other interactions are presented in the sketch? Inspect the visualisation in Yasara: Do you see the interactions in Yasara as well? solution Solution In YASARA, you observe hydrogen bonds between Asp113A as well as the carbonyl function of Asn312A and the charged amine function. PoseView indicates hydrogen bonds between Asp113A as well as the carbonyl function of Asn312A and the charged amine function. Furthermore, hydrogen bonds are indicated between the phenolic OH and Ser207A and Ser203A as well as the amine function and Ser203A. Furthermore, hydrophobic interactions are indicated for the methylbenzyl moiety and pi-pi interactions of Phe290A and the phenolic moiety. With YASARA Structure license, those hydrophobic interactions can also be visualised. Exploring the structure of a nanobody-stabilized active state of the β2 adrenoceptor - the nanobody In order to estimate the binding energy between the nanobody and the β2 adrenoceptor, we can use the FoldX tool AnalyseComplex. It is recommended to calculate these binding energies on energy-minimized structures. To illustrate the effect of the energy minimization, we compare the interaction energy of the current crystal structure and its minimized structure. Use the tool FoldX tool AnalyseComplex hands_on Given that energy-minimization takes a while for this rather large complex, please download the Yasara scene here Calculate the interaction energies between the chain A and B of the object 3P0G and the RepairObj1, respectively. Analyze - FoldX - Interaction energy of molecules question Questions What is the dG in the two cases? Any idea why the difference is rather hugh? solution Solution first case (X-ray structure): Interaction energy between molecule(s) A and B in object 1 = -9.86 (kcal/mol) second case: Interaction energy between molecule(s) A and B in object 2 = -20.19 (kcal/mol) Through the energy minimisation of the Repair Object function, the interactions of the amino acids are optimised. This command also creates a list of residues forming the interface of the two proteins. Hit space to see the list of residues in the interface. Tip: This list can also be useful if you want to make visualisations of the interaction site. Plugin>interface residues between A and B Plugin>TA66 TA68 IA72 IA127 RA131 AA134 IA135 TA136 SA137 PA138 FA139 KA140 QA142 YA219 VA222 EA225 AA226 LA266 KA267 EA268 AA271 LA272 TA274 LA275 IA278 IA325 YA326 RA328 SA329 PA330 SB27 IB28 FB29 SB30 IB31 TB33 AB50 IB51 eB52 SB56 TB57 NB58 YB100 AB102 VB103 LB104 YB105 EB106 YB107 Comparing the active and the inactive conformation of the β2 adrenoceptor In case, there is still time, I would recommend to try to use some of your capabilities you learned today and create a superposition of the inactive and active conformation of the β2 adrenoceptor. We take one of the crystal structures which are available: 3SN6 File - Load - PDB file from Internet You will be kind of overwhelmed once the structure is loaded into YASARA. In order to get a first quick overview, click on the ‘Center’ buttom in the menu of YASARA (5th buttom from the left). Then, it is time to look at the PDB entry of 3SN6 in the PDB database to have a first idea on what molecules are in the PDB file. As you see on the website 3SN6, the chain R consists of 2 molecules, the β2 adrenoceptor and lysozyme. In the corresponding article, it is stated that ‘the unstructured amino terminus of the β2AR is replaced with T4 lysozyme (T4L)’. Since this is an extra molecule in the crystal structure which disturbes our view, we will delete it. After the manipulation, the overall picture should look roughly like this. Figure 3: Overview of 3SN6 without lysozyme In the following step, we structurally align only the receptors. The rest of the structures will move along. It is suggested to use the first chain A from 3P0G as target. In order to do a structural alignment, it is suggested to use the first chain A from 3P0G as target. Analyze - Align - Pairwise, based on structure - Molecules with MUSTANG Investigate the differences in TM helices and the binding of the nanobody compared to the subunit of the G protein. Tip: Color the secondary structures to better identify the individual chains/units of G protein. Conclusion Sum up the tutorial and the key takeaways here. We encourage adding an overview image of the pipeline used."},{"title":"01 Exploring the Protein Data Bank exercises","url":"/topics/protein-structure-analysis/tutorials/explore-pdb/tutorial.html","tags":[],"body":"Search for a structure Via UniProt The way of searching for a specific protein structure depends on the data you already have. You might already have the PDB ID (a unique identifier), that’s an easy one. But mostly you have the protein name or you just have a sequence. In the last cases I recommend to start from the UniProt website at http://www.uniprot.org, which is the best annotated protein database in the world. Our first model protein will be the molecular chaperone DnaK from E. coli. Below is an image of the UniProt search box where you can start your search for proteins. Figure 1: Search box hands_on Explore a PDB structure on the Uniprot web site Go to the UniProt website and search for the DnaK protein The UniProt search engine returns a list of DnaK protein sequences from a variety of organisms. An entry with accession code P0A6Y8 and entry name DNAK_ECOLI should be near the top of this list. Click on the accession code (column Entry) to view the protein page of this DnaK from the model organism Escherichia coli. Click on Structure in the left-side menu and then look at the 3D structure databases table. question Guidelines which PDB structures to select Which structures (give the 4-character PDB ID) of the C-terminal domain of DnaK should preferentially be use for analysis and why? solution Solution Usually, the recommended selection criteria are using an X-ray structure with low resolution and low $R_{free}$ factor. Furthermore, the PDB database has pre-calculated a validation report for all of the structures. As an example, have a look at https://www.ebi.ac.uk/pdbe/entry/pdb/4EZX under the section ‘Experiments and Validation’. For many PDB structures, there is also a re-done structure available with a vast amount of information on the quality of the X-ray structure and suggested ‘better’ models e.g. (https://pdb-redo.eu/db/4ezx). In our case, we could opt for the structures 1DKX and 4EZX. This is a difficult example since there are so many high resolution structures available. So, it is recommended to study the articles and compare the available structures to find your favorite structure for further analysis. Via the Protein Data Bank by PDB ID You can find structural information directly at the PDB database. The web site of the PDB consortium is located at http://www.wwpdb.org. This web site provides links to all members of the PDB (left side). It is a question of taste which resource you start off with. For X-ray structures, it is currently PDBe, RCSB PDB, PDBj. For NMR structres, you find the BMRB. In today’s course, we focus on the PDB resources only. Below is an image of the RCSB search box http://www.rcsb.org/pdb/home/home.do where you can start your search for structures. Figure 2: PDB Search Box The PDB file with ID 1DKX contains the atomic coordinates of the molecular chaperone (DnaK) from E. coli. hands_on Search a structure on the RCSB web site Go to the PDB website and type 1DKX in the search box This will lead you to the same page we got earlier through UniProt. Via the Protein Data Bank by sequence In lots of cases we only have a sequence of which we would like to find out if there is structural information. The PDB can be searched using a sequence as input. Here is the sequence of the C-terminal substrate binding domain of DnaK: DVKDVLLLDVTPLSLGIETMGGVMTTLIAKNTTIPTKHSQVFSTAEDNQSAVTIHVLQGE RKRAADNKSLGQFNLDGINPAPRGMPQIEVTFDIDADGILHVSAKDKNSGKEQKITIKAS SGLNEDEIQKMVRDAEANAEADRKFEELVQTRNQGDHLLHSTRKQVEEAGDKLPADDKTA IESALTALETALKGEDKAAIEAKMQELAQVSQKLMEIAQQQHAQQQTAGADASANNAKDD DVVDAEFEEVKDKK The PDB allows sequence searches through the same search box we used before. Figure 3: PDB Search Box There is also an Advanced Search section, with a Blast/Fasta option in the Sequence Features section. Figure 4: BLAST hands_on Hands-on: BLAST search for PDB structure Go to the Advanced Search section Please select ‘Sequence BLAST/PSI-BLAST’ in the Query type drop down. This method allows you to change some parameters for the search. Copy and paste the sequence in the ‘‘Sequence’’ field Press ‘‘Submit query’’. You should see the same structures popping up as you saw in the UniProt page of DnaK. The PDB file Introduction A PDB (Protein Data Bank) file is a plain text file that contains the atom coordinates of a solved 3D structure of a protein or even DNA. Such coordinate files can be obtained at the Protein Data Bank at http://www.rcsb.org/pdb. Each PDB file has a unique identifier (ID) consisting of 4 characters, the first one is always a number. Note: It has been announced that the 4 character code will change in the future https://www.wwpdb.org/news/news?year=2017\\#5910c8d8d3b1d333029d4ea8. The PDB file with ID 1DKX contains the atomic coordinates of the molecular chaperone (DnaK) from E coli. hands_on Hands-on: BLAST search for PDB structure Go to the PDB website at http://www.rcsb.org/pdb Type 1DKX in the search and try to answer the following questions. question Questions How many molecules were solved in this PDB file? What kind of molecules are these (proteins, peptides, DNA, …)? Does the structure represent the full protein? If not, how many residues are missing? Hint: Click on the UniProt KB link in the Sequence tab to see the full sequence. Was this structure solved by X-Ray or NMR? What is the atomic resolution and R-factor? solution Solution Two, called polymers or chains: they are polypeptides To answer this question you can go to the sequence tab at the top: Summary: a large chunk of the N-terminus is missing from the structure, the C-terminus is virtually complete. X-RAY diffraction, as shown by Experimental Details Atomic resolution: 2.00 Ångstrom and R-factor of 0.206 Downloading the structure The file that holds the 3D coordinates can be downloaded by clicking on Download files in the top right corner and then choosing PDB file (text). For convenience, save this file on your desktop. The filename is the 4-character unique PDB ID. hands_on Hands-on: Open downloaded PDB file in an editor Open this file with a text editor, e.g. WordPad is an excellent tool for that. Do you see the different sections in the PDB file? Analyse some ATOM lines and try to explain what kind of data is in each column. Additional exercises on searching PDB can be found on the basic bioinformatics exercises page."},{"title":"02 Visualize a structure","url":"/topics/protein-structure-analysis/tutorials/visualise-structures/tutorial.html","tags":[],"body":"Install Python and PovRay Python and PovRay should be installed already, so you can skip this part. The programming language Python must be installed to use some very useful YASARA features. Simply start YASARA as administrator. Right click the YASARA icon on the desktop and choose “Run as administrator”. Once the program is opened, click Help > Install program > Python PovRay is used to make high quality publication-ready images and should be downloaded first with: Help > Install program > PovRay Tutorial movie Play the movie “Working with YASARA”: Help > Play help movie > General: Working with YASARA Scene styles Open the PDB with code 1TRZ in YASARA. File > Load > PDB file from Internet If this option is not there, it means you haven’t installed Python yet. Please check above. The molecule will be loaded and presented in the ball style. Different scene styles exist to rapidly change the view: F1: Ball F2: Ball & Stick F3: Stick F4: C-alpha F5: Tube F6: Ribbon F7: Cartoon F8: Toggle sidechains on/off (press multiple times and see what happens) Be careful! If you have just made a nice close-up of e.g. an active site where you show some residues and hide others, and put some atoms in balls while others are in sticks, you will lose everything when you press one of the F-keys!!! The F-keys change the viewing style without asking. Try all the different scene styles! Showing and hiding residues The function keys F1-F3 show all atoms and residues by default. The keys F4-F7 do not explicitly show atoms and residues but are merely a impressionistic representation of the structure. The F8 keys does, to a certain extent, show atoms, but only of side chains, not main chain atoms. Mostly to do structure analysis, we want to show only the most interesting residues, the ones we want to analyze, and hide all the others. The structure of insulin was crystallized together with some water molecules. In many cases, it is no problem to permanently delete those waters. To visualize the waters, select an atom view such as F1, F2 or F3. See the red water (oxygen) atoms floating around the surface? Edit > Delete > Waters Then select the base scene style without any explicit atoms, e.g. tube style (F5). Press F5. This is our representation of the backbone. There are several ways to show the residues of interest: From the menu View > Show atoms in > Residue Select Cys7 from Molecule A and press OK From the sequence selector Hover the mouse on the bottom of the screen, you will see the sequence selector opening. Open it permanently by pressing the blue nailpin on the left side of it. Search for Cys7 from Molecule B, right-click and select: Show > Residue Now show the atoms of His5 in Molecule B using a method of choice. And now that we’re on it, what is special about the two cysteines we just visualized? Hiding individual atoms or residues works in the same way as showing them, only now you should go to Hide atoms in the menus. Showing and hiding secondary structure Most published molecular images show a detailed active site and all the rest is hidden for clarity. From the previous exercise we show the atoms of 3 residues (let’s assume this is our active site). Now secondary structure of the rest of the molecule is also still visible. To hide all that, we do not have to hide atoms, but hide the secondary structure (the F5 tube view) from the rest of the structure. Atoms and residues in YASARA are not the same as the term ‘secondary structure’. Atoms and residues are balls and sticks, ‘secondary structure’ is an artistic impression of the structure (beta sheet arrows, helix ribbons, …). If you get this concept, you are a YASARA master. So let’s hide many of the secondary structure, but keep just a few stretches around our active site. Our active site is Cys7 (A), Cys7 (B) and His 5 (B). This can be done in several ways. Since we would have to hide almost everything, I propose to hide first everything and then show again those stretches that we want. But if you have a better idea, I would like to hear it. Hide all secondary structure: View > Hide secondary structure of > All Then show stretches of residues 2-10 in Mol B and residues 4-10 in Mol A in tube view as: View > Show secondary structure > Tube through > Residue Then select the correct stretches of residues by keeping the CTRL key pressed to select multiple residues. There are still some metal-bound histidines flying around that weren’t hidden because they are metal bound (a YASARA specific thing). Hide those histidines by clicking on one of the sidechain atoms, then right-click and select: Hide atoms > Residue The nasty dative bonds and metals can be removed simply by deleting all of them: Edit > Delete > Residue > Name In the name column select all the metals and ions you can find. Et voilà, a publication ready image! Figure 1: Insuline Labels You can put labels on the residues you want to highlight by going to the main menu or selecting an atom from a residue and right-click. In the latter case you select: Label > Residue Note that residue name and residue number is automatically selected. Change the height to 0.5 or so and select a nice color for the label. Presto! Colors You can color on all levels: atoms, residues, molecules and objects. So be careful, if you color a residue, all of its atoms will get that color. If you color a molecule, all atoms in that molecule will get that color. Let’s color the secondary structure (the backbone in our case) of our active site in orange. But the sidechains should keep their Element colors. So we shouldn’t color entire residues, but only a selected atom set. Therefore our selection will be at the atom level, not the residue level. Go to: View > Color > Atom > Belongs to or has > Backbone Then select the orange color (color code 150) and select ‘Apply unique color’. Hopefully, it is a satisfying result. Saving all the beautiful work It would be a pitty that you spent hours creating fancy molecular graphics for that next Nature paper while you can’t continue on the work the next day. That’s why YASARA can save the entire Scene including orientations, colors, views, everything. To save the current scene, go to: File > Save as > YASARA Scene Choose a filename such as MyInsulin.sce To load the work again in YASARA go to: File > Load > YASARA Scene Careful: loading a Scene will erase everything else! Creating high quality images To save the current view to a high quality publication ready image file, go to: File > Save as > Ray-traced hires screenshot This requires that the PovRay program has been installed. See the first item on this page. Usually, you prefer to have a transparent background, so check the respective box. Distances Distances between atoms are calculated as follows: select the first atom keep CTRL pressed and select the second atom. left of the screen indicates the ‘Marked Distance’ in Angstrom. question Questions What is the distance between the C-alpha (CA) atoms of Tyr19 and Leu16? solution Solution To solve the question you need to select a view that shows you atoms including C-alphas. Possible views or scene styles that show these atoms can be F1 (ball), F2 (stick), F3 (ball\\&stick) and F4 (C-alpha). The views F5-F8 won’t show you any CA’s explicitly. Try it. So you’ve probably noticed that pressing the CTRL button allows you to select multiple atoms. This is important for the next exercise. The distance is 5.8 Ångstrom. Hydrogen bonds To show hydrogen bonds, YASARA needs the actual hydrogens to be present. In NMR structures these are normally there. But in X-Ray structures hydrogens are missing. Luckily YASARA can add the hydrogens for you. Select tube view (F5) and toggle on the sidechains with F8. Add hydrogens with: Edit > Add > Hydrogens to all Then show the hydrogen-bonds: View > Show interactions > Hydrogen bonds of> All > OK If the view is to chaotic for you, toggle off the sidechains with F8 (press untill the sidechains are hidden). question Questions Do you see the typical helix and beta sheet pattern? Arg22 from Molecule/Chain B is making an hydrogen bonded electrostatic interaction (salt bridge) with another residue. Which residue? solution Solution The interaction partner is Glu17 from chain A. To remove the hydrogen bonds, you have multiple choices: View > Hide hydrogen bonds of > All or just delete all hydrogens (this will also delete all hydrogen bonds): Edit > Delete > Hydrogens Surfaces It can be very useful and informative to show the molecular surface of a protein. you can visualize cavities, ligand binding sites, etc … To show the molecular surface of one monomer of dimeric insulin, go to: View > Show surface of > Molecule Select in the Name column A and B (these are the two chains in 1 subunit). Press Continue with surface color and make sure Alpha is Any number lower than 100 will create transparency in the surface (could be nice as well). Molecular graphics exercise Try to reproduce the following image of the 1TRZ insulin structure (hints below): Figure 2: Insuline Hints: choose the proper secondary structure scene style (F6 was used here) find the correct orientation first color all backbone atoms in gray find the residue numbers of the 2 colored helices color those residues magenta show the sidechain atoms and the CA of the two histidines and the glutamate color the sidechain atoms of all residues in the Element color label the histidines and the glutamate if you need some help how to change the parameters for the label, please have a look at Help -> Show user manual and search in Commands / Index More coloring Download GroEL via PDB code 1WE3 in YASARA. Try to reproduce (approximately) the following image (hints below): Figure 3: GroEL Hints: load the PDB as File > Load > PDB file from internet zoom out and find the correct orientation delete the ADP, DMS and Mg molecules (are treated as residues in YASARA). So Edit > Delete > Residue > Adp … color by molecule (every molecule will get another color) and color by gradient (now you need to specify 2 colors, the begin and end color). choose a first color (eg. color with code 0) choose a second color (eg. color with code 300, so you go over the entire color wheel spectrum) More exercises can be found on the basic bioinformatics exercises page. Conclusion Now, you have explored the YASARA interface and acquainted with basic visualisations. You have identified how you can visualise secondary structure elements, surfaces, and hydrogen bonds. And most importantly, you can create publication-ready figures using Yasara."},{"title":"05 Predicting a protein structure - homology modelling","url":"/topics/protein-structure-analysis/tutorials/homology-modeling/tutorial.html","tags":[],"body":"Introduction The goal of homology modeling is to predict the 3D structure of a protein that comes close to what would be achieved experimentally with X-Ray experiments. Main principles of homology modeling We predict the structure of a protein sequence on the basis of the structure of another protein with a similar sequence (the template) If the sequences are similar, the structures will have a similar fold Structure is more conserved than sequence Main ingredients for homology modelling The sequence Last week my colleague sequenced a plant protein. He is not a bioinformatician. Yet, he would like to know what the structure might look like to do some rounds of rational mutagenesis. Let’s try to address the problem for him. He came up with this sequence: SVCCPSLVARTNYNVCRLPGTEAALCATFTGCIIIPGATCGGDYAN Searching for the template structure Actually, the first step is to check whether the PDB already contains the structure of this sequence. That would be easy so we don’t have to model anything. We will use Blast again to search with the sequence. hands_on Hands-on: BLAST search for PDB structure Go to the Advanced Search section Please select ‘Sequence BLAST/PSI-BLAST’ in the Query type drop down. This method allows you to change some parameters for the search. Copy and paste the sequence in the ‘‘Sequence’’ field Press ‘‘Submit query’’. You should see the same structures popping up as you saw in the UniProt page of DnaK. A suitable template structure to make a high quality model should have following properties: The highest possible sequence identity from all structures in the PDB when aligned to the target sequence A good resolution (and R-factor): if many identical template structures exist with the same sequence, filter by resolution Is solved by X-RAY, not NMR. question Questions Is there a difference in the number of identities, positives and gaps between the two remaining x-ray structures? What is the PDB ID with the highest resolution, does not have insertions or deletions and should thus be the better template structure? solution Solution TODO TODO Aligning target and template sequence and template selection The alignment is the most crucial part of homology modeling. We will not explain what an alignment is and how you make it, this should be known. In an alignment, we put homologous sequences on top of each other in a text file. The point is that amino acids that are on top of each other in the same column are assumed to have the equivalent position in the 3D structure. So if the template sequence has an Ala at position 3, where the target sequence has a Phe, homology modelling tools will use the backbone of the template structure and replace the sidechain at position 3 from Ala to Phe. Homology modelling evolved over the years and many online tools for homology modelling are available. In my experience, homology modelling can be rather difficult and needs expert knowledge depending on the actual situation (sequence conservation, available templates, etc.). Can you imagine what could be the reasons? Building the homology model with Swiss Model Our current request for homology modelling is a rather safe one, so we can use an automatic server for homology modelling. There are many automatic tools available and many of them compete in regular competitions like lastly, the 12th Community Wide Experiment on the Critical Assessment of Techniques for Protein Structure Prediction (CASP12) - [1]. In our example, we take the Swiss Model server. SWISS-MODEL is a fully automated protein structure homology-modelling server, accessible via the ExPASy web server, or from the program DeepView (Swiss Pdb-Viewer). The purpose of this server is to make Protein Modelling accessible to all biochemists and molecular biologists worldwide. hands_on Hands-on: Template selection step with Swiss Model Browse to the Swiss Model server On the first page, paste the sequence of our unknown protein in the field ‘Target Sequence’ and give the project a name. Figure 1: Start page of Swiss Model Click ‘Search templates’ to initiate the first step. Thereafter, the server identifies structural template(s) and gives an overview list of hits which you can select the templates from. question Question Which of the 10 (at the time of writing) possible template structures would you select as template for the model building process? solution Solution We suggest as template 1jxx.1.A given that it is an X-ray structure with high resolution and a very high sequence identity (X-ray, 0.9 Å, 78.26 %). hands_on Hands-on: Model Building Step and Visualisation Once you have selected the template, hit ‘Build Model’ to start the homology modelling procedure. The server will alignment of target sequence and template structure(s), build a model and evaluate it. These steps require specialized software and integrate up-to-date protein sequence and structure databases. Each of the above steps can be repeated interactively until a satisfying modelling result is achieved. Once the model has been built, you can download it. If the Swiss Model server is too busy at the moment you execute the request, you can download the model from here. Load the created model into YASARA. Perform a structural alignment with your reference e.g. 1CRN and try to detect the differences through manipulating the visualisations. Conclusion Homology modelling evolved over the years and many online tools for homology modelling are available. You have used the Swiss Model service with a reasonable simple modelling request. Often, in research projects, homology modelling can be rather difficult and needs expert knowledge depending on the actual situation (sequence conservation, available templates, etc.)."},{"title":"04 Predicting the effect of a mutation on a protein structure","url":"/topics/protein-structure-analysis/tutorials/mutate-structure/tutorial.html","tags":[],"body":"Introduction Mutations in proteins can have various origins. Natural occurring mutations are random and can have any kind of effect on the protein structure and/or function. Mutations can have no effect at all, be stabilizing of destabilizing. In the last two cases, these can lead to diseases. But we can also make mutations in the wet lab to study the effect of a single residue position on protein stability, interaction with a peptide ligand etc … Such site-directed mutagenesis in the wet lab is hard labour and costs money, I don’t have to explain that to you. So wouldn’t it be easier, cheaper and more rational if you could predict the effect of some mutations first with bioinformatics and then test the really interesting ones in the lab? FoldX is a molecular modeling tool that can quantitatively predict the change in free energy (kcal/mol) upon mutation. These values approach experimental determined values. FoldX is a non-interactive command line program. In other words, not user friendly. But the bright news is that I recently developed a YASARA plugin for FoldX, so that all predictions are just a few clicks away. And the nice thing is, it’s all free! P53 as example protein In this section we will let the FoldX plugin loose on some real world examples and give you step-by-step instructions on how to proceed and analyze the results. We will use the P53 tumor suppressor protein as our example molecule. In a first exercise you will make a point mutation with FoldX and determine if the mutation is stabilizing or destabilizing for the P53 structure. In a second exercise you will design a mutation in the P53 structure at the DNA binding interface and determine how the mutation affects the interaction energy of P53 with the DNA strand. Get data hands_on Hands-on: Data download Download the file 2AC0.sce. What do FoldX energies mean? Before we start, some basic information about FoldX energies is necessary. First of all, FoldX energies are expressed in kcal/mol. The main focus of FoldX is the prediction of free energy changes, e.g. what happens to the free energy of the protein when we mutate an Asp to a Tyr? FoldX will then calculate the free energy of the wild type (WT) and the mutant (MT) and make the difference: ddG(change) = dG(MT) - dG(WT) FoldX is trained using experimental values to predict ddG(change). It is important to realize that dG(WT) and dG(MT) are meaningless numbers as such. These do not correlate with experimental values. Only ddG(change) does. As a rule of thumb we use: ddG(change) > 0 : the mutation is destabilizing ddG(change) Load > YASARA Scene Figure 1: P53 monomer bound to DNA To Repair (or minimize) the structure with FoldX go to: Analyse > FoldX > Repair object Figure 2: Select the object for repairing And select the only object in the list. When the Repair is finished, the Repaired Object is placed in Object 2 (see top right corner) and superposed with the original Object 1. Take a look at the sidechains and see what FoldX has done while Repairing. If you feel the repair takes too long (more than 10 minutes) due to a slow computer, download and open this YASARA Scene with the Repaired Object. Because we will continue working with this Repaired Object, we can now hide the entire Object 1 by toggling the Visibility column in the top right corner head-up display (HUD). How to analyze a mutation FoldX has mutated the Ala to Trp and the structure with the Trp mutation has been loaded in the next Object (3) and is superposed with the wild type (WT, Object 2). We selected an option to show the VdW clashes in WT and mutant. The atoms that give rise to steric clashes are colored in red. Toggle the Visibility of Object 2 (WT) and Object 3 (mutant) and see how many clashes we introduced by mutating the Ala to Trp. Figure 3: Zoomed-in-view on the original Ala159, no Vander Waals clashes here Figure 4: Zoomed-in-view on the mutated Ala159Trp, lots of red Vander Waals clashes here question Questions Van der Waals clashes are red colored atoms. Do you see a difference around the mutation site between WT and mutant? solution Solution Toggle the Visibility of WT and mutant to see the differences. Open the Console by pressing the spacebar twice and see the free energy change of the mutation. Anything above a change of +0.5kcal/mol is already assumed to be destabilizing. In the console - to open press spacebar twice - we see an energy change of +29 kcal/mol. Figure 5: Open the console to explore the situation. This is clearly a destabilizing mutation. Study the effect of a second mutation Hide Object 3 by toggling its Visibility so that only Object 2 (the repaired WT) is visible. First turn on all atoms in the molecules G and H (DNA) again as you did previously, because the FoldX run has hidden it (it rearranged the view to show the VdW clashes). Show the sidechain of Arg273 of Object 2 by searching for it in the sequence selector, then right-click on it and go to: Show atoms > Sidechain and CA and zoom in on Arg273 Notice how the positively charged Arginine is making an electrostatic interaction with the negative phosphate from the DNA backbone. Figure 6: R273 makes an electrostatic interaction with the DNA phosphate groups. Let’s see what would happen to the interaction energy between the DNA and P53 when we mutate this Arginine to Alanine. Right-click on this Arg273 in the sequence selector and go to: FoldX > Mutate residue A number of menus is now presented and here is what you need to do in each menu: Select Calculate interaction energy change Select Ala ‘Move neighbours’ and ‘Show disrupted and new hydrogen bonds’ Don’t change any numerical options in the last menu Figure 7: View of the first options menu with 'Show new and disrupted hydrogen bonds' selected. question Questions What is the change in interaction energy is between P53 and DNA chain G upon mutation? And what is the reason? Why doesn’t the mutation affect the interaction with DNA chain H? solution Solution Toggle the Visibility between this mutant and the WT structure and see how the hydrogen bonding changes and check the output in the Console. Figure 8: Change in interaction energy We see that the mutation decreases the interaction with DNA strand G by approximately 1 kcal/mol since we lost 1 hydrogen bond. TODO Conclusion Instead of DNA-protein, FoldX can of course also calculate interaction energy changes in protein-protein or peptide-protein complexes."},{"title":"03 Compare Structures","url":"/topics/protein-structure-analysis/tutorials/compare-structures/tutorial.html","tags":[],"body":"Structural comparison and RMSD We compare structures by structurally aligning them on top of each other. That is, we align structurally equivalent atoms. For now, we will only use CA atoms as a representation of the backbones. But Yasara also can align on any type of atom you want. You always need to specify: source object(s): the structure(s) that needs to be rotated and translated to superpose on anoth er structure target object: the structure to superpose on An optimal alignment is found when the root-mean-square deviation (RMSD) is at a minimum. The RMSD is given as: Figure 1: calculation of RMSD where R is the distance between two structurally equivalent atom pairs (CA in our case) and n is the total number of atom pairs. hands_on Hands-on: Data download Download the following adapted PDB files from Zenodo 1DKX_1.pdb 1DKY_1.pdb 1DKZ_1.pdb 3DPO_1.pdb 3DPP_1.pdb Aligning multiple structures using YASARA Now load all of them in YASARA: File > Load > PDB File and select the CA (C-alpha) view (F4) and superpose with the MUSTANG algorithm: Analyze > Align > Objects with MUSTANG In the first window you have to select the source objects that will be repositioned. Select Objects 2 till 5. In the second window you select the target Object to superpose on. That would then be the first object. Notice that YASARA prints the RMSD of every structural alignment in the lower Console. Open the Console by pressing the spacebar once or twice to extend it. Color the atoms by their B-factor: View > Color > Atom > Belongs to or has > All Then choose BFactor in the next window and press 'Apply unique color'. High BFactors are yellow, low BFactors are blue. question Questions Do you see a correlation between the BFactors and the variability in the structure? solution Solution Conclusion Structural alignment of related structures is a very efficient approach to spot similarities and differences of structutally related proteins."},{"title":"7 Gitignore","url":"/topics/git-introduction/tutorials/7_gitignore/tutorial.html","tags":[],"body":"1. Introduction What if we have files that we do not want Git to track for us, like backup files or intermediate files created during data analysis? Remember that GitHub is not your next cloud storage infrastructure. Hence, (big) data should not be uploaded on GitHub. In fact, there’s a strict file size limit of 100MB so you won’t even be able to do so. Regardless of the above, it is often useful if your data is in the same projectfolder. And you can’t help the fact that Jupyter Notebooks makes intermediate checkpoints (.ipynb_checkpoints) in the same folder of the notebook. Git has a file, the .gitignore file in which we can write expressions that define the files it should ignore. This chapter will briefly discuss the .gitignore file with a few simple examples. 2. Expressions Imagine the following project folder structure: project-folder/ | |- .git/ |- .ipynb_checkpoints/ |- .Rhistory/ | |- data/ | |- R1.fastq | |- dataset.csv | ... Let’s discuss how to ignore a specific file and how we can use symbols to generalize the ignoring behaviour. Ignore a file: The easiest would be to define the file or the path to the file. E.g. the fastq file can be ignored by adding data/R1.fastq to the .gitignore file. Similar to a file, a folder can also be ignored. The folders data/ and .ipynb_checkpoints/ can be ignored by adding the following lines: data/ .ipynb_checkpoints/ *, ! and #: The asterisk is often used in .gitignore files and represents a wildcard. E.g. *.csv will ignore any csv file in your folder and subfolders. The asterisk can precede a file format in which case it will ignore all the files with that format (e.g. ignore all csv, fastq, sam, bam, xlsx, pdf, etc. files). An exclamation mark is used for exceptions. The following lines of code will ignore all files in the data folder, except for the dataset.csv file: data/ !data/dataset.csv Documentation lines are preceded by a #. 3. Standard files It’s always good to think this through and manually add the files or folders that need to be ignored. However, it’s also useful to know that there are standardized .gitignore files. These files have been created based on a specific programming environment. They are all accessible in this repository and contain .gitignore files for Python, R, Ruby, Java, Perl, C++, amongst many others. These files can also be added on the fly to a new repository by initializing the repository with one of these files (see figure below). Let’s continue with the next session!"},{"title":"Statistics with GraphPad Prism","url":"/topics/graphpad/tutorials/Introduction/tutorial.html","tags":[],"body":"Basic statistics with GraphPad Prism This introductory video has been created during a livestream session in March 2020. We cover basic statistics, advanced statistics, graphs, curve fitting and survival analysis."},{"title":"2 Configurations","url":"/topics/git-introduction/tutorials/2_configurations/tutorial.html","tags":[],"body":"1. Getting started As mentioned in the first chapter, we will first introduce Git in a terminal session. Linux and Mac users can open a terminal directly, Windows users have to open the ‘Git Bash’ program which will act like a normal Linux terminal. If we want to use git from the command line, we always start with typing git followed by a verb defining a more specific command. These commands can be anything like staging, committing, pushing, etc. If we want to have an overview of the most common Git commands we can enter git --help. Before we can get started, we have to personalize a couple of configurations (e.g. we need to tell git who we are). Git comes with a configuration file that allows us to control all aspects of how Git looks and operates. There are three different levels on which we can do configurations, for example we could set configurations on a specific project (in one folder) or we could set them on a more global level where the configurations are applicable for all our projects. We will only edit the global configurations file here which is fine for pretty much all cases. We can have a look at our global config file with the following command: git config --global --list However, if this is the first time it will result in an failure, telling us that this file does not exist. If we just run the following commands, Git will create the configuration file automatically and add resp. our GitHub username and the email address of our account. git config --global user.name \"yourgithubusername\" git config --global user.email \"your_email@domain.com\" With these settings we can already get started, however passing on information from and to GitHub in this manner is not really secure. Using the SSH protocol, we can connect and authenticate to remote servers and services in a secure way. With SSH keys, we can connect to GitHub without supplying our username or password at each visit. If you want to create one, follow the brief instructions below or find them at GitHub. SSH keys exist of a private key and a public key. The private key must stay secured on your computer at all times, the public key can be shared with third-party softwares to connect to them. List all the files (using Git Bash) to see if existing SSH keys are present. ls -al ~/.ssh If there is a public SSH key present (file ending in .pub) we’re all set and can continue to step 3, otherwise we will now generate a public key. The following will create a new ssh key, using the provided email as a label. ssh-keygen -t ed25519 -C \"your_email@domain.com\" When you’re prompted to “Enter a file in which to save the key,” press Enter. This accepts the default file location. Then it will ask us to type a secure passphrase, press Enter to skip this step. As long as no-one other than you has access to the key, you do not require a passphrase. Now we have our SSH keys, we can let them be managed by the ssh-agent. Ensure the ssh-agent is running with: eval `ssh-agent -s` Add your SSH private key to the ssh-agent. ssh-add ~/.ssh/id_ed25519 Lastly, we need to add the public key to GitHub. Copy paste the content of the public key file manually or with: clip in which we replace with vim, notepad, emacs, atom, sublime or any other editor you prefer. The next chapter is considered further reading material and will be discussed later in the course, however since it is related to the configurations file, we have mentioned it here. 2. Aliases The configuration file is also a place where we can make our own aliases. An alias is a new command tailored to your wishes. It often consists of an existing Git command (e.g. git log) followed by a bunch of variables. This omits that we have to type a long command the whole time. Here are some useful aliases for a structured history overview: git config --global alias.hist \"log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short\" git config --global alias.oneline \"log --graph --decorate --pretty=oneline --abbrev-commit --all\" git config --global alias.mylog1 \"log --pretty=format:'%h %s [%an]' --graph\" git config --global alias.mylog2 \"log --pretty=format:'%Cgreen%h%Creset %ai | %s %Cblue[%an] %Cred%d' --date=short -n 10 --color\" git config --global alias.mylog3 \"log --decorate --pretty='format:%C(auto) %h %d %s %Cgreen(%cr by %cn)%Creset' --graph --all\" Once they are set, you can use them whenever you like. E.g.: running git hist gives us the same result as git log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short. If at some point we are not happy any more about an alias, we can delete it with the following command: git config --global --unset alias. Let’s go to the next session!"},{"title":"01 qbase+ introduction","url":"/topics/qbase-plus/tutorials/qbaseplus-introduction/tutorial.html","tags":[],"body":"qbase+ is software to visualize and analyze qPCR data. It allows you to perform various types of analyses: statistical analysis of gene expression advanced copy number analysis miRNA profiling ChIP-qPCR analysis Installation and licensing You can find the installation instructions on VIB qbase+ support page VIB only offers qbase+ to VIB scientists, you need a valid VIB email address to run the software. Biogazelle (the company who has developed the software) have written a manual with instructions on how to use the software. Download Biogazelle’s user manual. Before you can download the manual you have to log on to the qbase+ website using your qbase+ account. Use your VIB email address for setting up this account. Training material slides Extra clean log10 transformed CNRQs for checking normality in Prism clean untransformed CNRQs for visualization in Prism R script for analysis and visualization log10 transformed CNRQs of control samples for analysis and visualization in R log10 transformed CNRQs of treated samples for analysis and visualization in R"},{"title":"05 Selecting reference genes: exercises","url":"/topics/qbase-plus/tutorials/reference-genes/tutorial.html","tags":[],"body":"Since normalization of qPCR data is based on the assumption that the reference targets have the same expression level in all samples it is crucial that the expression of the chosen reference genes is stable. However, none of the so-called housekeeping genes is universally stably expressed. Genevestigator, both the commercial and the free version, contains a tool, called RefGenes, that allows to identify candidate reference genes that display very stable expression in the context that you are working in, typically a certain tissue of a certain organism. Genevestigator is a platform that contains curated public microarray data from thousands of experiments/conditions. RefGenes allows you to select the conditions that are relevant for you, e.g. mouse liver, human fibroblasts, or Arabidopsis thaliana leaves. In a next step, RefGenes identifies the genes with the most stable expression in the selected conditions. Starting the RefGenes tool | How to start the RefGenes tool ? | | :——————————– | | - Open the RefGenes page. Click start GENEVESTIGATOR Click the Install/Start button This will automatically open a Genevestigator startup page. Keep this page open during the analysis. Closing this page will close Genevestigator. Login. Also for the free version you need to create an account (use your academic email for this since you will need your vib email to get access to the commercial version). Genevestigator is opened automatically The Genevestigator user interface The Genevestigator consists of the following components: Sample Selection panel: to choose the experimental conditions you’re interested in (green) Gene Selection panel: to choose the genes you’re interested in (blue) Center panel shows an overview of all available tools (purple). Once you have selected a tool, the panel will show the results of the analysis that is done by the tool. Home button (red) allows to return to the overview of the tools at any time. The text next to the home button indicates the toolset that you have selected. Click the RefGenes tool at the bottom. Using the RefGenes tool to find reference genes STEP 1: Choose samples from a biological context similar to those in your qPCR expriment | How to choose the samples you want to analyze ? | | :——————————– | | Click the New button in the Sample Selection panel. The selection of samples defines which data are used for the analysis. Select the organism you’re interested in (in this example: human) Select the array type you want to analyze (in this example: human 133_2). For most organisms Genevestigator contains expression data from multiple types of microarrays, e.g. different generations of Affymetrix GeneChips®. On these arrays, genes are sometimes represented by different sets of probes. To keep the analysis results easily interpretable, data from different array types are not mixed. Click the Select particular conditions button to select all samples with a certain annotation, e.g. all data from a certain tissue type. Select the type of conditions (red) you want to base your selection on (in this example: Anatomy). For each type (anatomy, neoplasms, perturbations, development…) you can browse the corresponding ontologies and select the desired condition(s) (green) (in this example: cardiac muscle). Click OK Note that you can select multiple tissues. When you select samples for use in the RefGenes tool, you have to focus on microarrays from samples that were collected in conditions similar to those in your qPCR experiment. Don’t make a too general selection, e.g. all human samples: you might end up with genes that are stable in most conditions but not in yours. Don’t make a very specific selection either, e.g. human heart samples from patients taking the same medication as yours. If you want to broaden your study later on with samples from other patients, your reference genes might not be valid anymore. It is recommended to select reference genes in the same organism and the same / a similar tissue type as the one that you used in your experiments. STEP 2: Select the gene(s) you want to measure in your qPCR experiment This step is not essential, but it helps you to see whether your target gene(s) is (are) strongly or weakly expressed in the conditions of interest selected in STEP1. This allows you to search for candidate reference genes in a similar range of expression. | How to choose the genes you want to analyze ? | | :——————————– | | Click the New button in the Gene Selection panel. Enter the name of your target gene in the text area (in this example: GOT1) and click OK Open the RefGenes tool (if you haven’t done that already). A red box plot representing the distribution of the expression levels of GOT1 in the 68 selected human heart samples appears in the center panel. As you can see, this gene is highly expressed in heart. STEP 3: Find candidate reference genes The reference genes that are suggested by GeneVestigator have the following characteristics: They have the most stable expression levels across all selected samples (a small boxplot) Their overall expression level is similar to that of the target gene(s) of your qPCR experiment | How to find the candidate reference genes ? | | :——————————– | |Click the Run button in the RefGenes tool. RefGenes will show the top 20 most stable genes with similar expression levels: Exercises Finding candidate reference genes in the free version of Genevestigator Now we will make a more elaborate exercise on finding candidate reference genes. We will do the analysis in the free version of RefGenes but the analysis in the commercial version is very similar. Suppose we want to compare the expression stability of the 4 commonly used reference genes for qPCR on mouse liver samples (ACTB, GAPDH, HPRT and TUBB4B) to that of 4 reference genes that are suggested by Genevestigator. To this end we open the RefGenes tool and select the liver samples of the mouse 430_2 arrays. | Check the expression stability of the 4 commonly used reference genes ? | | :——————————– | | Click the New button in the Gene Selection panel to create a new selection. The selection of samples defines which data are used for the analysis. Enter the name of your target gene in the text area (for example: ACTB) and click OK When you are using the commercial version, you may enter multiple genes at the same time, in the free version you have to enter them one by one. This means that you have to add the first gene as described above and then add the next gene by clicking the Add button and so on… Finally you end up with an expandable list of the genes you asked for and you can tick or untick them to control the display of their expression data in the main window. When you tick the 4 commonly used reference genes you can see how stable they are expressed in the 651 mouse liver samples that are stored in Genevestigator: As you can see, the expression levels of the commonly used reference genes in the selected mouse liver samples is pretty variable which is also confirmed by their relatively high SD values. Often there are multiple probe sets for the same gene. When you use the free version you may only choose one probe set per gene so you have to make a choice. How to make that choice ? Affymetrix probe set IDs have a certain meaning: what comes after the underscore tells you something about the quality of the probes: _at means that all the probes of the probe set hit one known transcript. This is what you want: probes specifically targeting one transcript of one gene _a_at means that all the probes in the probe set hit alternate transcripts from the same gene. This is still ok the probes bind to multiple transcripts but at least the transcripts come from the same gene (splice variants) _x_at means that some of the probes hit transcripts from different genes. This is still not what you want: the expression level is based on a combination of signals of all the probes in a probe set so also probes that cross-hybridize _s_at means that all the probes in the probe set hit transcripts from different genes. This is definitely not what you want: if the probes bind to multiple genes you have no idea whose expression you have measured on the array So I always ignore probe sets with s or x. If you have two specific probe sets for a gene, they should more or less give similar signals. If this is not the case, I base my choice upon the expression level that I expect for that gene based on previous qPCR results. As you can see, each of these 4 commonly used reference genes has a high expression level. Most genes do not have such high expression levels. In most qPCR experiments your genes of interest will have low or medium expression levels, so these reference genes will not be representative for the genes of interest. Reference genes should ideally have similar expression levels as the genes of interest. Therefore, we will select the four most stably expressed genes with a medium expression level (between 8 and 12) according to the RefGenes tool. | Select the 4 most stably expressed candidate reference gene with medium expression levels. | | :——————————– | | Untick all target genes. Click the Run button at the top of the main window and check if the range is set correctly Select the 4 candidates with the lowest SD: Then, we performed qPCR on a representative set of 16 of our liver samples to measure the expression of these 8 candidate reference genes and analyzed the data (See how to select the best reference genes using geNorm in qbase+). Finding candidate reference genes in the commercial version of Genevestigator We will do the same exercise as above in the commercial version of Genevestigator. The difference between the free and commercial version of RefGenes is the number of target genes you can select. In the free version you have to select one gene and then gradually add all other genes one at a time. The commercial version allows you to load as many target genes as you want simultaneously. As a consequence, you can select multiple probe sets for the same gene. All VIB scientists have free access to the commercial version of Genevestigator via their VIB email address. If you don’t know your VIB email address, check the Who’s Who of VIB. Open a browser and go to the Genevestigator website If it’s your first time to access Genevestigator, create an account by clicking join now button. You will be redirected to a new window in which you will give some personal information including a valid VIB email address. Click Register and check your email to activate your new account. Go back to the GeneVestigator website Choose the research field you want to investigate: pharma/biomediacal or plant biology by clicking the corresponding button Click Start Use your VIB email address and password to login to Genevestigator. This will automatically open a Genevestigator startup page in your browser. Keep this page open during the analysis. Closing this page will close Genevestigator. Genevestigator is opened automatically Open the RefGenes tool by clicking its icon in the Further tools secion and select the liver samples of the mouse 430_2 arrays as explained in the previous exercise. | Check the expression stability of the 4 commonly used reference genes ? | | :——————————– | | - Click the New button in the Gene Selection panel to create a new selection. The selection of samples defines which data are used for the analysis. Enter the names of the 4 commercial reference genes in the text area and click OK I still remove probe sets with an _s or _x since they do not specifically bind to one single gene: Finally you end up with an expandable list of the genes you asked for and you can tick or untick them to control the display of their expression data in the main window. By default all probe sets are ticked so you can see how stable the commonly used reference genes are expressed in the 651 mouse liver samples that are stored in Genevestigator: As you can see, the expression levels of the commonly used reference genes in the selected mouse liver samples is pretty variable which is also confirmed by their relatively high SD values. The next step of selecting the 4 most stable candidate reference genes with medium expression levels is exactly the same as described above for the free version of RefGenes. Create a new gene selection with 20 found candidate reference genes and call it mouse_references. Click the New button at the top of the main window to create a new selection. To change the name of the selection right click the name in the Gene selection panel and select Rename Identify perturbations where the mouse_references genes show more than 1,5 fold differential expression using the Condition perturbations tool. Click the Home button at the top to go back to the tools overview page. Click the Perturbations tool in the Condition Search tools section Make a New Sample selection including all mouse 430_2 arrays. Untick all genes except for the first one and filter the long heatmap for at least 1.5 fold change differential expression: You now get a list of mouse samples in which the gene is not stably expressed so you can check if any of these samples is related to the samples in your study. Hover your mouse over the name of a sample to see more details about the sample. You can do this for each of the candidate reference genes and select the ones that best fit your needs Exercise on selecting reference genes for metacaspases in Arabidopsis thaliana. In a geNorm pilot experiment you analyze a set of candidate reference genes in a representative set of samples that you want to test in your final experiment. Based on the M-values and CVs that are calculated by qbase+, you can choose the genes that most satisfy the criteria for a good reference gene. Exercise 1: reference genes for mouse liver We come back on the 8 candidate reference genes that we selected for mouse liver: 4 commonly used reference genes: ACTB, TUBB4B, GAPDH and HPRT 4 candidate reference genes with very stable medium expression levels selected based on expression data coming from more than 600 microarrays of mouse liver samples using Genevestigator: Gm16845, MUSK, OTOP3, EDN3 We have measured their expression in a represetative set of 16 of our mouse liver samples, each in triplicate. We will now analyze the stability of these candidate reference genes in our samples. Creating a new Experiment Create a new Experiment called GeNormMouse in Project1 Open qbase+ or, if the software is already open, click the Launch Wizard button. You can find the details on how to create a new experiment in Creating a project and an experiment Loading the data into qbase+ The data is stored in the RefGenes folder. It consists of 8 Excel files, one file for each candidate reference gene. If you are not working on a BITS laptop, download and unzip the folder. Import the data. This files are in qBase format. You can find the details on how to start the data import in Loading data into qbase+ Unlike the previous exercise, qbase+ does not allow you to do a quick import this time. In the Import Run window Manual import is selected: Make sure that Upload file to Biogazelle support for further analysis is NOT selected and click Next Make sure the correct File type is selected (qBase) and click Finish. This file contains the data of the geNorm pilot experiment. In the pilot experiment, 8 candidate reference genes were measured in 16 representative mouse liver samples. Analyzing the geNorm pilot data Specify the aim of the experiment. In this experiment we want to select the ideal reference genes for our next experiments so we choose selection of reference genes (geNorm) Check the quality of the replicates (use default parameter settings). You can find the details on how to check the quality of the replicates in the Checking the quality of technical replicates and controls section of Analyzing gene expression data in qbase+ We haven’t included any positive or negative controls so you don’t need to show their details. Select the Amplification efficiencies strategy you want to use. You can find the details on how to select the Amplification effciencies strategy in the Taking into account amplification efficiencies section of Analyzing gene expression data in qbase+ We haven’t included dilution series nor do we have data from previous qPCR experiments regarding the amplification efficiencies so we choose to use the same efficiency for all genes. It is of course better to include a dilution series for each gene to have an idea of the amplification efficiencies of each primer pair. Convert all genes to Reference genes. You can convert all the genes simultaneously by selecting Use all targets as candidate reference genes Click Finish. | Which genes are you going to use as reference targets in further experiments ? | | :——————————————- | | Upon clicking Finish, the geNorm window containing the analysis results is automatically opened. The geNorm window consists of three tabs. The tabs are located at the bottom of the window: geNorm M, geNorm V and Interpretation. The first tab, geNorm M, shows a ranking of candidate genes according to their stability, expressed in M values, from the most unstable genes at the left (highest M value) to the best reference genes at the right (lowest M value): The second tab, geNorm V, shows a bar chart that helps determining the optimal number of reference genes to be used in subsequent analyses: The number of reference genes is a trade-off between practical considerations and accuracy. It is a waste of resources to quantify more genes than necessary if all candidate reference genes are relatively stably expressed and if normalization factors do not significantly change when more genes are included. However, Biogazelle recommends the minimal use of 3 reference genes and stepwise inclusion of more reference genes until the next gene has no significant contribution to the normalization factors. To determine the need of including more than 3 genes for normalization, pairwise variations Vn/n+1 are calculated between two sequential normalization factors. Simply stated: V is measure of the added value of adding a next reference gene to the analysis. A large variation means that the added gene has a significant effect and should be included. In normal experiments like the Gene expression experiment (see Analyzing gene expression data in qbase+), we only have 3 reference genes so we will see only 1 bar here. But in this geNorm pilot experiment, we analyzed 8 candidate reference genes, so we see 6 bars. All pairwise variations are very low, so even the inclusion of a third gene has no significant effect. Based on a preliminary experiment that was done by Biogazelle, 0.15 is taken as a cut-off value for V, below which the inclusion of an additional reference gene is not required. Normally this threshold is indicated by a green line on the geNorm V bar chart. However since all V-values fall below the threshold in this geNorm pilot experiment, you don’t see this line on the bar chart. So, these results mean that for all subsequent experiments on these samples, two reference genes, EDN3 and MUSK, would be sufficient. However, as stated before, Biogazelle recommends to always include at least three reference genes in case something goes wrong with one of the reference genes (so also include Gm16845). | These are artificial data. But when you read the paper by Hruz et al., 2011 you see that the genes that are selected by Genevestigator are often outperforming the commonly used reference genes. Exercise 2: reference genes for human heart Creating a new Experiment Create a new Experiment called GeNormHuman in Project1 You can find the details on how to create a new experiment in Creating a project and an experiment Loading the data into qbase+ | Import Run6 . This file is in qBase format. | | :——————————————- | | You can find the details on how to start the data import in Loading data into qbase+. Unlike the previous exercise, qbase+ does not allow you to do a quick import this time. In the Import Run window Manual import is selected: Make sure that Upload file to Biogazelle support for further analysis is NOT selected and click Next. Select the correct File type (qBase) and click Finish. This file contains the data of the geNorm pilot experiment. In the pilot experiment, 10 candidate reference genes were measured in 20 representative samples. Analyzing the geNorm pilot data Specify the aim of the experiment. In this experiment we want to select the ideal reference genes for our next experiments so we choose selection of reference genes (geNorm) Check the quality of the replicates and the controls (use default parameter settings). You can find the details on how to check the quality of the replicates in the Checking the quality of technical replicates and controls section of Analyzing gene expression data in qbase+ All replicates and controls have met the quality criteria so there’s no need to inspect them further.     Select the Amplification efficiencies strategy you want to use.   You can find the details on how to select the Amplification effciencies strategy in the Taking into account amplification efficiencies section of Analyzing gene expression data in qbase+. We haven’t included dilution series nor do we have data from previous qPCR experiments regarding the amplification efficiencies so we choose to use the same efficiency (E=2) for all genes. It is of course better to include a dilution series for each gene to have an idea of the amplification efficiencies of each primer pair. | Convert all genes to Reference genes. | | :———————————————————————————————————— | | You can convert all the genes simultaneously by selecting Use all targets as candidate reference genes | Click Finish. | Which genes are you going to use as reference targets in further experiments ? | | :——————————————- | | Upon clicking Finish, the geNorm window containing the analysis results is automatically opened. The geNorm window consists of three tabs. The tabs are located at the bottom of the window: geNorm M, geNorm V and Interpretation. The first tab, geNorm M, shows a ranking of candidate genes according to their stability, expressed in M values, from the most unstable genes at the left (highest M value) to the best reference genes at the right (lowest M value): The second tab, geNorm V, shows a bar chart that helps determining the optimal number of reference genes to be used in subsequent analyses: The number of reference genes is a trade-off between practical considerations and accuracy. It is a waste of resources to quantify more genes than necessary if all candidate reference genes are relatively stably expressed and if normalization factors do not significantly change when more genes are included. However, Biogazelle recommends the minimal use of the 3 most stable candidate reference genes and stepwise inclusion of more reference genes until the next gene has no significant contribution to the normalization factors. To determine the need of including more than 3 genes for normalization, pairwise variations Vn/n+1 are calculated between two sequential normalization factors. Simply stated: V is measure of the added value of adding a next reference gene to the analysis. A large variation means that the added gene has a significant effect and should be included. In normal experiments like the Gene expression experiment, see Analyzing_gene_expression_data_in_qbase+, we only have 3 reference genes so we will see only 1 bar here. But in this geNorm pilot experiment, we analyzed 10 candidate reference genes, so we see 8 bars. All pairwise variations are very low, so even the inclusion of a third gene has no significant effect. Based on a preliminary experiment that was done by Biogazelle, 0.15 is taken as a cut-off value for V, below which the inclusion of an additional reference gene is not required. Normally this threshold is indicated by a green line on the geNorm V bar chart. However since all V-values fall below the threshold in this geNorm pilot experiment, you don’t see this line on the bar chart. So, these results mean that for all subsequent experiments on these samples, two reference genes, HPRT1 and GADP, would be sufficient. However, as stated before, Biogazelle recommends to always include at least three reference genes in case something goes wrong with one of the reference genes (so also include YHWAZ). In this example we will analyze data from an artificial expression study containing the following samples: 6 treated samples: treated1, treated2, … treated6 6 control samples: control1, control2, … control6 In this study, the expression of the following genes was measured: 4 commonly used reference genes: ACTB, HPRT, GAPDH, and TUBB4. We have seen in the previous exercise that the expression of these reference genes in mouse liver samples is not as stable as generally thought. 3 genes of interest: Low: a gene with low expression levels Medium: a gene with moderate expression levels HighVar: a gene with low and very noisy expression In general, the lower the expression level, the more noisy the qPCR results will become. For each of the genes of interest we have included a run in which a 2-fold difference in expression between control and treated samples was created (Low1, Medium1 and HighVar1) and a run with a 4-fold difference in expression (Low2, Medium2 and HighVar2). There are three technical replicates per reaction. In a second experiment we used the reference genes that were obtained via Genevestigator and that proved to be more stably expressed in mouse liver samples than the commonly used references. The data can be found in the NormGenes folder on the BITS laptops or can be downloaded: from our website. Creating a new experiment Create a new Experiment called NormGenes1 in Project1 You can find the details on how to create a new experiment in Creating a project and an experiment Loading the data Import Run1 to Run5. These files are in qBase format. You can find the details on how to import the data file in the Loading the data into qbase+ section of Analyzing data from a geNorm pilot experiment in qbase+ We are going to compare expression in treated versus untreated samples so we need to tell qbase+ which samples are treated and which not. To this end, we have constructed a sample properties file in Excel containing the grouping annotation as a custom property called Treatment. Import the Sample Properties file. You can find the details on how to import the data file in the Adding annotation to the data section of Loading data into qbase+. Select to import the custom property. So as you can see we have 6 treated and 6 untreated samples and we have measured the expression of the 4 commonly used reference genes and 6 genes of interest: Analyzing the data Which amplification efficiencies strategy are you going to use ? You don’t have data of serial dilutions of representative template to build standard curves so the only choice you have is to use the default amplification efficiency (E = 2) for all the genes. Appoint the reference genes. ACTB, GAPDH, HPRT and TUBB4B are the reference genes: You can find the details on how to appoint reference targets in the Normalization section of Analyzing gene expression data in qbase+   Is the stability of the reference genes ok ? The M and CV values of the reference genes are shown in green so the stability of the reference genes is ok. Which scaling strategy are you going to use ? Since you have a treated and a control group, it seems logical to use the average of the control group for scaling. You can find the details on how to specify the scaling strategy in the Scaling section of Analyzing gene expression data in qbase+ Look at the target bar charts. In the target bar charts plot the average expression level of each group. In the Grouping section at the bottom of the chart you can select Plot group average: Now do exactly the same for the second experiment with the same genes of interest but with other reference genes. This means that you have to return to the Analysis wizard. To this end, click the Launch wizard button a the top of the page: Create a new Experiment called NormGenes2 in Project1 You can find the details on how to create a new experiment in Creating a project and an experiment Import Run5 to Run9. These files are in qBase format. You can find the details on how to import the data file in the Loading the data into qbase+ section of Analyzing data from a geNorm pilot experiment in qbase+ Import the Sample Properties file. You can find the details on how to import the data file in the Adding annotation to the data section of Loading data into qbase+. Select to import the custom property. So as you can see we have 6 treated and 6 untreated samples and we have measured the expression of the 4 new reference genes and 6 genes of interest: | Appoint the reference genes. |EDN3, Gm16835, MUSK and OTOP3 are the reference genes: | :—————————————————- | | You can find the details on how to appoint reference targets in the Normalization section of Analyzing gene expression data in qbase+ | Is the stability of the reference genes ok ? The M and CV values of the reference genes are shown in green so the stability of the reference genes is ok. As you can see the M and CV values of these reference genes is much lower than these of the 4 commonly used reference genes pointing to the fact that genes are more stably expressed. It’s not that the commonly used reference genes are bad references. Then qbase+ would not display them in green. It’s just that the other reference genes are more stable. But this can have a big impact on the results of your analysis… Use the average of the control group for scaling You can find the details on how to specify the scaling strategy in the Scaling section of Analyzing gene expression data in qbase+ Plot the average expression level of each group. Now we will compare the target bar charts of the second and the first experiment to assess the influence of the stability of the reference targets on the analysis results. How to display the target bar charts of the second and the first experiment next to each other ? You can display the bar charts next to each other by clicking the tab of the bar chart of the second experiment. Drag the tab to the right while you hold down the mouse button until you see and arrow at the right side of the qbase+ window and a dark grey box in the right half of qbase+ window. Release the mouse button when you see the arrow and the box. Now the two bar charts should be next to each other. Some laptop screens are too small to nicely display the two bar charts next to other. If this is the case switch to full screen mode by double clicking the tab of the first experiment. Now you can compare the expression of each gene in the first and in the second experiment. When we do this for HighVar1 for instance, you see that the average expression levels of both groups are the same in the first and the second experiment (check the scales of the Y—axis!). Both experiments detect the two-fold difference in expression level between the groups. However, the error bars are much larger in the first experiment than in the second. The variability of the reference genes does have a strong influence on the errors and the size of the error bars will influence the outcome of the statistical test to determine if a gene is differentially expressed or not. The larger the error bars the smaller the less likely it is that the test will say that the groups differ. Remember that the error bars represent 95% confidence intervals: if the error bars of the two groups do not overlap: you are certain that the difference between the means of the two groups is significant if they do not overlap: you know nothing with certainty: the means can be different or they can be the same. Of course the more they overlap the smaller the chance that there is a significant difference between the groups. Check out the results of HighVar2. Here, you clearly see the influence of the reference genes. Again, the fourfold difference in expression is detected by both experiments but: the least stable reference genes (experiment 1) give large overlapping error bars the most stable reference (experiment 2) give smaller, barely overlapping error bars This means that in experiment 2, a statistical test will probably declare that HighVar2 is differentially expressed while in experiment 1 this will not be the case. We will test this assumption by performing a statistical test. Statistical analysis of differential expression Use a non-parametric test to identify DE genes in experiment 1 ? You can find full details on statistical analyses in qbase+ in the statistical analysis section of analyzing gene expression data in qbase+. In brief, you need to perform the following steps: Open the Statistical wizard The goal of this analysis is to compare the mean expression levels of our genes of interest in treated and untreated samples Use the Treatment property to identify treated and untreated samples Analyze all genes of interest Use the default settings to perform the non-parametric Mann-Whitney test As you can see, none of the genes is considered DE by the very conservative non-parametric test. Additionally most genes have the same p-value. That’s normal when you don’t have many replicates. In our case, we have 6 replicates. Non-parametric tests are based on a ranking of the data values and there are not so many ways to rank 6 data points. This is why you see the same p-values for many genes. As said before, the non-parametric test is very stringent. If the data do come from a normal distribution, the test will generate false positives. Some of the genes might have have been labeled not DE while in fact they are DE so you might have missed some differential expression. The choice of statistical test with 6 biological replicates depends on what you prefer: false negatives or false positives. Most people will choose false negatives since they don’t want to invest time and money in research on a genes that was labeled DE while in fact it is not DE. Suppose I don’t mind false positives but I don’t want to miss any potential DE genes. In that case, it’s better to go for a t-test. Let’s repeat the test n ow choosing a parametric t-test. | Use a t-test to identify DE genes in experiment 1 ? | | :—————————————————- | | You can find full details on statistical analyses in qbase+ in the statistical analysis section of analyzing gene expression data in qbase+. In brief, you need to perform the following steps: Open the Statistical wizard The goal of this analysis is to compare the mean expression levels of our genes of interest in treated and untreated samples Use the Treatment property to identify treated and untreated samples Analyze all genes of interest Describe the data set as log-normally distributed Still none of the genes is considered DE but you do see that the p-values of the t-test are lower than these of the Mann-Whitney test. | Use a non parametric test to identify DE genes in experiment 2 ? | | :—————————————————- | | You can find full details on statistical analyses in qbase+ in the statistical analysis section of analyzing gene expression data in qbase+. In brief, you need to perform the following steps: Open the Statistical wizard The goal of this analysis is to compare the mean expression levels of our genes of interest in treated and untreated samples Use the Treatment property to identify treated and untreated samples Analyze all genes of interest Use default settings Now you see that 4 out of the 6 genes are considered DE. This is also what we expected since 3 of our genes of interst have a 4-fold difference in expression level between the two groups. It’s understandable that it’s hard to detect 2-fold differences in expression especially when the expression of the gene is somewhat variable as is the case for Low1 and HighVar1 but a 4-fold difference is a difference that you would like to detect. | Use a t-test to identify DE genes in experiment 2 ? | | :—————————————————- | | You can find full details on statistical analyses in qbase+ in the statistical analysis section of analyzing gene expression data in qbase+. In brief, you need to perform the following steps: Open the Statistical wizard The goal of this analysis is to compare the mean expression levels of our genes of interest in treated and untreated samples Use the Treatment property to identify treated and untreated samples Analyze all genes of interest Describe the data as log normally distributed Again the t-test generates lower p-values than the Mann-Whitney test but realize that choosing the t-test when the data is not normally distributed will generate false positives !"},{"title":"04 Gene expression analysis","url":"/topics/qbase-plus/tutorials/gene-expression/tutorial.html","tags":[],"body":"Create a project When you use qbase+ for the first time, you can’t do anything unless you create a project to store your experiments in. Create a new Project When you double click the qbase+ icon, the software starts up automatically opens the Start page where you can create a new project by clicking the Create new project button : This will create a new project with a default name like Project 1 . Create an experiment To open actual data (one/more runs) in qbase+, creating a project is not sufficient. You need to create an experiment in this project to hold the run data. Create a new Experiment called GeneExpression in the new project. Select the Create a new qbase+ experiment option in the Start page. Type a name for th new experiment . Click the Next button at the bottom of the page . This will create the experiment. When you leave the Start page, the Import run page is automatically opened allowing you to import the actual qPCR data into qbase+. Loading the data First a few quick words about the data set. We’ll be working with data coming from 3 runs (plates in the qPCR instrument): Run1, Run2 and Run3 The data consist of Cq values for: 3 reference target genes: Stable, Nonregulated, and Flexible 3 target genes of interest: Duvel, Leffe, and Palm each measured twice (= technical replicates) in 16 different samples. Half of the samples have undergone a treatment, half of them are untreated control samples. The data set also contains a series of standard samples consisting of a four-fold dilution series of cDNA for each target gene. These measurements allow to generate a standard curve from which target-specific amplification efficiencies can be calculated. Finally, negative controls (No Template Controls) have been measured. The goal of the analysis is to identify target genes of interest that have different expression levels in the treated samples compared to the untreated control samples. | In GeneExpression load CFX run files Run1, Run2 and Run3. | :—————————- | | Click the Import runs button to open the Import Run window Click the Browse button to go to the directory that stores the files containing the qPCR data Select the 3 run files simultaneously by holding the Ctrl key on your keyboard during the selection in Windows or the command button in MacOSX. Click the Open button Now you go back to the Import Run window, click the Next button (purple) qbase+ tries to recognize the format of the selected import files. If only one format matches the files (as in our case CFX), it is selected and the quick import option is enabled. Click the Finish button. In the Imported run names area on the Import run page you should now see the names of the 3 run files. If these are the correct files, click the Next button at the bottom of the page. Adding annotation to the data When you leave the Import run page, you are redirected to the Sample target list page, which gives you an overview of the targets (= genes) and samples qbase+ detected when reading in the datafiles. Take a look at the data. You see that the list of samples and targets matches the description of the qPCR experiment at the top of this page. The samples in this experiment are divided into two groups: samples that received some kind of treatment and untreated control samples. This information was not included in the run files so qbase+ does not know which sample belongs to which group. However, this is relevant information: in our analysis we are going to compare the expression of our genes of interest between treated and untreated samples. This means that qbase+ needs the grouping annotation to be able to perform the analysis we want to do. So we have to give qbase+ this annotation: we can do this by adding a custom sample property. To do this we need to create a sample properties file with a specific format that is described in the tutorial. You can find the file in the qbase+ folder on the BITS laptops or you can download the file here. How to add the grouping annotation ? To import the file containing to grouping annotation: select Add samples and targets click Import sample list browse to the folder that contains the samples file select the file and click Open click Next In the Importing samples window, you have to tell qbase+ which sample annotation you want to import from the sample properties file In our case we could import Quantities (this annnotation is available in the sample properties file) but the quantities of the standard samples were included in the run files so qbase+ has already imported this annotation from the run files during data import. We definitely need to import the Custom properties since they were not a part of the run files. The Treatment property will tell qbase+ which samples belong to the group of control samples and which samples belong to the group of treated samples. Click the Next button at the bottom of the page to finish the import. At this point you don’t see the custom annotation that you have imported, you will see it later in the analysis during scaling Leaving the Sample target list page takes you to the Run annotation page, where you have to confirm again that the sample and gene names are ok. If this is not the case you can adjust the annotation here. Click the Next button at the bottom of the page Our data file contains all required annotation: Cq values sample and target names sample types quantities for the standard samples grouping of the samples Once runs are imported, you can start analyzing the data. Data consist of Cq values for all the wells. Specifying the aim of the experiment On the Aim page you tell the software what type of analysis you want to do. Different types of analyses require different parameters, parameter settings and different calculations. By selecting the proper analysis type, qbase+ will only show the relevant parameters and parameter settings. Since we are doing a gene expression analysis in this exercise, this the option we should select. Click the Next button on the bottom of the page to go to the Technical quality control page. Checking the quality of technical replicates and controls The Technical quality control page handles the settings of the requirements that the data have to meet to be considered high quality. For instance the maximum difference between technical replicates is defined on this page. If there are technical replicates in the data set, qbase+ will detect them automatically (they have the same sample and target name) and calculate the average Cq value. In theory, technical replicates should generate more or less identical signals. How to set the maximum difference in Cq values for technical replicates ? The quality criterium that the replicates must meet to be included for further analysis is one of the parameters in qbase+. You can set it on the Technical quality control page: The default maximum allowed difference in Cq values between technical replicates is 0.5 Additionally, you can do quality checks based on the data of the positive and negative controls. | How to set quality requirements for the control samples ? | :—————————- | | On the same Technical quality control page you can define the minimum requirements for a well to be included in the calculations: Negative control threshold : minimum allowed difference in Cq value between the sample with the highest Cq value and the negative control with the lowest Cq value: the default is 5 which means that negative controls should be more than 5 cycles away from the sample of interest. Lower and upper boundary : allowed range of Cq values for positive controls. Excluded means that the data are ignored in the calculations. How to check if there are wells that do not meet these criteria ? You can see flagged and excluded data by ticking the Show details… options on the Technical quality control page and clicking the Next button (purple) at the bottom of the page. Qbase+ will open the results of the quality checks for the replicates and the controls on two different tabs. These tabs show lists of samples that failed the quality control criteria. When you open the replicates tab you can get an overview of the flagged or the excluded (purple) wells. Select the failing wells. When the difference in Cq between technical replicates exceeds 0.5, the wells end up in the flagged or failing list. They are included in calculations unless you exclude them by unticking them. You see that the two replicates of Palm in Sample05 have very different Cq values. All other bad replicates are coming from standard samples. If you are finished checking the data quality, click Next to go to the Amplification efficiencies page. Taking into account amplification efficiencies Qbase+ calculates an amplification efficiency (E) for each primer pair (= gene). Genes have different amplification efficiencies because: some primer pairs anneal better than others the presence of inhibitors in the reaction mix (salts, detergents…) decreases the amplification efficiency inaccurate pipetting Qbase+ has a parameter that allows you to specify how you want to handle amplification efficiencies on the Amplification efficiencies page. How to specify the amplification efficiencies strategy you want to use ? Since we have included a dilution series for creating a standard curve in our qPCR experiment, we will select Use assay specific amplification efficiencies Calculate efficiencies from included standard curves Amplification efficiencies are calculated based on the Cq values of a serial dilution of representative template, preferably a mixture of cDNAs from all your samples. Since you know the quantity of the template in each dilution, you can plot Cq values against template quantities for each primer pair. Linear regression will fit a standard curve to the data of each gene, and the slope of this curve is used to calculate the amplification efficiency. How to check the amplification efficiencies of the genes ? Once you have made this selection, qbase+ starts calculating the efficiencies and the results are immediately shown in the calculation efficiencies table. In this way, one amplification efficiency (E) for each gene is calculated and used to calculate Relative Quantities (RQ): ∆Cq is calculated for each well by subtracting the Cq of that well from the average Cq across all samples for the gene that is measured in the well. So ∆Cq is the difference between the Cq value of a gene in a given sample and the average Cq value of that gene across all samples. Cq is subtracted from the average because in this way high expression will result in a positive ∆Cq and low expression in a negative ∆Cq. So at this point the data set contains one RQ value for each gene in each sample. Click Next to go to the Normalization page. Normalization Differences in amplification efficiency are not the only source of variability in a qPCR experiment. Several factors are responsible for noise in qPCR experiments e.g. differences in: amount of template cDNA between wells RNA integrity of samples efficiency of enzymes used in the PCR or in the reverse transcription Normalization will eliminate this noise as much as possible. In this way it is possible to make a distinction between genes that are really upregulated and genes with high expression levels in one group of samples simply because higher cDNA concentrations were used in these samples. In qPCR analysis, normalization is done based on housekeeping genes. Housekeeping genes are measured in all samples along with the genes of interest. In theory, a housekeeping gene should have identical RQ values in all samples. In reality, noise generates variation in the expression levels of the housekeeping genes. This variation is a direct measure of the noise and is used to calculate a normalization factor for each sample. These normalization factors are used to adjust the RQ values of the genes of interest accordingly so that the variability is eliminated. These adjusted RQ values are called Normalized Relative Quantities (NRQs). In qbase+ housekeeping genes are called reference genes. In our data set there are three reference genes: Stable, Non-regulated and Flexible. On the Normalization page we can define the normalization strategy we are going to use, appoint the reference genes and check their stability of expression. How to specify the normalization strategy you want to use ? You can specify the normalization strategy you want to use on the Normalization method page: Reference genes normalization is based on the RQ values of the housekeeping genes Global mean normalization calculates normalization factors based on the RQ values of all genes instead of only using the reference genes. This strategy is recommended for experiments with more than 50 random genes. Random means that the genes are randomly distributed over all biological pathways. Custom value normalization is used for specific study types. This strategy allows users to provide custom normalization factors such as for example the cell count. None means that you choose to do no normalization at all. This option should only be used for single cell qPCR. We have incorporated 3 housekeeping genes in our experiment so we select the Reference genes strategy. How to appoint reference targets ? You have to indicate which targets should be used as reference genes since qbase+ treats all genes as targets of interest unless you explicitly mark them as reference genes on the Normalization method page: We have measured 3 housekeeping genes: Stable, Flexible and Non-regulated so we tick the boxes in front of their names. It’s not because you have appointed genes as reference genes that they necessarily are good reference genes. They should have stable expression values over all samples in your study. Fortunately, qbase+ checks the quality of the reference genes. For each appointed reference gene, qbase+ calculates two indicators of expression stability M (geNorm expression stability value): calculated based on the pairwise variations of the reference genes. CV (coefficient of variation): the ratio of the standard deviation of the NRQs of a reference gene over all samples to the mean NRQ of that reference gene. It is considered that the higher these indicators the less stable the reference gene. Are Flexible, Stable and Nonregulated good reference targets ? M and CV values of the appointed reference genes are automatically calculated by qbase+ and shown on the Normalization method page: The default limits for M and CV were determined by checking M-values and CVs for established reference genes in a pilot experiment that was done by Biogazelle. Based on the results of this pilot experiment, the threshold for CV and M was set to 0.2 and 0.5 respectively. If a reference gene does not meet these criteria it is displayed in red. As you can see the M and CV values of all our reference exceed the limits and are displayed in red. If the quality of the reference genes is not good enough, it is advised to remove the reference gene with the worst M and CV values and re-evaluate the remaining reference genes. Which reference target are you going to remove ? Both the M-value and the CV are measures of variability. The higher these values the more variable the expression values are. So we will remove the gene with the highest M and CV. You can remove a reference gene simply by unticking the box in front of its name. Are the two remaining reference genes good references ? After removing Flexible as a reference gene the M and CV values of the two remaining reference genes decrease drastically to values that do meet the quality criteria. M and CV values that meet the criteria are displayed in green. This exercise shows the importance of using a minimum of three reference genes. If one of the reference genes does not produce stable expression values as is the case for Flexible, you always have two remaining reference genes to do the normalization. See how to select reference genes for your qPCR experiment. So after normalization you have one NRQ value for each gene in each sample. Click Next to go to the Scaling page. Scaling Rescaling means that you calculate NRQ values relative to a specified reference level. Qbase+ allows you to rescale the NRQ values using one of the following as a reference: the sample with the minimal expression the average expression level of a gene across all samples the sample with the maximal expression a specific sample (e.g. untreated control) the average of a certain group (e.g. all control samples): this is often how people want to visualize their results positive control: only to be used for copy number analysis After scaling, the expression values of the choice you make here will be set to 1 e.g. when you choose average the average expression level across all samples will be set to 1 and the expression levels of the individual samples will be scaled accordingly. How to scale to the average of the untreated samples ? You can specify the scaling strategy on the Scaling page. Select Scale to group and set the Scaling group to the untreated samples . This is one of the reasons why you need the grouping annotation. Rescaling to the average of a group is typically used to compare results between 2 groups, e.g. treated samples against untreated controls. After rescaling, the average of the NRQs across all untreated samples is 1 and the NRQs of the treated samples are scaled accordingly. Click Next to go to the Analysis page. Visualization of the results One of the things you can select to do on the Analysis page is viewing the relative expression levels (= scaled NRQs) of each of the genes in a bar chart per gene. It is recommended to visualize your results like this. It is possible to view the relative expression levels of all genes of interest on the same bar chart. You can use this view to see if these genes show the same expression pattern but you cannot directly compare the heights of the different genes because each gene is independently rescaled! How to visualize single gene expression bar charts ? Select Visually inspect results For individual targets on the Analysis page and click Finish How to visualize the expression levels of Palm in each sample ? Select Visually inspect results For individual targets on the Analysis page and click Finish The Target select box allows you to select the gene you want to view the expression levels of. Relative expression levels are shown for each sample. Error bars are shown and represent the technical variation in your experiment (variation generated by differences in amounts pipetted, efficiency of enzymes, purity of the samples…). You see that Palm has a low expression level and a very large error bar in Sample05 because the two replicates of this sample had very different Cq values. You can group and colour the bars according to a property. How to group the bars of Palm according to treatment (so treated at one side and untreated at the other side) In the Grouping section you can specify the property you want to group by. How to view average expression levels in each group ? In the Grouping section you can choose to plot individual samples as shown above but you can also choose to plot group average expression levels. The error bars that you see here represent biological variation and will be used later on in the statistical analysis. The error bars are 95% confidence intervals which means that they represent the range that will contain with 95% certainty the real average expression level in that group of samples. The nice characteristic of 95% confidence intervals is the following: if they do not overlap you are sure that the expression levels in the two groups are significantly different, in other words the gene is differentially expressed if they do overlap you cannot say that you are sure that the expression levels are the same. You simply don’t know if the gene is differentially expressed or not. | Assess the effect of switching the Y-axis to a logarithmic scale for Palm. | :—————————- | | In the Y axis section you can specify if you want a linear or logarithmic axis. As you can see you do not change the expression values, you just change the scale of the Y axis. Switching the Y-axis to a logarithmic scale can be helpful if you have large differences in NRQs between different samples Assess the effect of switching the Y-axis to a logarithmic scale for Flexible. Switch to the bar charts of Flexible. By switching the Y-axis to logarithmic you can now see more clearly the differences between samples with small NRQs. Statistical analysis Once you generate target bar charts you leave the Analysis wizard and you go to the regular qbase+ interface. Suppose that you want to perform a statistical test to prove that the difference in expression that you see in the target chart is significant. At some point, qbase+ will ask you if your data is coming from a normal distribution. If you don’t know, you can select I don’t know and qbase+ will assume the data are not coming from a normal distribution and perform a stringent non-parametric test. However, when you have 7 or more replicates per group, you can check if the data is normally distributed using a statistical test. If it is, qbase+ will perform a regular t-test. The upside is that the t-test is less stringent than the non-parametric tests and will find more DE genes. However, you may only perform it on normally distributed data. If you perform the t-test on data that is not normally distributed you will generate false positives i.e. qbase+ will say that genes are DE while in fact they are not. Performing a non-parametric test on normally distributed data will generate false negatives i.e. you will miss DE genes. Checking if the data is normally distributed can be easily done in GraphPad Prism. To this end you have to export the data. | How to export the data ? | :—————————- | | To export the results click the upward pointing arrow in the qbase+ toolbar: You want to export the normalized data so select Export Result Table (CNRQ): You will be given the choice to export results only (CNRQs) or to include the errors (standard error of the mean) as well . We don’t need the errors in Prism so we do not select this option. The scale of the Result table can be linear or logarithmic (base 10) . Without user intervention, qbase+ will automatically log10 transform the CNRQs prior to doing statistics. So we need to check in Prism if the log transformed data are normally distributed. Additionally, you need to tell qbase+ where to store the file containing the exported data. Click the Browse button for this . Exporting will generate an Excel file in the location that you specified. However, the file contains the results for all samples and we need to check the two groups (treated and untreated) separately. The sample properties show that the even samples belong to the treated group and the odd samples to the untreated group. This means we have to generate two files: a file containing the data of the untreated samples a file containing the data of the treated samples Now we can open these files in Prism to check if the data is normally distributed. | How to import the data of the untreated samples in Prism ? | :—————————- | | Open Prism Expand File in the top menu Select New Click New Project File In the left menu select to create a Column table. Data representing different groups (in our case measurements for different genes) should always be loaded into a column table. Select Enter replicate values, stacked into columns (this is normally the default selection) since the replicates (measurements for the same gene) are stacked in the columns. Click Create Prism has now created a table to hold the data of the untreated samples but at this point the table is still empty. To load the data: Expand File in the top menu Select Import Browse to the resultslog.csv file, select it and click Open In the Source tab select Insert data only Since this is a European csv file commas are used as decimal separators so in contrast to what its name might imply, semicolons and not commas are used to separate the columns in the csv file (you can open the file in a text editor to take a look). In American csv files dots are used as decimal separator and the comma is used to separate the columns. Prism doesn’t know the format of your csv file so you have to tell him the role of the comma in your file. Select Separate decimals Go to the Filter tab and specify the rows you want to import (the last rows are these of the standard and the water samples, you don’t want to include them) Click Import As the file is opened in Prism you see that the first column containing the sample names is treated as a data column. Right click the header of the first column and select Delete | How to check if the data of the untreated samples comes from a normal distribution ? | :—————————- | | Click the Analyze button in the top menu Select to do the Column statistics analysis in the Column analyses section of the left menu In the right menu, deselect Flexible. It’s a bad reference gene so you will not include it in the qbase+ analysis so there’s no point checking its normality (it is probably not normally distributed). In that respect you could also deselect the other two reference genes since you will do the DE test on the target genes and not on the reference genes. Click OK In the Descriptive statistics and the Confidence intervals section deselect everything except Mean, SD, SEM. These statistics is not what we are interested in: we want to know if the data comes from a normal distribution. The only reason we select Mean, SD, SEM is because if we make no selection here Prism throws an error. In the Test if the values come from a Gaussian distribution section select the D’agostino-Pearson omnibus test to test if the data are drawn from a normal distribution. Although Prism offers three tests for this, the D’Agostino-Pearson test is the safest option. Click OK Prism now generates a table to hold the results of the statistical analysis: As you can see, the data for Palm are not normally distributed. Since we found that there’s one group of data that does not follow a normal distribution, it’s no longer necessary to check if the treated data are normally distributed but you can do it if you want to. We will now proceed with the statistical analysis in qbase+. Statistical analyses can be performed via the Statistics wizard. How to open the Statistics wizard ? You can open it in the Project Explorer (window at the left): expand Project1 if it’s not yet expanded expand the Experiments folder in the project if it’s not yet expanded expand the GeneExpression experiment if it’s not yet expanded expand the Analysis section if it’s not yet expanded expand the Statistics section double click Stat wizard This opens the Statistics wizard that allows you to perform various kinds of statistical analyses. Which kind of analysis are you going to do ? On the Goal page: Select Mean comparison since you want to compare expression between two groups of samples so what you want to do is comparing the mean expression of each gene in the treated samples with its mean expression level in the untreated samples. Click Next. How to define the groups that you are going to compare ? On the Groups page: specify how to define the two groups of samples that you want to compare. Select Treatment as the grouping variable to compare treated and untreated samples. Click Next. How to define the genes that you want to analyze ? On the Targets page: specify for which targets of interest you want to do the test. Deselect Flexible since you do not want to include it in the analysis. It’s just a bad reference gene. Click Next. On the Settings page you have to describe the characteristics of your data set, allowing qbase+ to choose the appropriate test for your data. The first thing you need to tell qbase+ is whether the data was drawn from a normal or a non-normal distribution. Since we have 8 biological replicates per group we can do a test in Prism to check if the data are normally distributed. Which gene(s) is/are differentially expressed ? On the Settings page you describe the characteristics of your data set so that qbase+ can choose the ideal test for your data. For our data set we can use the default settings. Click Next. In the results Table you can see that the p-value for Palm is below 0.05 so Palm is differentially expressed. In this example we will analyze data from another expression study with the following characteristics: All samples fit in a single run: Run7 We have the following samples: 5 control samples: control1, control2… 5 treated samples: treated1, treated2… 1 no template control: NTC The expression of the following genes was measured: 2 reference genes: refgene1 and refgene2 2 genes of interest: gene1 and gene2 There are two technical replicates per reaction Creating a new experiment Create a new Experiment called GeneExpression2 in Project1 You can find the details on how to create a new experiment in Creating a project and an experiment Loading the data Import Run7. This file is in qBase format. You can find the details on how to import the data file in the Loading the data into qbase+ section of Analyzing data from a geNorm pilot experiment in qbase+ Adding sample annotation Download the the sample properties file. | Add a custom sample property called Treatment. | :—————————- | | You can find the details on how to add a custom sample property in the Adding annotation to the data section of Loading data into qbase+ Analyzing the data Choose the type of analysis you want to perform.   | Check controls and replicates. | :—————————- | | First set the minimum requirements for controls and replicates You see that 6 replicates do not meet these requirements . Select to Show details and manually exclude bad replicates All negative controls pass the test . Positive controls were not included in this analysis. Qbase+ will now open the results for the failing replicates: as you can see the difference in Cq values between these replicates is not that big. They fail to meet the requirement just slightly. Which amplification efficiencies strategy are you going to use ? You don’t have data of serial dilutions of representative template to build standard curves so the only choice you have is to use the default amplification efficiency (E = 2) for all the genes. Appoint the reference genes as reference targets. You can find the details on how to appoint reference targets in the Normalization section of Analyzing gene expression data in qbase+ Is the stability of the reference genes ok ? In the Reference target stability window the M and CV values of the reference genes are shown in green so the stability of the reference genes is ok. You can find the details on how to check reference target stability in the Normalization section of Analyzing gene expression data in qbase+ Which scaling strategy are you going to use ? Since you have a treated and a control group, it seems logical to use the average of the control group for scaling. You can find the details on how to specify the scaling strategy in the Scaling section of Analyzing gene expression data in qbase+ Look at the target bar charts. In the target bar charts group the samples according to treatment. You can find the details on how to group the samples in the Visualization of the results section of Analyzing gene expression data in qbase+ The samples of each group are biological replicates so you might want to generate a plot that compares the average expression of the treated samples with the average expression of the untreated samples. In the target bar charts plot the group averages instead of the individual samples. In the Grouping section at the bottom of the chart you can select Plot group average: Are there any genes for which you see a clear difference in expression between the two groups ? For gene 1, the mean expression levels in the two groups are almost the same and the error bars completely overlap. When you look at the title of the Y-axis, you see that 95% confidence levels are used as error bars. In case of 95% confidence intervakls you can use the following rules: if they do not overlap: you are certain that the difference between the means of the two groups is significant if they do not overlap: you know nothing with certainty: the means can be different or they can be the same So for gene 1 the means are very close but just based on the plot we may not make any conclusions with certainty. For gene 2, the mean expression levels in the two groups are very different and the error bars do not overlap. So the 95% confidence intervals do not overlap meaning that we can be certain that the difference between the means of the two groups is significant. Use a statistical test to compare the expression levels between the two groups of samples ? You only have 5 replicates per group so you cannot test if the data comes from a normal distribution. Qbase+ will assume they’re not normally distributed and perform a non-parametric Mann-Whitney test. The p-value of gene2 is smaller than 0.05 so it has a statistically significant difference in expression levels in treated samples compared to untreated samples. For gene1 the p-value is 1 so we have no evidence to conclude that the expression of gene1 is different in treated compared to untreated samples. You can find the details on how to compare the means of the two groups in the Statistical analysis section of Analyzing gene expression data in qbase+"},{"title":"02 Experiment design exercises","url":"/topics/qbase-plus/tutorials/experiment-design/tutorial.html","tags":[],"body":"Exercise 1: simple gene expression study In my qPCR experiment I want to study the expression of 12 genes of interest in 8 samples of interest. I want to use 2 PCR replicates for each reaction. How many 96 well plates do I need for this experiment ? I have 12 genes in 8 samples which gives a total of 96 reactions (one plate). I want to perform each reaction twice (2 PCR replicates) so I need two plates. However, I need to include reference genes in my experiment, preferably more than one. I can put these reference genes on a separate plate, I do not have to include them on each plate. Ideally, you need to include 3 reference genes so having 8 samples and 2 replicates this gives an additional 48 reactions. Thus, I need three 96 well plates to perform this experiment. Do I need to include IRCs (inter-run calibrators) ? No, I can easily fit all samples of the same gene on the same plate so I don’t need to include IRCs. Exercise 2: a large study In my qPCR experiment I want to study the pattern of expression of 96 genes (genes of interest and reference genes) in 96 samples of interest, divided into a few groups. I want to use 2 PCR replicates for each reaction. Do I need to include IRCs (inter-run calibrators) ? No, I can fit all samples of the same gene on the same plate so I don’t need to include IRCs. I want to include PCR replicates. Do I need to include IRCs when I work on a 96 well plate ? Yes, I have 192 reactions per gene so I cannot place them on the same plate. Remember that replicates have to be located on the same plate ! Do I need to include IRCs when I work on a 384 well plate ? No, I have 192 reactions per gene so I can even place two genes on the same plate. I want to include no template controls but I don’t want to increase the number of plates. What is the most elegant strategy to make room for including negative controls ? This kind of study screen for expression patterns and requires statistical analysis. Since you have many samples divided over a few groups it means you have many biological replicates so you could easily do without the PCR replicates. By doing so you preserve the biological variability which is often far greater than the technical variation. Exercise 3: how to fill plates ? In my qPCR experiment I want to study the pattern of expression of 5 genes (genes of interest and reference genes) in 38 samples (samples of interest and control samples). I want to use 2 PCR replicates for each reaction. | What is the minimum number of 96 well plates I need for this experiment ? | | :—————————— | | 5 genes * 38 samples * 2 replicates = 380 reactions. I need a minimum of 4 plates for this experiment. If I use the minimum number of 96 well plates do I need to include IRCs ? Yes, 5 genes spread over 4 plates with 72 reactions per gene means that at least one gene will be spread over multiple plates. What can I do to avoid inter-run variability ? I can use 5 plates and fill them with one gene each. They will not be completely filled (72 reactions) but at least I do not have to use IRCs (which are additional reactions that also cost money) and I have no inter-run variation. Suppose there’s only one 96-well plate left in your lab. You have 10 samples (samples of interest + control samples) and you want to make the most of what you have. | How many genes of interest would you measure ? | | :—————————— | | Since you want to make most of what you have, let’s assume you are omitting PCR replicates. Theoretically, you could fit 9 genes on your 96-well plate. However, to avoid pipetting mistakes I would measure only 8 genes so I can work with one row / gene. This is very handy for multichannel pipets. Exercise 4: a growing study In my qPCR experiment I want to study the pattern of expression of 24 genes (genes of interest and reference genes) in 48 samples (samples of interest and control samples). I want to use 2 PCR replicates for each reaction. | How many genes can I analyze on one 384 well plate ? | | :—————————— | | 48 samples * 2 replicates = 96 reactions per gene. I can analyze 4 genes on each 384 well plate. Each week I receive 2 additional samples to analyze. | Do I analyze them immediately after I get them ? | | :—————————— | | No. Since the samples are placed on different plates as in the previous experiment, you have to use IRCs. You typically need 3 IRCs and a no template control sample. It means that if you want to analyze these 2 samples you have to include 4 additional samples for each gene. This is a lot of overhead for just 2 samples ! Try to avoid this: it’s better to wait a few weeks until you have 6 or 8 or even more samples. Exercise 5: a diagnostic copy number screen In diagnostic screens all samples are important: you cannot leave out samples and all measurements need to be of the highest quality possible. In my qPCR experiment I want to study copy number variation of 16 genes (genes of interest and reference genes) and 2 calibrator samples (samples with known copy number). Since we need high quality data we will use 4 technical replicates. Are we going to use sample maximization ? No. In contrast to gene expression studies, where we want to compare expression levels of a gene between different groups of samples, copy number analyses do compare genes. It means that in this case the sample maximization approach (placing all samples of the same gene on the same plate) is not valid. Instead we use a gene maximization approach here (placing same sample for different genes on the same plate). | How many samples can I fit on a 384 well plate ? | | :—————————— | | We have 16 (genes) * 4 (replicates) = 64 reactions per sample. This means that we can fit 6 samples on a 384 well plate: 4 unknowns and 2 calibrators. Exercise 6: fix experiments with bad or missing data In my qPCR experiment I want to study gene expression of 6 genes (3 genes of interest and 3 reference genes) in 20 samples (samples of interest and control samples). I want to use 2 technical replicates. One of my genes of interest failed completely and I want to repeat the measurements for this gene in a new run. Do I need to include IRCs ? No. We can put the 20 samples of the gene that failed on a single plate so we do not have to include IRCs. Do I need to include reference genes ? No. We just repeat all samples for the gene that failed and replace the old data with the new results. One of the reference genes failed completely. What should I do ? Depending on the quality of the two remaining reference genes, you should either do nothing or do the same as in the previous example where one of your genes of interest failed. If the two remaining reference genes are stable you can do the normalization with the two remaining reference genes. Three samples failed completely. What’s the first thing I need to do ? Since they failed completely, they are probably of low quality. Therefore, you have to prepare the samples again, check their quality and then use them for qPCR. Do I need to include IRCs ? Yes. If you want to compare these samples with the samples that didn’t fail, you have to perform inter-run calibration. Three samples failed for one of the genes of interest What is the first question I need to ask ? Is the gene expressed in these samples ? Is it possible the RNA of these three samples was of low quality ? Not likely, the measurements for the other genes in these samples are ok. Three samples failed for one of the reference genes Can I use the measurements of that reference gene in the non-failing samples for normalization ? No, qbasePLUS requires that you use the same reference genes for all samples so you have to discard all samples for that reference gene. Exercise 7: dilution series for calculating amplification efficiencies In my qPCR experiment I want to study 8 new genes for which I had to design new primer pairs in 12 samples (samples of interest and control samples). I want to use 2 technical replicates and 96 well plates. What is the first thing I need to do ? Perform a pilot experiment to determine the amplification efficiencies of these primer pairs. For this I need a dilution series of representative cDNA template. How many dilutions would you include ? A dilution series with 6 dilutions for 8 genes nicely fits into a 96 well plate. A few weeks after my initial qPCR experiment I want to test these 8 genes in a new set of samples. Do I have to repeat the pilot experiment ? No, dilution series do not need to be repeated."},{"title":"05 Analyzing data from different qPCR experiments over time","url":"/topics/qbase-plus/tutorials/multiple-experiments/tutorial.html","tags":[],"body":"You need to do inter-run calibration if you want to compare samples from different runs e.g.: when it is not possible to get all samples for the same gene on the same plate when you do additional runs weeks or months after your initial experiment Of course there is a lot of variability between runs on a qPCR instrument: thermal block is not always heating uniformously quality of the lamp, the filters and the detector decreases over time data analysis settings on the qPCR instrument (baseline correction and threshold) can be slightly different efficiency of reagents (polymerase, fluorophores) is variable optical properties of the plastic plates vary Fortunately, inter-run calibration allows you to eliminate most of this variability. In this experiment we will analyze the data from the gene expression experiment (see Analyzing gene expression data in qbase+) together with data from 2 runs (Run4 and Run5) that were done weeks after the initial gene expression experiment. Because the data comes from two different experiments spread over time, we have included three inter-run calibrators on the plates: Sample01, Sample02 and Sample03. The principle of the IRCs is very similar to that of the reference genes: In theory, the IRCs should have the same NRQ in each run. In practice, the difference in NRQ between two runs is a measure of the inter-run variation and can be used to adjust the NRQs to remove the inter-run variation. Creating a new Experiment | Import Run1, Run2, Run3(all three in CFX format), Run4 and Run5 (the latter two are in qBase format). | :—————————– | | Since the data is in files of two different format, you have to do a separate import for each format. So first import Run1, Run2 and Run3, then import Run4 and Run5. You can find the details on how to import CFX files in Loading data into qbase+. The details of importing qBase files are in Analyzing data from a geNorm pilot experiment in qbase+ Analyzing the data Use assay specific amplification efficiencies. You can find the details on how to convert the targets in the Taking into account amplification efficiencies section of Analyzing gene expression data in qbase+ In Analyzing gene expression data in qbase+ we have already checked the stability of the reference genes (see Normalization section). We determined that Flexible did not show stable expression. Convert Stable and Nonregulated to Reference targets. You can find the details on how to convert the targets in the Normalization section of Analyzing gene expression data in qbase+ Appoint Sample01, Sample02 and Sample03 as IRCs. Leave the Analysis wizard by clicking the Close wizard button in the top menu. Expand Intermediate results (red) in the Project Explorer Double click Interrun calibration (green) This opens the Interrun calibration window: Click the New button (blue) to create a IRC Once the IRC is created you have to appoint samples to it: select Sample01 in the list of Other samples Click the Add Sample button (purple) Remember that you cannot give IRCs the same name in different runs: the software would think that they are technical replicates spread over different plates (which is not allowed). Therefore, in Run4 and Run5 we have given Sample01 another name: Sample01_2. Select Sample01_2 in the list of Other samples Click the Add Sample button (purple) You have appointed the first IRC (grey), now do the same for the other two IRCs. Remember that for each target the variability of the normalized expression levels of the IRCs between different runs will be used to adjust the other normalized expression levels of that target gene. The adjustment is done by amplifying the normalized expression levels with a calibration factor that is calculated based on the normalized expression levels of the IRCs. Since variability between runs is the same for each IRC, you expect that all IRCs measure the variability between the runs to the same extent, hence leading to similar calibration factors. Do these IRCs generate similar calibration factors ? Open the Calibration Factors tab (red) of the Interrun calibration window and look at the result for Duvel: You see that IRC2 returns a substantially different calibration factor in Run5 (green) so the validity of this IRC should be interpreted with care. For Leffe the IRCs also gives inconsistent results in Run5. Switch to the results for Leffe by selecting Leffe in the Targets list (blue) | Do you still see the same expression pattern for Palm as you did in the first three runs ? | :—————————– | | Open the target bar chart for Palm. You see that the pattern Palm showed in the first three runs (sample01 to sample16): high expression in the odd and low expression in the even samples is reversed in the samples from Run4 and Run5 (sample17 to sample25). In the latter runs you see high expression in the even and low expression in the odd samples. However, without annotation for Run4 and Run5 (which samples are treated and which not) it’s impossible to interpret the bar chart. Link"},{"title":"03 Primer design exercises","url":"/topics/qbase-plus/tutorials/primer-design/tutorial.html","tags":[],"body":"The following exercise will make you familiar with the Primer3Plus software for designing primers for PCR. Primer3Plus is the user-friendly version of Primer3, the standard software for primer design. Criteria for qPCR primers Primers for qPCR have to follow all the gudelines for regular primers is and an additional set of rules specific for qPCR primers: qPCR products are small: 80-160 bp use intron or exon-exon junction spanning primers to detect genomic DNA contamination in the RNA samples. Primers of intron spanning primer pairs are located at both sides of an intron and will therefore generate a larger product on genomic DNA (containing the intron). Primer pairs containing an exon-exon junction spanning primer will not generate a PCR product on genomic DNA since the exon-exon junction only exist in the cDNA. primer length between 9 and 30 bp with an optimum at 20 bp melting temperature (Tm) of the primers between 58 and 60°C with an optimum at 59°C maximum Tm difference between the primers of a pair: 2°C GC content of the primers between 30 and 80% with an optimum at 50% the 5 nucleotides at the 3’ end of the primers should have no more than 2 G or C bases avoid runs of 4 or more identical nucleotides (especially Gs) primers must specifically target the region you want to amplify There are many programs for designing primers, the most important ones: Primer3 [1] or use it’s user-friendly version: Primer3Plus[2] PrimerBLAST[3] The major downside of Primer3 and Primer3Plus is the fact that you have to check the specificity of the primers yourself. Primer3 will suggest a number of primer pairs that fulfill all of the above requirements, but Primer3 will not check the specificity of the primers. So you have use BLAST to check the specificity of the suggested primer pairs. Very often, the selected primers are not specific and you have to repeat the entire Primer3 analysis. If you use Primer3 and do the BLAST yourself, BLAST against Refseq sequences unless they are not available for the organism you work with or you have reasons to believe that they are not complete (i.e. they do not represent the full genome). For model organisms, you can BLASTagainst the Refseq database. Limit the database to sequences from the organism you work with. Additionally, it is especially important to check that the primers are specific at the 3’ end because that’s the site where the polymerase will attach nucleotides. So it is recommended to not use primers that contain long identical stretches (> 15nt for primers of 20nt long) to other regions in the genome, and certainly not if these stretches comprise the last nucleotide at the 3’ end of the primer. For these exercises we will use PrimerBLAST since it uses the same algorithm to pick primers as Primer3 [4] and does the specificity check for you! Designing qPCR primers for the fruit fly tap gene Designing qPCR primers using PrimerBLAST The RefSeq entry NM_079400 contains the sequence of the D. melanogaster mRNA coding for tap, the target of Poxn. Tap encodes a bHLH protein expressed in larval chemosensory organs and involved in the response to sugar and salt. We wish to amplify the region encoding the Helix-loop-helix domain. In the sequence of the RefSeq record, the domain is located between position +577 and +745. We want to design qPCR primers for measuring the expression level of the hlh domain using SYBR green. Remember that it is advised to design intron/exon-exon junction spanning primers for qPCR experiments that are based on fluorescent labels to detect/avoid amplification of contaminating genomic DNA. Check in NCBIs Gene database if the hlh domain contains any introns ? To know the location of the introns, you need the genomic sequence instead of the mRNA sequence. Go to the NCBI RefSeq record. In the right menu click the link to the Gene record In the Genomic regions, transcripts and products secton you can see that the gene contains no introns: the transcript is not chopped up into pieces when aligned to the genome. Click here for an example of a gene with introns. Next, we will design primers to measure the expression of the hlh domain. Go to Primer BLAST by using the link in the Refseq record Go back to the RefSeq mRNA record. There, you can go directly to PrimerBLAST by clicking the Pick Primers link in the Analyze this sequence section of the right menu. Since you want to measure the expression of the hlh domain you want primers that are located inside the domain. Define the range of the sequence in which you want to design primers. You have to specify the range as follows: Define the primer parameters to comply with the rules of qPCR primer design: product size and Tm. To comply with the rules for qPCR primer design, you have to change the settings for PCR product size and melting temperature: The PrimerBLAST automatically decides to check primer specificity in the Drosophila (organism ID: 7227) RefSeq mRNA database which is exactly what you want. For the qPCR you are going to use RNA samples from fruitfly. This means that the primers will only come into contact with Drosophila mRNAs so you only have to check their specifity in this database. Make sure the last 2 nucleotides are completely specific. You want to ensure that the 3’ end of the primers really is specific: The PrimerBLAST gives you a set of 9 primer pairs that are specific (according to the criteria that you have specified) and that fulfill all other requirements that you have defined. Look at the detailed report of the first primer pair: All parameters are quite self-explanatory except for the Self complementary and Self 3’complementarity scores. The first score represents the local alignment score when aligning a primer to itself. The scoring system gives 1.00 for a match, -1.00 for a mismatch. This means that the lower the score (the more mismatches), the less likely that the primer binds to itself. The second score represents the global alignment score when aligning a primer to itself. Here again, the lower the score, the better. The scores are followed by information on the specificity of the primer: alignments of the two primers to all target sequences from the database that match the criteria that you specified. In these alignments dots represent matching nucleotides while letters represent mismatches. A specific primer pair will have two alignments (one for each primer): both perfect alignments (all dots) to the sequence you want to amplify. Analyzing primer characteristics using OligoAnalyzer OligoAnalyzer is a tool implemented by ID\\&T (who sell primers) to check the characteristics of your primers. Take the first primer that is suggested by Primer-BLAST, the pair resulting in a product of 100bp. | What’s the Tm of the first primer ? | | :—————————— | |Copy the sequence of the first primer in the Sequence box, adjust the concentrations to these that are typically used in PCR (see slides) and click Analyze: As you can see the predicted melting temperature is 63.9 ºC, which is slightly different from the prediction made by BLAST. There are many different methods to predict Tm and each method will give a different result. Assumed concentrations of primers and ions have an enormous impact on the Tm prediction. So don’t worry about these differences: these are theoretical calculations anyway, the only way to determine Tm values is by doing actual PCR. As long as the difference in Tm between the two primers is not too large, everything is fine. What’s the Tm of the second primer ? Copy the sequence of the second primer in the Sequence box and click Analyze. The predicted melting temperature is also 63.9 ºC , the same Tm as the first primer. Remember that the second primer had a large Self complementarity score according to PrimerBLAST. Check the self-complementarity of the second primer in OligoAnalyzer ? Click Self-Dimer: You see that the highest scoring alignment indeed has 6 matches, giving a score of 6 as predicted by PrimerBLAST. | Do you expect this self-complementarity will give problems in the PCR ? | | :—————————— | |No, the complementarity is concentrated at the center of the primer, not at the 3’ end. Since polymerases add bases at the 3’ end of the primer, the primer duplex cannot be extended so it will not give rise to aspecific products. ID&T recommends to avoid complementary stretches of more than 2 bp at the 3’ end. However, even if the primer dimer cannot be extended, it could interfere when its formation competes with the annealing of primer and target. This is only the case when the stability of the dimer is similar to the stability of a perfectly matched primer-target duplex. The stability of the perfectly matched duplex is shown as a Maximum Delta G at the top of results. So non-extendable dimer structures that are much shorter than the intended duplex, as we have here, are not going to disrupt the PCR reaction. It is advised to review all possible interactions between primers so both Self-Dimer (primers binding to themselves) and Hetero-Dimer (primers binding to each other) interactions between primers are examined. Is it likely that the primers bind to each other ? Click Hetero-Dimer: This opens a text box to enter the second primer. Click Analyze. There is one structure (the fourth one) that looks problematic because there is a stretch of 3 matching nucleotides at the 3’end of one of the primers. So you might consider taking a look at the second pair of primers that PrimerBLAST suggests. On the other hand, this structure is has relatively high free energy (delta G). The structure with the lowest total free energy, the target-primer duplex, is most important because it will dominate in solution. Structures with higher free energy are less stable and will be present in smaller amounts in the reaction mixture. Take a look at the second primer pair that was suggested by PrimerBLAST. Is it likely that these primers bind to each other ? No these primers do not form duplex structures that could pose a problem during PCR. Designing qPCR primers for the human F9 gene Designing qPCR primers using PrimerBLAST The RefSeq entry NM_000133.3 contains the sequence of the human mRNA coding for coagulation factor F9. The gene contains 8 coding exons and gives rise to a transcript of 2780 bp encoding a protein of 461 amino acids. Next, we want to design primers to measure the expression of the F9 gene. Go to the RefSeq record of this transcript to study its structure. When you scroll down to the features section you see that the CDS is located from position 40 to position 1415. Since RNA degradation starts at the 5’end of transcripts, we don’t want to pick primers at the 5’end. On the other hand, we don’t want to pick primers in the long 3’UTR either because it doesn’t contain any introns (the exons are all coding) and we want to design exon-exon junction or intron spanning primers. Let’s try to find exon-exon junction spanning primers between position 400 and 1600, with optimal anneal temperature = 60. Find primers that fulfill the above defined criteria Go to PrimerBLAST and fill in the form as follows: Exclude predicted sequences in the database to search in . Find primers that fulfill the above defined criteria Go to PrimerBLAST and fill in the remainder of the form as follows: The PrimerBLAST gives you a set of 10 primer pairs. Look at the detailed report of the first primer pair: As you can see the primers are not specific: they can bind to various other targets albeit with lower affinity because of the mismatches . The best option seems to be primer pair 7, which binds to both F9 transcript variants and potentially to one unintended target, but as you can see the last nucleotide at the 3’ end of both primers are specific. In silico PCR in the UCSC Browser We will proceed using the third primer pair Primer-BLAST suggests. You can visualize the PCR product (and additional annotation) in the UCSC Genome Browser using UCSC’s In Silico PCR tool. Select the most recent version of the human genome and paste the sequences of forward and reverse primers in their respective boxes. Click submit Normally, this returns the location and the sequence of the PCR product but our primer pair doesn’t return a match. When you think about this was to be expected since we are working with exon-exon junction spanning primers that are not able to match the genome sequence. So checking SNPs is not so straight-forward in the case of exon-exon junction spanning primers. We will repeat the primer search now searching for intron-spanning primers to show you how to use the in silico PCR tool. Taking into account the fact that the results for the exon-exon junction spanning primers were so messy we will make the search more stringent this time: We will the minimum number of mismatches to 4 and at least 3 mismatches in the last 3 bps at the 3’end Find intron spanning primers that fulfill the above defined criteria Go back to the Primer-BLAST and fill in the form like in the previous exercise except that they should span an intron: Primer-BLAST returns 10 primer pairs. Again the seventh primer pair is the specific one. | Take the seventh suggested primer pair and check for SNPs in the UCSC Browser | | :—————————— | |Go to PrimerBLAST and paste the sequences of forward and reverse primers in their respective boxes. This time the search finds a PCR product: Clicking the location visualizes the PCR product in the UCSC genome browser. Remove unnecessary trancks by right clicking the box in front of them and selecting hide Add tracks showing relevant annotation like position of SNPs… Setting the SNPs track from hide to full shows the SNPs in the browser. Center the forward primer by grabbing and dragging it to the center. Zoom in to base display to see if the forward primer is matching any SNPs. As you can see the forward primer does match two SNPs but none of them are located near the 3’end of the primer. http://frodo.wi.mit.edu/ http://primer3plus.com/cgi-bin/dev/primer3plus.cgi http://www.ncbi.nlm.nih.gov/tools/primer-blast/index.cgi?LINK_LOC=BlastHome http://www.ncbi.nlm.nih.gov/tools/primer-blast/primerinfo.html"},{"title":"8 GitHub and RStudio","url":"/topics/git-introduction/tutorials/8_github_rstudio/tutorial.html","tags":[],"body":"1. Introduction Rstudio is a popular platform for downstream data-analysis, statistics, machine learning and more scientific related analysis using the R language. If you’re unfamiliar with R and Rstudio, some materials on this website that will get you started are accesible via this link. Uptil now we have focused on the core principles of Git & GitHub, which gives us enough knowledge to start integrating in other platforms. There are three plausible scenarios: You have a version controlled project on your computer which you want to integrate in Rstudio You have a version controlled project on GitHub which you want to integrate in Rstudio locally You have an Rstudio project that you now want to start version controlling Creating a version controlled project in Rstudio from each of these scenarios is discussed in section 2: Starting a project. Exploiting Git’s features in Rstudio is further exploited in section 3: Exploring Git’s integration in Rstudio. We will exploit the repository that we created in the previous chapters of this tutorial. A sample repository is also downloadable here. Download the repository as a ZIP-file and extract it. 2. Starting a project 2.1 Integrating a version controlled project in Rstudio (scenario 1 & 2) Let’s start by making a new project (File > New project…). The following screen pops up: There are two options relevant for us to create a project in RStudio initialized with GitHub: Existing Directory: The preferred choice when a project folder already exists and which has previously been initialized with Git. Version Control: Ideally for creating a new R project based on a repository in GitHub. Given the situation that there is a folder on our computer, created during this tutorial and initialized with Git, we will go for the first option. Select Existing Directory, browse to the location of the project folder and create the project. (If you’ve downloaded the sample repository mentioned above, this option does not hold as it only downloads the files) Alternatively, if we were to choose to create a new R project based on a GitHub repository, you would need to select Version Control, followed by Git and then copy the link of the GitHub repository from the green Clone or Download button and add it as the repository URL, and finally create the project. Using the sample repository for this option would mean that we need to fill in the following link as repository URL: https://github.com/vibbits/introduction-github.git. Notice that after creating the repository, a .gitignore file is added on the fly containing the following 4 lines. These lines will make sure that irrelevant information related to Rstudio is neglected. .Rproj.user .Rhistory .RData .Ruserdata 2.2. Initiating version controlling on an existing Rstudio project (scenario 3) A third option assumes that you already have an R/Rstudio project. Click on Tools > Version control > Project Setup…. In the new screen, select Git as the version control system as depicted below and select yes when asked “Do you want to initialize a new git repository for this project?”. Rstudio will need to restart for the changes to take place. This approach will initialize Git on the project. As discussed in chapter 3, this local repository does not exist on GitHub yet, hence we can’t push our commits to GitHub. In order to do so, we’ll have to make a repository on GitHub first (see chapter 3.2.). This repository should be initialized without(!!) a README file, .gitignore file or license. Copy the link that GitHub created for the new repository (e.g. https://github.com/vibbits/rstudio-project.git). In Rstudio, find a Git tab in the upper right corner and click on New Branch (or the icon next to it). Click on add remote in the new screen, paste the GitHub link and add the name of the project. Finally, add the name of the new branch main and hit create. Select overwrite when asked. 3. Git’s features in Rstudio By initializing Git on an Rstudio project, there appears a Git tab in the upper right corner as depicted below. The tab consists of the main actions that can be performed with Git (the window might be too small to contain the keywords related to the symbol). Neglecting the diff keyword which is out of scope for this tutorial, we can find the following actions: Commit, Pull, Push, History and More followed by New Branch, the name of the branch (main) and a refresh button. Stage: The only action we’re missing is the staging. Rstudio & Git actually continuously process the files within the project searching for new changes. If there is a new change it will appear in the list in the screen as depicted here for the .gitignore file. Commit: Opens a new screen that controls the staging area and committing. Pull: Pulls upstream changes from the GitHub repository into our, this local repository. Push: Pushes previous commits to the GitHub repository. History: Neatly visualizes the history log of the repository. Each commit, branch, contributor is reviewed in this screen. More: Allows us to revert (undo) changes to a previous commit or ignore selected files (discussed below). New Branch: Creates a new branch. 4. Routine usage Recall the routine usage: stage-commit-push. Staging changes in Rstudio is done by simply checking the tickmarks in the list. This approach makes it very user-friendly to stage changes that are related with each other and that should be contained within the same commit. Subsequently, click on commit and find a similar screen: Let’s explore this screen for a while: We can find a history tab summarizing all the previous commits in this repository. As this project already existed before, it also contains the commits from before the integration in RStudio. Next to that tab we can switch the branch, generally we leave this untouched as we’re already in the preferred branch. The staging tab allows us to stage and unstage specific files, even after they were staged in a previous step. The revert tab is neglected in this tutorial Ignore allows us to edit the .gitignore file by simply selecting the file that we want to ignore and clicking on Ignore. If you’re happy with the changes and the staging area, a commit message is written in the right tab and finalized by hitting the Commit button. A message will pop up summarizing the commit in a technical way. If the commit has to appear on GitHub we need one more step. Click on Push and find your new status of the project in the GitHub repository. hands_on Exercise Add the .gitignore file to the staging area and exploit the Ignore button to add the Rproj file to the .gitignore file. Write a commit message, and commit and push your changes to GitHub. If the Rproj file already is in the .gitignore file, make a new example R-script which you can ignore. solution Solution Select File > New File > R Script, write something like # test and save the file. When they are saved, they will appear in the Git-tab. Select the files in the Git-tab and click on More > Gitignore. When you do this, the explicit name of the file will appear in the gitignore file. Click on Save. Now the gitignore file will apear in the Git-tab, ready to be staged, and the new file (or Rproj file) has disappeared from it. The rest of the workflow remains the same. Click on the tickmarcks to stage the files, click on commit, write a message in the designated textbox and push your changes to the repository on GitHub."},{"title":"01 Linux installation and training material","url":"/topics/linux/tutorials/introduction-installation/tutorial.html","tags":[],"body":"What is Linux? Linux is a very popular operating system in bioinformatics. In this training you will learn why that is and how it can help you with your bioinformatics analysis. After this training you will be able to: install software on Linux use command line to run tools use command line to handle files write small scripts to automate your analysis Linux installation Live modus Want to test a Linux distribution? Follow this procedure: Grab an USB key and put your Linux distribution (e.g. Ubuntu) on it. Boot your computer from that bootable USB key, and you have a full linux OS to play around with. This ‘live modus’ is an easy way to test the new stuff linux has to offer. Before you test anything else, check if your hardware works (printer, sound,…) and check internet connection. Secondly, do you like the desktop environment? Does it suit your needs? Play around and test. Done testing? Just reboot your computer, remove the USB key, and the original operating system will start up again as if nothing has happened… Virtual machine Go to https://www.virtualbox.org and choose Downloads. Download the correct installer for your platform and install VirtualBox on your computer. Sometimes VirtualBox displays errors when starting. Or trying VirtualBox for the first time, a virtual machine might not start. These problems might be related to not having virtualization enabled on your CPU. All the latest processors and motherboards support virtualization technology (vt-x/amd-v). It many cases, VirtualBox requires this to be enabled. To do so, you have to reboot your computer, and get into the BIOS menu. In the BIOS menu, you should enable virtualization. Where this setting is located is different between computers, so check your hardware vendor for the BIOS options, or browse around in your BIOS menu until you find it. Most of the times it is named in a decent way. Enable the option, and boot your computer. We need to download an .iso file, which is a (binary) copy of an installation DVD containing your distribution of choice. You can find it in the downloads section of the distribution’s web page. You can download it using a direct download, depending on your preference and the options offered by the distribution’s web page. You can run Linux in ‘live modus’ (see instructions above) and install it directly on your virtual machine. Afterwards you have to reboot your virtual machine to get out of the live modus. Dual boot Multi-booting allows more than one operating system to reside on one computer, for example if you have a primary operating system and an alternate system that you use less frequently. Another reason for multi-booting can be to investigate or test a new operating system without switching completely. Multi-booting allows a new operating system to configure all applications needed, and migrate data before removing the old operating system. Training material slides On the training there is a Linux Ubuntu installation available on a cloud environment. To access Linux we use Google Chrome and the ‘VNC Viewer for Google Chrome’ application. When you launch the application, you have to enter an IP address, this will be mentioned on the training. Additional information Linux Beginner’s Cheat page The practical command line cheat sheet AWK Terminal keyboard shortcuts"},{"title":"04 Text mining, scripting and loops","url":"/topics/linux/tutorials/textmining-scripting-loops/tutorial.html","tags":[],"body":"A script A script is just a plain text file. I will show this below. It contains written instructions, that can be understood by a programming language, in our case bash . An example script Create a text file named ‘buddy’ in your home with following content: badday=\"Cheer up\" goodday=\"Doing great\" echo \"$badday, $USER !\" echo \"$goodday, $USER !\" One way of doing this is: nano buddy and copy of the contents of the header above. Save the contents by pressing +O. Close nano with +x What type of file did you create? file buddy buddy: ASCII text That file contains plain text. To execute the commands in that file, feed it as an argument to the program ‘bash’. bash buddy Cheer up, bits ! Doing great, bits ! Few things to notice: in the script, we have defined 2 variables ‘badday’ and ‘goodday’ their values can be displayed by the program echo which takes as an argument the name of the variable preceded by a $ sign. the $USER variable, is an environment variable. They can be used in scripts. Env variables are typically written in capitals. Getting more professional We can make this easier. If you start your script with the symbol ‘#’ and next specify the path to the interpreter, the terminal will feed this script automatically to the right interpreter for you! To see what this means, follow these steps. Find out the path to the program bash which bash /bin/bash Now we know the path to bash, we have to provide this path, on the very first line, preceded by #! (shebang or crunchbang). If you have another type of script, let’s say perl, you find out the path to perl, and at this path behind a #! on the very first line. Open the text file ‘buddy’, and add at the start of the file ‘#!’ followed by the path to bash: nano buddy … edit the text cat buddy #!/bin/bash badday=\"Cheer up\" goodday=\"Doing great\" echo \"$badday, $USER !\" echo \"$goodday, $USER !\" Before turning the text file into a script, set the execute permission (to allow execution) with chmod chmod +x buddy What type of file is your script? file buddy buddy: Bourne-Again shell script, ASCII text executable By setting the shebang, the interpreter on the command line knows that this is a bash script! Now run your script as if it were a program (./) ./buddy Cheer up, bits ! Doing great, bits ! To make it more readable, often the extension .sh is given to the text file. Note that this is not necessary! Linux does not define file types by extensions. Rename your script to ‘buddy.sh’ $ mv buddy buddy.sh Alternative (less typing!) $ mv buddy{,.sh} A good habit The last line of your script should be ‘exit 0’. If bash reaches this lines, it means that the script was successfully executed. Add it by opening the file with ‘nano’ and modifying its contents. $ cat buddy.sh #!/bin/bash badday=\"Cheer up\" goodday=\"Doing great\" echo \"$badday, $USER !\" echo \"$goodday, $USER !\" exit 0 Alternative. Less typing! echo \"exit 0\" >> buddy.sh This was our first bash script! I hope it was a painless experience. Download a Perl script Many bioinformatics programs are written in python or perl. It’s quick to type some python or perl code in a text file, and get your job done. Those scripts are text files. You can download and store scripts on your computer. Usually these files have .py or .pl extension. As long as you have python or perl on your system (by default in Linux!), you can run the scripts. Run perl code Let’s try a small script below. Download a simple perl script here Download the dna file here Save the file, under ~/Downloads for now. Open Geany on your computer, and copy the script code to Geany. Execute the script by clicking the little ‘gear’ box. For this script, you will need to download the dna.txt file as input. The results of the script appear in a small window. It will ask for an input (depending on your script). Enter the required details. Extract some lines Download the bed file here via command line wget http://data.bits.vib.be/pub/trainingen/Linux/TAIR9_mRNA.bed Look at the first 10 lines of this file. $ head TAIR9_mRNA.bed chr1 2025600 2027271 AT1G06620.1 0 + 2025617 2027094 0 3 541,322,429, 0,833,1242, chr5 2625558 2628110 AT5G08160.1 0 - 2625902 2627942 0 6 385,143,144,186,125,573, 2167,1523,1269,928,659,0, chr5 2625558 2628110 AT5G08160.2 0 - 2625902 2627942 0 7 258,19,143,144,186,125,573, 2294,2167,1523,1269,928,659,0, chr4 12006985 12009520 AT4G22890.5 0 + 12007156 12009175 0 10 370,107,97,101,57,77,163,98,80,263, 0,802,1007,1196,1392,1533,1703,1945,2120,2272, chr4 12007040 12009206 AT4G22890.2 0 + 12007156 12009175 0 9 315,113,97,101,57,77,163,98,101, 0,741,952,1141,1337,1478,1648,1890,2065, chr4 12006985 12009518 AT4G22890.3 0 + 12007156 12009175 0 10 370,113,97,101,57,77,163,98,80,257, 0,796,1007,1196,1392,1533,1703,1945,2120,2276, chr4 12006985 12009520 AT4G22890.4 0 + 12007156 12009175 0 10 370,104,97,101,57,77,163,98,80,263, 0,805,1007,1196,1392,1533,1703,1945,2120,2272, chr4 12006985 12009520 AT4G22890.1 0 + 12007156 12009175 0 10 370,113,97,101,57,77,163,98,80,263, 0,796,1007,1196,1392,1533,1703,1945,2120,2272, chr2 14578539 14581727 AT2G34630.2 0 + 14578688 14581632 0 11 293,93,81,72,132,87,72,86,133,189,275, 0,797,1120,1320,1488,1711,1898,2165,2435,2649,2913, chr2 14578629 14581727 AT2G34630.1 0 + 14579725 14581632 0 11 203,96,81,72,132,87,72,86,133,189,275, 0,704,1030,1230,1398,1621,1808,2075,2345,2559,2823, This is a typical bioinformatics text file, with every row divided in field by tabs. Extract all lines that start with chr1 from the TAIR9_mRNA.bed and put them in a new text file “chr1_TAIR9_mRNA.bed”. $ grep \"^chr1\" TAIR9_mRNA.bed > chr1_TAIR9_mRNA.bed Checking the data Download human chromosome 21 from this link and unzip the file. wget https://data.bits.vib.be/pub/trainingen/Linux/Homo_sapiens.dna.chromosome21.zip unzip Homo_sapiens.dna.chromosome21.zip Entries in a fasta file start with > How many entries are in that fasta file? Remember you can combine commands with a |. grep \"^>\" Homo_sapiens.GRCh37.73.dna.chromosome.21.fa | wc -l How many? Use the TAIR9_mRNA.bed file used in the first exercise. Remember it looks like this chr1 2025600 2027271 AT1G06620.1 0 + 2025617 2027094 0 3 chr5 2625558 2628110 AT5G08160.1 0 - 2625902 2627942 0 6 chr5 2625558 2628110 AT5G08160.2 0 - 2625902 2627942 0 7 chr4 12006985 12009520 AT4G22890.5 0 + 12007156 12009175 0 10 chr4 12007040 12009206 AT4G22890.2 0 + 12007156 12009175 0 9 If you want to find entries that lie on the + strand of a certain chromosome, you need to find lines that start with the chromosome number and that contain a + sign. The number of characters between the chromosome number and the + sign is variable. How many genes are lying on the + strand of the first chromosome ? Since you need to use the + sign to represent a set of characters of variable length you need to use egrep for this: grep \"^chr1.+\\+\" TAIR9_mRNA.bed | wc -l More complex extraction Get the last exon size for all mRNA records in Arabidopsis. Use TAIR9_mRNA.bed for this: this file contains the exon sizes. See the .BED page to check that the field we need is field 11. This contains a comma separated list of the sizes of all the exons of a mRNA Get the exon sizes for all mRNA records in Arabidopsis. Write them to a file called exons.txt awk '{ print $11 }' TAIR9_mRNA.bed > exons.txt Take a look at the first 10 lines of exons.txt head exons.txt If we try to print the last field with awk, using ‘,’ as a delimiter, things go wrong: awk -F',' '{ print $NF }' > lastexons.txt The reason is that the last field is empty, because the lines end with a ‘,’. We need to remove the last ‘,’ and can use sed for this. Remove the last comma from the lines and save in a file called exonsclean.txt. You want to substitute the comma at the end of the line by nothing: sed 's/,$//' exons.txt > exonsclean.txt head exonsclean.txt Fetch the last field from exonsclean.txt and save in a file called lastexons.txt awk -F',' '{ print $NF }' exonsclean.txt > lastexons.txt head lastexons.txt Sort exonsizes from largest to smallest into a file called lastexonssort.txt sort -nr lastexons.txt > lastexonssort.txt head lastexonssort.txt You can use uniq to summarize the results uniq -c lastexonssort.txt | head 2 6885 1 5616 1 5601 1 5361 1 5239 1 4688 2 4470 1 4446 1 4443 1 4275 Analyzing a short read alignment SAM (‘sequence alignment map’) file format is the format which summarizes the alignment of reads to a reference genome. Is is one of the key files in NGS analysis, and you can learn a lot from it. See the SAM page for a description of this format. Download the sam file from here wget http://data.bits.vib.be/pub/trainingen/Linux/sample.sam How many lines has the SAM file? wc -l sample.sam 100015 lines How many lines start with ‘@’, which is the comment symbol in the SAM format. grep '^@' sample.sam | wc -l 15 lines You can use grep to skip the lines starting with ‘@’, since they are comment lines. grep -v '^@' sample.sam | head Write the FLAG field (second field) to a file called flags.txt and pipe the grep results to awk to print the second field. grep -v '@' sample.sam | awk '{ print $2 }' > flags.txt head flags.txt Sort and summarize (using uniq) flags.txt and pipe the grep results to awk to print the second field. sort -nr flags.txt | uniq -c Sort the results on number of times observed (the first field). We build on the previous command, and just pipe the output to sort -nr. We do not have to use the option -k, since sort always takes the first field. sort -nr flags.txt | uniq -c | sort -nr Advanced We use the TAIR9_mRNA.bed to answer this. First we check how many different genes are in the file. A gene has the code ATG. Splice variants have to same AT number but different version number (the numbers after the . are different. We are not interested in splice variants so want to remove the .1, .2… before counting. You can do this by using the . as a field delimiter Remove everything after the . and save in a file called TAIRpart.txt awk -F'.' '{ print $1 }' TAIR9_mRNA.bed > TAIRpart.txt head TAIRpart.txt Now you need to summarize the fourth column of this file and count the lines of the result How many different genes are in the file? cut -f4 TAIRpart.txt | sort | uniq | wc -l 27379 When you look at TAIR9_mRNA.bed you see that the the fifth column contains 0. Check if there is any entry that contains another number in that column ? (summarize will give you the answer) cut -f5 TAIR9_mRNA.bed | sort -nr | uniq -c No Another example: Show all Arabidopsis mRNA with more than 50 exons awk '{ if ($10>50) print $4 }' TAIR9_mRNA.bed Print the number of exons (field number 10) of mRNAs from the first chromosome. grep '^chr1' TAIR9_mRNA.bed | awk '{ print $10 }' Obtain AT numbers (field 4) and exon info (field 11) awk '{ print $4,\",\",$11 }' TAIR9_mRNA.bed Bash Aliases to enhance your productivity You specify aliases in the .bashrc file in your home directory. alias myalias=\"\" Change ‘my fancy command’ to a real command!! Before you can use your new aliases, you have to reload the .bashrc file. You do this by $ source ~/.bashrc or $ . ~/.bashrc Now, let’s do this exercise. Sometimes you might want to open a big text file from the end on, and start scrolling towards the top. We will create an alias for this in this exercise. Create an alias that starts scrolling from the bottom. Tip: it’s less and the appropriate option you must configure. Read through the man page of less. To help you: you can search for the string “at the end”. Open the man page of less $ man less Type “/at the end” and . Less will search in the content for \"at the end\". Examine the entries with the string./ Go to the following result by typing \"/\" followed by ENTER. The option is: add the alias by opening .bashrc with an editor, and adding the line: alias sell=\"less +G\" When you have changed the content of .bashrc, it needs to be reloaded. Close your terminal and fire it up again. OR execute: $ . ~/.bashrc $ source ~/.bashrc We now have sell to our disposal, which starts scrolling large text files from the end of the file. $ sell /var/log/syslog Show all aliases on your system Forgot an alias? To see all your aliases, run the command $ alias. Writing loops For loops are used to repeat commands a number of times. We will start with two simple examples. Write a for loop to create 3 files: test1.txt, test2.txt, test3.txt for i in 1 2 3 do touch test$i.txt done ls -l Write a for loop to create 3 folders: folder1, folder2, folder3 for i in 1 2 3 do mkdir folder$i done ls -l"},{"title":"03 Linux file system","url":"/topics/linux/tutorials/file-system/tutorial.html","tags":[],"body":"Tutorial on the linux file system Which protocol achieves highest compression ratio? Let’s do a little test. Download this compressed file. Create a folder named ‘Compression_exercise’ in your home. Copy the downloaded tar.gz to it. $ cd $ mkdir Compression_exercise $ cp Downloads/data_linux_training.tar.gz Compression_exercise/ Unpack the data_linux_training.tar.gz file. $ tar -xvzf data_linux_training.tar.gz Alternative: you can specify the options without the ‘-‘ sign. $ tar xvfz data_linux_training.tar.gz Decompress the file DRR000542_1.fastq.subset.gz $ gunzip DRR000542_1.fastq.subset.gz Copy the DRR000542_1.fastq.subset file to a new file called ‘bzip2_test.fastq’. Compress this file with bzip2. $ bzip2 bzip2_test.fastq Tip! If you would like to know how long the command took to finish, use “time” $ time bzip2 bzip2_test.fastq real 0m5.878s user 0m5.728s sys 0m0.112s Three different times are given. What matters to you is the line ‘real’, also called the wall-clock time. Copy DRR000542_1.fastq.subset file to a new file called gzip_test.fastq and compress with gzip. $ time gzip gzip_test.fastq real 0m5.878s user 0m5.728s sys 0m0.112s A relatively unknown package is lrzip, ‘long range zip’, which achieves very good results on big files. Let’s try that one also! Copy DRR000542_1.fastq.subset file to a new file called lrzip_test.fastq and compress with lrzip. $ lrzip lrzip_test.fastq The program 'lrzip' is currently not installed. You can install it by typing: sudo apt-get install lrzip apt-get is the command line tool to install software on Debian distro’s. Equivalent to the software center. $ sudo apt-get install lrzip [sudo] password for joachim: Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: libnet-ip-perl diffstat libnet-dns-perl libparse-debianchangelog-perl gir1.2-unique-3.0 kde-l10n-engb python-webpy libnet-domain-tld-perl libemail-valid-perl libapt-pkg-perl python-flup kde-l10n-zhcn Use 'apt-get autoremove' to remove them. The following NEW packages will be installed: lrzip 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded. Need to get 159 kB of archives. After this operation, 313 kB of additional disk space will be used. Get:1 http://be.archive.ubuntu.com/ubuntu/ precise/universe lrzip amd64 0.608-1 [159 kB] Fetched 159 kB in 0s (780 kB/s) Selecting previously unselected package lrzip. (Reading database ... 662617 files and directories currently installed.) Unpacking lrzip (from .../lrzip_0.608-1_amd64.deb) ... Processing triggers for man-db ... Setting up lrzip (0.608-1) ... Now we can compress: Output filename is: lrzip_test.fastq.lrz lrzip_test.fastq - Compression Ratio: 6.724. Average Compression Speed: 0.563MB/s. Total time: 00:03:02.97 real 3m3.026s user 3m1.947s sys 0m0.804s Compare the sizes of the different resulting compressed files. $ ls -lh *zip* -rw------- 1 bits bits 17M Oct 22 14:06 bzip2_test.fastq.bz2 -rw------- 1 bits bits 21M Oct 22 14:06 gzip_test.fastq.gz -rw------- 1 bits bits 104M Oct 22 14:06 lrzip_test.fastq -rw------- 1 bits bits 16M Oct 22 14:10 lrzip_test.fastq.lrz Decide for yourself whether the extra time needed for higher compression is worth the gain in compression. Put the three files in a newly created folder ‘results’, and make an archive of it. $ mkdir results $ mv *{bz2,q.gz,lrz} results/ $ ls results/ bzip2_test.fastq.bz2 gzip_test.fastq.gz lrzip_test.fastq.lrz $ tar cvf results.tar results/ $ rm -rf results/ $ ls -lh total 281M -rw------- 1 bits bits 104M May 4 2011 ERX000016.test.fastq -rw-r--r-- 1 bits bits 21M Oct 22 14:02 ERX000016.test.fastq.tar.gz -rw------- 1 bits bits 104M Oct 22 14:06 lrzip_test.fastq -rw-r--r-- 1 bits bits 53M Oct 22 14:28 results.tar Symbolic links Symbolic links (symlinks) point to a file, making the file accessible in another directory than where the file is. So you can avoid copying! When the original file is deleted, the symlink is dead. When you remove the symlink, the original file is still present. The syntax for symbolic links is: $ ln -s /home/bits /data/large.fastq /home/bits /Projects/ProjectA/ Tip: when using ln, preferably provide absolute paths. If you want to use relative paths, make sure first going to the directory you want the link to be in, and create the link using a relative path (using ‘.’ and ‘..’ to make the path). Removing symbolic links as such: $ unlink /home/bits /Projects/ProjectA In contrast, there is also something as a “hard link” (ln without the -s option). When you delete a hard link, the file to which it referred is gone. So ‘ln -s’ is mostly used. Linking data instead of copying In the Rice Example directory (should be available under your home): download this annotation file into the ‘Genome data’/’Annotation’ directory. Make a symbolic link to this file in the ‘Genome data’/’Sequence’ directory. Read the first 10 lines from the symbolic link file. When you have tried yourself, see the solution. $ cd Rice\\ Example/ ~/Rice Example $ ls bin Genome data ~/Rice Example $ cd Genome\\ data/Annotation/ ~/Rice Example/Genome data/Annotation $ ls ~/Rice Example/Genome data/Annotation $ wget http://rice.plantbiology.msu.edu/pub/data/Eukaryotic_Projects/o_sativa/annotation_dbs/pseudomolecules/version_7.0/all.dir/all.gff3 --2013-10-28 11:45:26-- http://rice.plantbiology.msu.edu/pub/data/Eukaryotic_Projects/o_sativa/annotation_dbs/pseudomolecules/version_7.0/all.dir/all.gff3 => `all.gff3' Resolving http://rice.plantbiology.msu.edu (http://rice.plantbiology.msu.edu)... 35.8.196.190 Connecting to http://rice.plantbiology.msu.edu (http://rice.plantbiology.msu.edu)|35.8.196.190|:21... connected. Logging in as anonymous ... Logged in! ==> SYST ... done. ==> PWD ... done. ==> TYPE I ... done. ==> CWD (1) /pub/data/Eukaryotic_Projects/o_sativa/annotation_dbs/pseudomolecules/version_7.0/all.dir ... done. ==> SIZE all.gff3 ... 81498659 ==> PASV ... done. ==> RETR all.gff3 ... done. Length: 81498659 (78M) (unauthoritative) 100%[======================================>] 81,498,659 1.34M/s in 65s 2013-10-28 11:46:33 (1.20 MB/s) - `all.gff3' saved [81498659] ~/Rice Example/Genome data/Annotation $ ls .. Annotation Sequence ~/Rice Example/Genome data/Annotation $ cd ../Sequence/ ~/Rice Example/Genome data/Sequence $ ln -s ../Annotation/all.gff3 . ~/Rice Example/Genome data/Sequence $ ls -l total 381300 lrwxrwxrwx 1 bits bits 22 Oct 28 11:49 all.gff3 -> ../Annotation/all.gff3 -rw-r--r-- 1 bits bits 390444160 Mar 8 2013 IRGSPb5.fa.masked -rw-r--r-- 1 bits bits 55 Mar 8 2013 IRGSPb5.fa.masked.gz.md5 ~/Rice Example/Genome data/Sequence $ head all.gff3 ##gff-version 3 Chr1 MSU_osa1r7 gene 2903 10817 . + . ID=LOC_Os01g01010;Name=LOC_Os01g01010;Note=TBC%20domain%20containing%20protein%2C%20expressed Chr1 MSU_osa1r7 mRNA 2903 10817 . + . ID=LOC_Os01g01010.1;Name=LOC_Os01g01010.1;Parent=LOC_Os01g01010 Chr1 MSU_osa1r7 exon 2903 3268 . + . ID=LOC_Os01g01010.1:exon_1;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 3354 3616 . + . ID=LOC_Os01g01010.1:exon_2;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 4357 4455 . + . ID=LOC_Os01g01010.1:exon_3;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 5457 5560 . + . ID=LOC_Os01g01010.1:exon_4;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 7136 7944 . + . ID=LOC_Os01g01010.1:exon_5;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 8028 8150 . + . ID=LOC_Os01g01010.1:exon_6;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 8232 8320 . + . ID=LOC_Os01g01010.1:exon_7;Parent=LOC_Os01g01010.1 Introduction: symbolic links to easily install manually applications If a package is not available via a package manager, manual installation might be an option. I put manually applications in ‘’‘/opt’’’. Next, I link them to a correct location on our system, usually ‘’‘/usr/local/bin’’’. Below you have some examples of this, which you can try out yourself. If you want to manually install apps, ‘’‘/opt’’’ is the advised directory. However, only the administrator (‘root’) can access /opt. You can check that the /opt directory belongs to root with ls -l /opt To be able to copy and write stuff into /opt, we need root permissions. To do so, precede your commands with ‘'’sudo’’’, as exemplified in the next exercise below. When we do that, our password will first be asked. Next, the command is executed with root permissions. In this way, we can edit contents in root-owned directories! You are a sudoer! Transpose, a tool to transpose Transpose is an extremely convenient text tool to transpose tabular data. We will use it later. The code is hosted on SourceForge. Download transpose installation file (zip) via the browser. Copy them to /opt using sudo cp. Go to the Sourceforce website with the browser, and click on the Download button. Downloads $ sudo cp transpose-2.0.zip /opt [sudo] password for joachim: Downloads $ We need to precede the ‘‘cp’’ command with the ‘‘sudo’’ command, since only the root user can copy into ‘‘/opt’’. Unpack the installation in /opt, compile the binary and test it with ‘tranpose –help’. Use sudo to do so. $ pwd /opt $ ls trans* transpose-2.0.zip $ sudo unzip transpose-2.0.zip Archive: transpose-2.0.zip creating: transpose-2.0/ creating: transpose-2.0/win32-bin/ inflating: transpose-2.0/win32-bin/transpose.exe creating: transpose-2.0/src/ inflating: transpose-2.0/src/transpose.c inflating: transpose-2.0/README The zip file is now unpacked. Let us now compile the code. ALWAYS have a look at the README file for this. $ cd transpose-2.0 $ head README To Compile: gcc transpose.c -o transpose To Install - Just copy into your path. e.g.: cp transpose /usr/local/bin/ $ cd src/ $ sudo gcc transpose.c -o transpose The program gcc compiles the human readable code in the file transpose.c and produces a binary file out of it, called transpose. We can now run the binary file from within the directory. $ ./transpose --help Description: This software is released under the GPL license Reshapes delimited text data - amongst other things, it can transpose a matrix of plain text data. Create a symbolic link to the newly created binary to /usr/local/bin. This directory collects binaries/commands to be used on the command line. $ sudo ln -s /opt/transpose-2.0/src/transpose /usr/local/bin $ which transpose /usr/local/bin/transpose"},{"title":"02 Linux command line","url":"/topics/linux/tutorials/command-line/tutorial.html","tags":[],"body":"Tutorial on the linux command line We will first hold your hand: type over these commands below step by step, and watch what they do. Use cd to change the current working directory (user bits). To create your own directories use the mkdir (make directory) command. $ cd ~ $ mkdir sequences $ cd sequences $ mkdir proteins $ cd proteins $ pwd /home/bits/sequences/proteins $ cd ../.. $ pwd /home/bits To create a new file, use the touch command: $ cd ~/sequences/proteins/ $ touch my_sequence.txt $ ls -l -rw-r--r-- 1 bits users 0 Sep 19 15:56 my_sequence.txt In the last command above, the -l (a lowercase “L”, not a “1” (one)) option was used with the ls command. The -l indicates that you want the directory contents shown in the “long listing” format. Most commands accept options. But which options can you use? The command man helps you. Type man followed by the command name. E.g. man ls to see what options are available for the ls command. You get a the list of options. Keep pressing Space until the page stops scrolling, then enter “q” to return to the command prompt. Luckily, most tools have the –help option. (ls –help for example). These 2 methods should help you further. To see what options can be used with ls, enter man ls. $ man ls To delete a file, use the rm (remove) command: $ cd ~/sequences/proteins/ $ ls my_sequence.txt $ rm my_sequence.txt $ ls $ To remove a directory, use the rmdir (remove directory) command. The directory needs to be empty to do this. $ cd ~/sequences/ $ ls proteins $ rmdir proteins $ ls $ To copy a file, use the cp (copy) command: $ cd ~/sequences $ touch testfile1 $ ls testfile1 $ cp testfile1 testfile2 $ ls testfile1 testfile2 To rename a file, or to move it to another directory, use the mv (move) command: $ cd $ touch testfile3 $ mv testfile3 junk $ mkdir testdir $ mv junk testdir $ ls testdir junk To download a file, use the wget command: $ cd ~/Downloads $ wget http://data.bits.vib.be/pub/trainingen/Linux/sample.sam $ ls sample.sam $ The commands covered so far represent a small but useful subset of the many commands available on a typical Linux system. Make a project folder structure We assume that start from your home folder. Create the following directory structure Figure 1: Tree $mkdir -p docs/{pdf/{man,readme},html/{man,readme}} The ‘{‘ and ‘}’ can group arguments but you can also create the structure step by step. The little tree figure above is created with the ‘tree’ command. Display such a tree. tree /home/bits/docs/ Downloading and storing bioinformatics data Create a project folder The first thing to do when you start a bioinformatics project, is to create a structure of folders to put your data in an organised fashion. Downloading As an example, we will download the rice genome from the Rice Annotation Project database. But first create the folder structure. Create following folder structure. $ mkdir \"Rice Example\" $ cd Rice\\ Example $ mkdir Genome\\ data $ cd Genome\\ data $ mkdir Sequence $ mkdir Annotation $ cd ** Be aware of white spaces on the command line!** On the command line, programs, options and arguments are separated by white spaces. If you choose to use a folder name containing a white space, it will interpret every word as an option or argument. So you have to tell Bash to ignore the white space. This can be done by: putting strings between quotes like ‘ or “ escape a white space with . See the examples above. Hence, you might save yourself some trouble (and typing!) by putting _ instead of white spaces in names. Also make sure to use tab expansion, wherever possible! Download the genome data directly on the command line You can fetch the rice genome from this link. Download the genome data to the “Rice example”/”Genome data”/Sequence folder. Use wget to download from the link. Right-click on the download link, and copy the download link. The download link is: http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz Go the directory and execute wget $ cd ## to go back to the home directory $ cd Ric $ cd Gen/Seq $ wget http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz --2013-10-15 09:36:01-- http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz Resolving rapdb.dna.affrc.go.jp (rapdb.dna.affrc.go.jp)... 150.26.230.179 Connecting to rapdb.dna.affrc.go.jp (rapdb.dna.affrc.go.jp)|150.26.230.179|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 122168025 (117M) [application/x-gzip] Saving to: `IRGSPb5.fa.masked.gz' 100%[======================================>] 122,168,025 973K/s in 2m 40s 2013-10-15 09:38:42 (747 KB/s) - `IRGSP-1.0_genome.fasta.gz' saved [122168025/122168025] $ ls IRGSPb5.fa.masked.gz Allright. We have fetched our first genome sequence! Did your data get through correctly? Large downloads or slow downloads like this can take a long time. Plenty of opportunity for the transfer to go wrong. Therefore, large downloads should always have a checksum mentioned. You can find the md5 checksum on the downloads page. The md5 checksum is an unique string identifying (and calculated from) this data. Once downloaded, you should calculate this string yourself with md5sum. $ md5sum IRGSPb5.fa.masked.gz 7af391c32450de873f80806bbfaedf05 IRGSPb5.fa.masked.gz You should go to the rice genome download page, and compare this string with the MD5 checksum mentioned over there. You can do this manually. Now that you know the concept of checksums, there is an easier way to verify the data using md5sum. Can you find the easier way? Search how to use md5sum to check the downloaded files with the .md5 file from the website. Check the man page $ man md5sum It does not say much: in the end it refers to $ info coreutils 'md5sum invocation' Reading the options, there is one option sounding promising: `-c' `--check' Read file names and checksum information (not data) from each FILE (or from stdin if no FILE was specified) and report whether the checksums match the contents of the named files. This way we can check the download: $ wget http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz.md5 --2013-10-15 09:47:02-- http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz.md5 Resolving rapdb.dna.affrc.go.jp (rapdb.dna.affrc.go.jp)... 150.26.230.179 Connecting to rapdb.dna.affrc.go.jp (rapdb.dna.affrc.go.jp)|150.26.230.179|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 55 [application/x-gzip] Saving to: `IRGSPb5.fa.masked.gz.md5' 100%[======================================>] 55 --.-K/s in 0s 2013-10-15 09:47:03 (757 KB/s) - `IRGSPb5.fa.masked.gz.md5' saved [55/55] $ ls IRGSPb5.fa.masked.gz IRGSPb5.fa.masked.gz.md5 $ md5sum -c IRGSPb5.fa.masked.gz.md5 IRGSPb5.fa.masked.gz: OK Ensuring integrity of downloads A handy tool to use is the DownThemAll addon for Firefox, in which you have to provide the checksum at the time of download. It will automatically check whether the download is finished. The Short Read Archive (SRA), storing NGS data sets, makes use of Aspera to download data a great speeds, ensuring integrity. To download from SRA using aspera in linux, follow the this guide from EBI. Extracting the data What type of file have you downloaded? $ file IRGSPb5.fa.masked.gz IRGSPb5.fa.masked.gz: gzip compressed data, was \"IRGSPb5.fa.masked\", from Unix, last modified: Wed Aug 18 03:45:47 2010 It is a compressed file. Files are compressed to save storage space. Before using these files, you have to decompress them. What can you do with this type of file? Check the command apropos. $ apropos gzip gzip (1) - compress or expand files lz (1) - gunzips and shows a listing of a gzip'd tar'd archive tgz (1) - makes a gzip'd tar archive uz (1) - gunzips and extracts a gzip'd tar'd archive zforce (1) - force a '.gz' extension on all gzip files apropos is a command that helps you discover new commands. In case you have a type of file that you don’t know about, use apropos to search for corresponding programs. Decompress the file. Check the man page of gzip. From the man page:gunzip [ -acfhlLnNrtvV ] [-S suffix] [ name … ] $ gunzip IRGSPb5.fa.masked.gz $ ls IRGSPb5.fa.masked IRGSPb5.fa.masked.gz.md5"},{"title":"4 History & status","url":"/topics/git-introduction/tutorials/4_history_status/tutorial.html","tags":[],"body":"1. Status Git can display the state of your working directory and staging area. The command that we’ll use for this is git status and depending on the situation the output will look differently, but it will always give you some informative status description. $ git status On branch main Your branch is up to date with 'origin/main'. nothing to commit, working tree clean The first sentence tells us that we’re on the main branch, which is the default branch name in Git. More on branches later. The second sentence tells us that our local branch is exactly the same as our origin. This means that all of the files and folders within our local project are identical to the ones in the remote GitHub repo. Lastly, git tells us that there is nothing to commit, which makes sense as we don’t have any changes at the moment. Let’s make some changes to one of our files again. Check the status again with git status. $ git status On branch main Your branch is up to date with 'origin/main'. Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git restore ...\" to discard changes in working directory) modified: plot1.R no changes added to commit (use \"git add\" and/or \"git commit -a\") This time, git tells us that there are changes in the file plot1.R and they are not in the staging area. There are two options here: Use git add plot1.R to add the changes to the staging area Use git restore plot1.R to remove the changes from your working directory. This will undo the changes that you made since the last time you committed it. Add the file to the staging area and check the status again with git status $ git status On branch main Your branch is up to date with 'origin/main'. Changes to be committed: (use \"git restore --staged ...\" to unstage) modified: plot1.R The file is now in the staging area and we have two options: Use git commit -m \"some informative text\" to commit the changes to the commit repository Use git restore --staged plot1.R to remove the file from the staging area. Let’s do the latter, check the status again and then remove the changes from your working directory. 5. The history (log) Besides checking the current state of your project with git status, there is also a possibility to have a look in your commit history. In order to list all your previous commits, enter git log. The output is a long list containing several blocks like this: commit e2d7e9a0b4614a6bee6b3ffd7583237125671dc1 Author: username Date: Wed Jan 01 01:23:45 2020 +0200 The informative commit message git log lists all commits made to a repository in reverse chronological order. Each commit starts with an identifier which is a unique code for each commit (hash). Besides the identifier, the commit’s author and date are given, and the commit message is given. If we have pushed the commits to our Github repository (online) we will see the last commit ID somewhere in the upper right corner. This is a verification for us so we know that the remote repository is up to date with the local repository. Can you also find an overview of all commits in GitHub? question Question Why is it useful to have the author’s name and e-mail address in the history log? solution Solution It’s obvious that in this local project we’ve been doing all the changes & commits. However at a certain point you migth collaborate with someone else on the same project. In this case it’s useful to know who did what changes. Git log can be extended with many other parameters, e.g. combine it with the --oneline argument, or add --graph to display the commit history as a text-based graph and --decorate to indicate which commits are associated with the current HEAD, the current branch main, or other Git references. Git’s aliases are very useful in this case as the way how the history is displayed is very personal. With this information you should understand the last section of Chapter 2 better and create your own alias. Intermezzo / extra reading: When the output of git log is too long to fit in your screen, git uses a program to split it into pages of the size of your screen. When this “pager” is called, you will notice that the last line in your screen is a :, instead of your usual prompt. To get out of the pager, press Q. To move to the next page, press Spacebar. To search for some_word in all pages, press / and type some_word. Navigate through matches pressing N (next). Let’s continue with the next session!"},{"title":"1 Introduction","url":"/topics/git-introduction/tutorials/1_introduction/tutorial.html","tags":[],"body":"1. Introduction Have you also been in a similar and recognizable situation as depicted below? Saving different versions of your files and scripts is essential to keep track of changes, though it can become chaotic very quickly if we do not use the excellent tools we have available to us. Git is one of these excellent tools. It works similar to Google Docs’ history feature in which Google automatically saves your document and the changes that happened at a particular moment in time. However, Git allows you to control and decide yourself when changes are worth saving, hence making it much more powerful and flexible. Each change is saved together with a message that enables you or your collaborators to keep an overview of the history of the project. Git is an open-source tool that keeps track of the changes made to your project files throughout their history. Why should you version control? Keeping track of changes to your files done by yourself or your collaborators. At any moment you can exploit the history of the project to see who wrote what on a particular day. It even allows you to go back to a specific version or undo specific edits. Synchronizes files between different people or infrastructures (i.e. laptops, servers, …), making it a powerful collaborating system. Testing new code/changes. Git can control multiple alternative versions of the same project in which you can make some changes and only when you or your collaborators are happy with hem, you can include them in the main version. There is a major difference between Git and GitHub though. Git is software that works on your computer, whereas GitHub is a service for connecting and uploading/downloading files much like saving files in the cloud. There are some alternatives for Git (link) which will not be discussed in this course, and there are some for GitHub with Gitlab and Bitbucket as main competitors. These alternatives essentially share the same concepts and therefore we choose for the tools that enjoy the most traction in the community, namely Git and GitHub. In this course we will learn how Git works on your computer, giving us a proper understanding of its functionalities. Grasping these concepts is important if we want to use Git in other apps (e.g. in Chapter 8 we will learn how GitHub and RStudio interact). 2. Installations For this course we will explore version controlling in a mixture of Git via the command-line and GitHub. The former requires some basic understanding of the Linux command line. If you’re not familiar with Linux command line, you can have a look at the materials here. After discussing Git’s essential features, we’ll introduce how you can setup a collaboration with externals or colleagues, how to integrate version controlling in Rstudio, etc. Git can be installed for any OS (Windows, Mac or Linux) from this link. Please keep the recommended and default settings as is. Make an account on GitHub. We will address further configurations in the next chapter. 3. Three conceptual areas Before diving in, let’s have a look at how Git works. It’s important to understand the three conceptual areas that exist locally when using Git on your computer: the development area, the staging area and the repository containing the commits. We already know that we want to use Git for keeping track of changes in our files. To keep track of those changes we need to run through these conceptual areas: first we edit a file on our computer (development area), then we tell Git about it (add it to the staging area) and lastly we commit those changes (commits repository). Let’s have a closer look: The development area is where your coding happens. Usually this is a folder with multiple files on your computer. Git will never change anything at this level, actually it won’t really do anything. The only thing Git does is remembering that it needs to keep track of changes made in this folder or its files. However, for this we first need to initialize Git on this folder (only once in the beginning). The staging area is an intermediate stage which assembles the files that contain changes. We can select one or multiple files with changes and stage them for a commit. This means that we’re telling Git that we will want to save those changes. Hence, imagine that we want to save a file, we first have to add it to the staging area before we can commit it. Files that are in the staging area are then committed to what we’ll call the commit repository. Once we have done that, we stored a specific version of the committed files. Committing is a synonym for saving the files in the Git terminology. The repository with commits contains a list of all the commits that we have done in a project. It’s neatly structured in a history log which we can call at any point. Notice that all of this is still happening on our computer. Here’s an example. Let’s assume that we’re starting a new project. Usually that also means that you make a new folder on your computer where you will keep all the files related to the project. The first thing you have to do is to tell Git that it has to keep track of this folder.In this step, we’re initializing Git on this folder. Now, you just made your first file. Even though it is stored on your computer, it’s not automatically saved in Git. First, you’ll have to add it to the staging area and afterwards you need to commit it to the repository. When we initialized Git on the folder, a new folder .git/ was created which will store the different versions. That allows us to only have the latest version of the files visible on our computer and all of its histories in the .git/ folder. If we make a second file, the only thing we have to do is adding it to the staging area and then commit it. Notice that the repository is not yet visible on github.com. For this we would still need a fourth and last step, namely pushing the commits repository from your computer to GitHub. By pushing your commits repository, you will push the files within the project to GitHub. After this last step, your project and all of the files are accessible in a GitHub repository. During our adventure through Git & GitHub we’ll use some specific glossary. Confused on what the meaning of all these new words are? Check out the GitHub glossary. Let’s go to the next session!"},{"title":"02 Matrices, data frames and lists","url":"/topics/R/tutorials/Matrices_Dataframes_Lists/tutorial.html","tags":[],"body":"Data structures in R The power of R lies not in its ability to work with simple numbers but in its ability to work with large datasets. R has a wide variety of data structures including scalars, vectors, matrices, data frames, and lists. Matrices A matrix is a table, the columns are vectors of equal length. All columns in a matrix must contain the same type of data. The top row, called the header, contains column labels. Rows can also have labels. Data values are called elements. Indices are often used as column and row labels. Creating a matrix To create a matrix M use the matrix() function M 3] Data frames Just like a matrix, a data frame is a table where each column is a vector. But a data frame is more general than a matrix: they are used when columns contain different data types, while matrices are used when all data is of the same type. comment Comment R has a number of built-in data frames like mtcars. Creating a data frame To create a data frame D use the function data.frame() with the vectors we want to use as columns: D Plant_study[Plant_study$Plants > 2,\"Days\"] > subset(Plant_study,Plants > 2,Days) question Question What will happen when you run this code ? Plant_study[Plant_study[\"Plants\"] > 2,\"Days\"] hands_on Hands-on: Extra exercise 10d Create vector q by extracting the a column of data frame ab (exercise 9) with and without subset(). Retrieve the second element of column a of data frame ab Add column c with elements 2,1,4,7 to data frame ab solution Solution q <- ab$a subset(q,select=a) ab$a[2] ab$c <- c(2,1,4,7) Removing elements from a data frame To remove elements from a data frame use negative indices just as in a vector e.g. to remove the second row from data frame D use: D <- D[-2,] comment Comment The minus sign only works with numbers not with column labels. To remove columns based on labels assign them to NULL: D$genome <- NULL comment Comment Setting a column to NULL is done via an assignment so the removal is permanent. comment Comment Insteading of removing elements you can also define the elements you want to keep. hands_on Hands-on: Demo From the demo script run the Data removal: data frames section Reordering columns in a data frame Reordering columns is a special case of retrieving columns, e.g. for a data frame that has 4 columns you can switch the position of the second and third column as follows: D2 <- D[ ,c(1,3,2,4)] comment Comment The first comma means keep all the rows, and the 1,3,2,4 refer to column indices. You can use indices or labels to refer to the columns. You can also use subset(): D2 <- subset(D,select=c(1,3,2,4)) hands_on Hands-on: Demo From the demo script run the Column reordering: data frames section hands_on Hands-on: Exercise 11a Switch the position of the second and the third column of Drug_study solution Solution Drug_study[,c(1,3,2)] question Question What will happen when you run this code ? subset(Drug_study,select=c(1,3,2)) Lists A list is an ordered collection of objects (of any data type: string, numbers, vectors, matrices, data frames). Lists can even contain other lists as objects! A list allows you to gather a variety of objects under one name. It is not mandatory but very useful to give each object in a list a label. Creating a list To create a list L use the list() function: L <- list(label1=object1,label2=object2,label3=object3) hands_on Hands-on: Extra exercise 12a Create a list called myList with the following objects: 5, 6, the word seven, the matrix mat. Print the list. solution Solution myList<-list(5,6,\"seven\",mat) question Question What will happen when you run this code ? subset(Drug_study,select=c(1,3,2)) Referring to the elements of a list Referring to the elements of a list can be done in exactly the same way as for data frames, using row and column indices or labels in between square brackets. However, since a list can contain other lists or data frames you have to use double square brackets [[ ]] to retrieve elements. comment Comment The $ operator also works to access the objects of a list."},{"title":"Introduction to Genome Assembly","url":"/topics/basic-bioinformatics/tutorials/general-introduction/tutorial.html","tags":[],"body":"Genome assembly with Velvet: Background Velvet is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads). The assembly method is based on the manipulation of de Bruijn graphs, via the removal of errors and the simplication of repeated regions. comment Comment For information about Velvet, you can check its (nice) Wikipedia page. For this tutorial, we have a set of reads from an imaginary Staphylococcus aureus bacterium with a miniature genome (197,394 bp). Our mutant strain read set was sequenced with the whole genome shotgun method, using an Illumina DNA sequencing instrument. From these reads, we would like to rebuild our imaginary Staphylococcus aureus bacterium via a de novo assembly of a short read set using the Velvet assembler. Agenda In this tutorial, we will deal with: Get the data Evaluate the input reads Assemble reads with Velvet Collect some statistics on the contigs Discussion Get the data We will now import the data that we will use for the tutorial. hands_on Hands-on: Getting the data Create and name a new history for this tutorial. Import from Zenodo or from the data library the files: mutant_R1.fastq mutant_R2.fastq tip Tip: Importing data via links Copy the link location (Right-click on the filename then “Copy Link Address”) Open the Galaxy Upload Manager Select Paste/Fetch Data Paste the link into the text field Change the data-type to fastqsanger Press Start Change the name of the files to mutant_R1 and mutant_R2. As a default, Galaxy uses the link as the name of the new dataset. It also does not link the dataset to a database or a reference genome. tip Tip: Renaming a dataset Click on the galaxy-pencil pencil icon for the dataset to edit its attributes In the central panel, change the Name field Click the Save button Inspect the content of a file. tip Tip: Inspecting the content of a dataset Click on the galaxy-eye (eye) icon next to the relevant history entry View the content of the file in the central panel question Questions What are four key features of a FASTQ file? What is the main difference between a FASTQ and a FASTA file? solution Solution Each sequence in a FASTQ file is represented by 4 lines: 1st line is the id, 2nd line is the sequence, 3rd line is not used, and 4th line is the quality of sequencing per nucleotide In a FASTQ file, not only are the sequences present, but information about the quality of sequencing is also included. The reads have been sequenced from an imaginary Staphylococcus aureus bacterium using an Illumina DNA sequencing instrument. We obtained the 2 files we imported (mutant_R1 and mutant_R2) question Question Why do we have 2 files here if we only sequenced the bacteria once? solution Solution The bacteria has been sequenced using paired-end sequencing. The first file corresponds to forward reads and the second file to reverse reads. Evaluate the input reads Before doing any assembly, the first questions you should ask about your input reads include: What is the coverage of my genome? How good is my read set? Do I need to ask for a new sequencing run? Is it suitable for the analysis I need to do? We will evaluate the input reads using the FastQC tool. This tool runs a standard series of tests on your read set and returns a relatively easy-to-interpret report. We will use it to evaluate the quality of our FASTQ files and combine the results with MultiQC. hands_on Hands-on: FastQC on a fastq file FastQC tool with the following parameters “Short read data from your current history” to (Multiple datasets) mutant_R1.fastq and mutant_R2.fastq MultiQC tool with the following parameters “Software name” to FastQC “Result file” to the raw data files generated by FastQC MultiQC generates a webpage combining reports for FastQC on both datasets. It includes these graphs and tables: General statistics This is important in setting maximum k-mer size for an assembly. comment Getting the length of sequences Click on Configure Columns Check Length Close the window question Questions How long are the sequences? What is the average coverage of the genome, given our imaginary Staphylococcus aureus bacterium has a genome of 197,394 bp? solution Solution The sequences are 150 bp long We have 2 x 12,480 sequences of 150 bp, so the average genome coverage is: 2 * 12480 * 150 / 197394, or approximately 19 X coverage. Sequence Quality Histograms Dips in quality near the beginning, middle or end of the reads may determine the trimming/cleanup methods and parameters to be used, or may indicate technical problems with the sequencing process/machine run. Figure 1: The mean quality value across each base position in the read question Questions What does the y-axis represent? Why is the quality score decreasing across the length of the reads? solution Solution The y-axis represents the quality score for each base (an estimate of the error during sequencing). The quality score is decreasing accross the length of the reads because the sequencing become less and less reliable at the end of the reads. Per Sequence GC Content High GC organisms tend not to assemble well and may have an uneven read coverage distribution. Per Base N Content The presence of large numbers of Ns in reads may point to a poor quality sequencing run. You will need to trim these reads to remove Ns. k-mer content The presence of highly recurring k-mers may point to contamination of reads with barcodes or adapter sequences. comment Comment For a fuller discussion of FastQC outputs and warnings, see the FastQC website link, including the section on each of the output reports, and examples of “good” and “bad” Illumina data. We won’t be doing anything to these data to clean it up as there isn’t much need. Therefore we will get on with the assembly! Assemble reads with Velvet Now, we want to assemble our reads to find the sequence of our imaginary Staphylococcus aureus bacterium. We will perform a de novo assembly of the reads into long contiguous sequences using the Velvet short read assembler. The first step of the assembler is to build a de Bruijn graph. For that, it will break our reads into k-mers, i.e. fragments of length k. Velvet requires the user to input a value of k (k-mer size) for the assembly process. Small k-mers will give greater connectivity, but large k-mers will give better specificity. hands_on Hands-on: Assemble the reads FASTQ interlacer tool with the following parameters “Type of paired-end datasets” to 2 separate datasets “Left-hand mates” to mutant_R1.fastq “Right-hand mates” to mutant_R2.fastq Currently our paired-end reads are in 2 files (one with the forward reads and one with the reverse reads), but Velvet requires only one file, where each read is next to its mate read. In other words, if the reads are indexed from 0, then reads 0 and 1 are paired, 2 and 3, 4 and 5, etc. Before doing the assembly per se, we need to prepare the files by combining them. velveth tool with the following parameters “Hash Length” to 29 “Input Files”: click on Insert Input Files “file format” to fastq “read type” to shortPaired reads “Dataset” to the pairs output of FASTQ interlacer The tool takes our reads and break them into k-mers. velvetg tool with the following parameters “Velvet Dataset” to the output of velveth “Using Paired Reads” to Yes This last tool actually does the assembly. Two files are generated: A “Contigs” file This file contains the sequences of the contigs longer than 2k. In the header of each contig, a bit of information is added: the k-mer length (called “length”): For the value of k chosen in the assembly, a measure of how many k-mers overlap (by 1 bp each overlap) to give this length the k-mer coverage (called “coverage”): For the value of k chosen in the assembly, a measure of how many k-mers overlap each base position (in the assembly). A “Stats” file This is a tabular file giving for each contig the k-mer lengths, k-mer coverages and other measures. Collect some statistics on the contigs question Question How many contigs have been built? What is the mean, min and max length of the contigs? solution Solution 190 To compute this information, we can use the Datamash tool on the 2nd columns (length). Be careful with the first line, the header. As a result, we obtain: 597.82 as mean, 1 as min and 12904 as max. It would mean that the smallest contig has a length of 1 bp, even smaller than k. The length on the 2nd column corresponds to length of the contig in k-mers. This means that the smallest contig has a length of 1k = 29. So to obtain the real length, we need to add k-1 to the length. We then obtain a mean contig length of 625.82 bp, a min contig of 29 bp and a max contig of 12,932 bp. This table is limitted, but we will now collect more basic statistics on our assembly. hands_on Hands-on: Collect fasta statistics on our contigs Quast tool with “Contigs/scaffolds output file” to the output of velvetg “Type of data” to contig “Reference File” to wildtype.fna “Type of organism” to Prokaryotes “Lower Threshold” to 500 “Thresholds” to 0,1000 This tool generates 5 output files, but we will focus on the HTML report and the Icarus viewer. question Question What is represented in the Icarus viewer? solution Solution Icarus is a novel genome visualizer for accurate assessment and analysis of genomic draft assemblies. It draws contigs ordered from longest to shortest, highlights N50, N75 (NG50, NG75) and long contigs larger than a user-specified threshold The HTML report reports many statistics computed by QUAST to assess the quality of the assembly: Statistics about the quality of the assembly when compared to the reference (fraction of the genome, duplication ratio, etc) Misassembly statistics, including the number of misassemblies A misassembly is a position in the contigs (breakpoints) that satisfy one of the following criteria: the left flanking sequence aligns over 1 kbp away from the right flanking sequence on the reference; flanking sequences overlap on more than 1 kbp flanking sequences align to different strands or different chromosomes Unaligned regions in the assembly Mismatches compared to the reference genomes Statistics about the assembly per se, such as the number of contigs and the length of the largest contig question Question How many contigs have been constructed? Which proportion of the reference genome do they represent? How many misassemblies have been found? Has the assembly introduced mismatches and indels? What are N50 and L50? Is there a bias in GC percentage induced by the assembly? solution Solution 190 contigs have been constructed, but only 47 have a length > 500 bp. The contigs represents 87.965% of the reference genome. 1 misassembly has been found: it corresponds to a relocation, i.e. a misassembly event (breakpoint) where the left flanking sequence aligns over 1 kbp away from the right flanking sequence on the reference genome. 8.06 mismatches per 100 kbp and 4.03 indels per 100 kbp are found. N50 is the length for which the collection of all contigs of that length or longer covers at least half an assembly. In other words, if contigs were ordered from small to large, half of all the nucleotides will be in contigs this size or larger. And L50 is the number of contigs equal to or longer than N50: L50 is the minimal number of contigs that cover half the assembly. The GC % in the assembly is 33.64%, really similar to the one of the reference genome (33.43%). Discussion hands_on (Optional) Hands-on: Rerun for values k ranging from 31 to 101 velveth tool with the same parameters as before except “Hash Length” to a value between 31 and 101 velvetg tool with the same parameters as before Quast tool with the same parameters as before We have completed an assembly on this data set for a number of k values ranging from 29 to 101. A few of the assembly metrics appear below. Figure 2: Number of contigs in the assembly for various k-mer sizes Figure 3: Largest contig in each of the assemblies by k-mer size Figure 4: Total number of base pairs in all the contigs for each assembly by k-mer size Figure 5: N50 metric for each of the assemblies by k-mer size question Questions Are there any distinct features in the charts? Does it look like one assembly might be better than some of the others? The reasons for these patterns will be discussed in detail in the De Bruijn graph assembly slides and tutorial."},{"title":"04 Data manipulation","url":"/topics/R/tutorials/Functions/tutorial.html","tags":[],"body":"Manipulation of variables General functions The big difference between R and other programming languages is that functions in R are designed to be applied to variables rather than to individual values to avoid loops e.g. if we want to log transform a whole dataset we can do this using a single operation: > v log10(v) [1] 0 1 2 3 4 The log10() function is written in such a way that it can be applied on a vector. This is true for all functions and operators in R: > v - 1 [1] 0 9 99 999 9999 R has built-in functions for virtually any standard mathematical task. Figure 1: Overview of built-in functions Arithmetic operators can be used on variables. Provided that the variables have the same dimensions, you can do element-wise addition, subtraction, multiplication and division of two vectors or tables. Element-wise means that the calculation is performed on the equivalent positions between the two variables: first element + first element, second element + second element etc. > v1 v2 z z [1] 5 7 9 If you perform operations on vectors with different lengths (not recommended) then the vector with the shorter length is recycled to the length of the longer vector so that the first element of the shorter vector is appended to the end of that vector (a way of faking that it is of equal length to the longer vector) and so forth. You will get a warning, but R does let you perform the operation: > x1 x2 x3 x3 [1] 4 6 6 hands_on Hands-on: Demo From the demo script run the Operations on variables section hands_on Hands-on: Exercise 13a Calculate log base2 of the activity in Drug_study Round the result to the nearest integer solution Solution log.act <- (log2(Drug_study$activity)) round(log.act) hands_on Hands-on: Extra exercise 13b Create vector v as the sum of newVector and threes using an arithmetic operator Print the content of v Do the same for newVector and vector x2 with elements 3,1 Join the elements of newVector and threes into 1 vector q solution Solution v <- newVector + threes v x2 <- c(3,1) newVector + x2 q <- c(newVector,threes) hands_on Hands-on: Exercise 13c Add a column called geneDensity to genomeSize containing the number of bp per gene for every organism Round the numbers to the nearest integer solution Solution dens.fl <- genomeSize$size / genomeSize$geneCount genomeSize$geneDensity <- round(dens.fl) Some functions only work on vectors. For instance sort() will sort data from smallest to largest (arguments allow other ordering) and order() returns the indices of the sorted elements: x [1] 1 3 11 1 7 sort(x) [1] 1 1 3 7 11 order(x) [1] 1 4 2 5 3 In the sorted vector the first element is also the first element of the original vector, the second element of the sorted vector has index 4 in the original vector etc. To sort a data frame use order() inside square brackets: mtcars[order(mtcars$mpg),] To sort on two columns (first on mpg, then on cyl): mtcars[order(mtcars$mpg,mtcars$wt),] To sort in descending order place a minus sign in front of the variable: mtcars[order(mtcars$mpg,-mtcars$wt),] Select the labels of a vector or table using names(). For tables rownames() and colnames() can access or set the either row or the column labels. Both functions will not work on vectors. The length() function retrieves the number of elements of a vector. Used on data frames it doesn’t throw an error but returns the number of columns instead. The same is true for match(x,y). It compares x and y and returns a vector with the same length as x containing: NA for elements of x that are not in y the index in y for elements in x that are in y On data frames it will not do an element-wise comparison but a column-wise comparison: match(D1,D2) will return a vector with length equal to the number of columns in D1 containing: NA for columns of D1 that are not in D2 the index in D2 for columns in D1 that are in D2 (so the complete column has to match, not the individual elements) Important is to see the difference between the + operator and sum(). The former works element-wise on two variables, the latter calculates the sum of all elements of one vector. There are also functions to be used only on tables, e.g. dim() returns how many rows and columns a table has, nrow() and ncol() will get these values individually t() transposes matrices (exchanges rows and columns), the output is a transposed matrix: the columns are the rows of the original matrix and vice versa Use merge() to join two data frames. Let?s say D1 has a column A with values. Data frame D2 has the same values stored in column A. Merge the two data frames on the basis of this common column: newD <- merge(D1,D2) If (some of) the values of the common column differ, merge() will ignore these values. Use argument all.x to add an extra row for every different value to the resulting data frame. All rows where the values of the two data frames don?t correspond, will be filled up with NA values. Most functions operate on numbers but there are also functions for manipulating text, e.g. paste(x,y,sep=\" \") concatenates two strings x and y (glues them together into one string) separating them by the character defined by sep. Arguments x and y can be strings but they can also be vectors. If they are vectors, they are concatenated element-wise to give a character vector result. Furthermore there are also functions specific for factors. For instance to select the names of the categories (levels) of a factor use levels() and table() to create a contingency table. table(cell_phone_data$own, cell_phone_data$grade) Figure 2: Example of a contingency table hands_on Hands-on: Exercise 13d You repeat the plant study experiment this time having the following numbers of plants developing lesions: 1, 6, 6, 5, 4 Add these data as a third column to the data frame Relabel columns to Day, Infected and Repeat Use paste() to add the word ?day? to the elements of the Day column. Look at the documentation first ! solution Solution Plant_study$repeated <- c(1,6,6,5,4) names(Plant_study) <- c(\"Day\",\"Infected\",\"Repeat\") ?paste Plant_study$Day <- paste(Plant_study$Day,\"day\",sep=\"\") question Question What will happen when you run this code ? paste(Plant_study[,\"Day\"],\"day\",sep=\"\") hands_on Hands-on: Exercise 13e Change the label of the second column of Drug_study to drug How many rows does Drug_study contain? Order the rows according to decreasing activity solution Solution colnames(Drug_study)[2] <- \"drug\" nrow(Drug_study) Drug_study[order(Drug_study$activity,decreasing=TRUE),] question Question What happens when you run this code ? colnames(Drug_study$ID) <- \"id\" question Question What happens when you run this code ? colnames(Drug_study[2]) <- \"blabla\" question Question What will happen when you run this code ? Drug_study[order(Drug_study$activity),\"ID\"] question Question What will happen when you run this code ? n <- order(Drug_study$activity,decreasing=TRUE) Drug_study[n,] hands_on Hands-on: Extra exercise 13f Sort the elements of z from smallest to largest Now use order(z). What’s the difference with the previous exercise? How many elements does z contain? solution Solution sort(z) order(z) length(z) hands_on Hands-on: Extra exercise 13g Add a new row to data frame ab containing values: 3,4,7 solution Solution d <- c(3,4,7) ab <- rbind(ab,d) hands_on Hands-on: Extra exercise 13h How many rows and columns are in the built-in data frame CO2 (data on CO2 uptake by plants) Use levels() to retrieve the names of the Treatment categories Create a contingency table with counts (number of plants) in every category of CO2 that is defined by Type and Treatment Use unique() to count how many plants were studied solution Solution dim(CO2) levels(CO2$Treatment) table(CO2$Type,CO2$Treatment) length(unique(CO2$Plant)) Functions helpful for working with large data sets Research in biology/medicine often generates very large data sets. When you work with very large data sets, it is often useful to show only a small part of the data set; head() shows the first 6 elements (vector) or rows (table) of a variable tail() prints the last 6 elements or rows hands_on Hands-on: Exercise 14a View the first 6 rows of the mtcars data frame Return TRUE if mtcars contains cars with 6 gears and FALSE if not How many cars with 3 gears are in mtcars? solution Solution head(mtcars) nrow(subset(mtcars,gear==6))!=0 nrow(subset(mtcars,gear==3)) Functions for finding indices of specific elements There are functions that help you locate specific values, the which functions: which.min(x) which.max(x) return the location (index) of the minimum, maximum or a specific value of a vector x. So max() will return the highest value in the data, which.max() will return the index of the highest value in the data. The argument of which() is a logical expression and which() will return the indices of the elements for which the logical expression is TRUE. x <- c(1,5,8,4,6) x # [1] 1 5 8 4 6 which(x == 5) # [1] 2 which(x != 5) # [1] 1 3 4 5 hands_on Hands-on: Exercise 15a Get the data of the patient with the highest activity in Drug_study solution Solution Drug_study[which.max(Drug_study$activity),] question Question What will happen when you run this code ? n <- which.max(Drug_study$activity) Drug_study[n,] hands_on Hands-on: Exercise 15b Get the index of the column called cyl in mtcars Create a data frame that contains the car with the lowest mpg for each category of cyl solution Solution which(names(mtcars)==\"cyl\") C4m <- mtcars[order(mtcars$cyl,mtcars$mpg),][1,] C6 <- subset(mtcars,cyl==6) C6m <- C6[which.min(C6$mpg),] C8m <- mtcars[order(-mtcars$cyl,mtcars$mpg),][1,] rbind(C4m,C6m,C8m) Checking and converting types of variables To check the data structure of an object you can use str() and the generic class() function: class(c(10,12,30)) # [1] \"numeric\" class(c(\"alana\",\"britt\",\"chris\")) # [1] \"character\" class(c(TRUE,TRUE,FALSE)) # [1] \"logical\" You can also use the specific is. functions e.g. is.numeric(), is.character(), is.Date(), is.vector(), is.matrix(), is.data.frame() etc. The is.na(x) function returns TRUE when an element of x is missing: x <- c(1,2,3,NA) is.na(x) # [1] FALSE FALSE FALSE TRUE To recode values to missing values you don?t need is.na(). Select the rows that contain the value you want to recode, e.g. 99, and change the value using an assignment: data$v1[data$v1==99] <- NA To exclude missing values you can use is.na() but there are alternatives. The problem with missing values is that when you apply arithmetic functions on variables that contain missing values they will return missing values and you will have no result. To circumvent this problem many functions have the na.rm argument. If you set na.rm=TRUE missing values are deleted before calculations are done. mean(x) # NA mean(x,na.rm=TRUE) # 2 The function na.omit() allows to create a new vector without missing values. If you apply this function on a data frame it will remove complete rows that contain one or more NA-values. newdata <- na.omit(x) You can convert the data type of an object by using the as. functions e.g. as.numeric(), as.character(), as.Date(), as.vector(), as.matrix(), as.data.frame() etc. hands_on Hands-on: Demo From the demo script run the Checking and converting data types section hands_on Hands-on: Exercise 16a We created a vector containing the days of the week and loaded this into a data frame called Plant_study. If we want to replace the days of the week by real dates, how should we proceed? To create a Date object in R: define the date as a string in the following format: 1970-01-01 transform the string into a date by using as.Date() Replace the days of the week by the dates of this week What type of data is Plant_study ? Convert Plant_study into a matrix called PS Did the conversion work? Look at the matrix to see if there is a problem. solution Solution Plant_study$Days <- as.Date(c(\"2019-01-09\",\"2019-01-10\",\"2019-01-11\",\"2019-01-12\",\"2019-01-13\")) class(Plant_study) PS <- as.matrix(Plant_study) PS hands_on Hands-on: Extra exercise 16b Check the data type of the second column of Drug_study. Retrieve the column using a comma. Convert the second column into a vector. What is different now? Look at the vector. solution Solution class(Drug_study[,2]) v <- as.vector(Drug_study[,2]) v hands_on Hands-on: Exercise 16c Instead of deleting missing values with na.omit() you can select the non-missing values. Create a vector with a missing value Multiply all elements with 2. What happens? Check if the 2nd element is missing Delete the missing value using is.na() and the strategy above solution Solution x <- c(1,2,3,NA) x*2 is.na(x[2]) x[!is.na(x)] hands_on Hands-on: Extra exercise 16d Check if z is a vector or a data frame Check if z contains numbers or characters Convert z into a matrix Convert the elements of z into characters solution Solution is.vector(z) is.data.frame(z) is.character(z) is.numeric(z) as.matrix(z) as.character(z) hands_on Hands-on: Extra exercise 16e Create a vector called words containing Hello, Hi Convert the words into numbers. What happens? solution Solution words <- c(\"Hello\",\"Hi\") as.numeric(words) R is smart enough to catch you if you try to do an illogical conversion, such as convert characters to numbers. It does the conversion but the data is converted to NA values."},{"title":"6 Forks","url":"/topics/git-introduction/tutorials/6_forks/tutorial.html","tags":[],"body":"1. Introduction In this chapter we will discuss a strategy for collaborating on projects. These strategies are especially useful when we do not have authorisation to change the content of someone else’s project, even though we still have a useful edit/suggestion in mind. Imagine that you’re starting a project with some colleagues and you want to version control the project. If it were to be a document where each of you needs to write part of it, you could simply start a Google Doc. For coding purposes the situation is a bit more complex. There might be a base version of the code already to which you need to add separate parts, however you always need to test whether your part is working together with the rest of the code. For this purpose, GitHub encourages the Fork & Pull workflow. Basically one forks a central repository, making it a personal forked repository. This repository can constantly be up to date with the central repository by merging those upstream changes in your personal forked repository. After you forked a repository, it will appear as a new repository in your GitHub account. The next step would be to clone the repository locally so you can work on the project from your computer. It’s always a good idea to make changes in a new branch and keep the main branch clean. Hence, after cloning the repository, you could make a new branch. Editing the files, staging, committing and pushing your changes remains the same and they will appear in your new personal forked repository. When you are happy about your changes, when all the commits are pushed to your forked repository, these changes can be merged back into the central repository by creating a pull request. The main author can now decide whether he/she is happy about your suggestions and can include (part of) them. This workflow leaves the central repository untouched untill the moment you want to incorporate changes. Two important terms in this fork & pull workflow are: upstream: generally refers to the original repository that you have forked origin: is your fork: your own repository on GitHub As mentioned in section 4.4, the “origin” is used to refer to the GitHub original repository’s URL. This also lasts here. The remote origin refers to your fork on GitHub, not the original repository it was forked from. To summarize the above, the Fork & Pull workflow consists of the following steps: Fork Clone Branch Stage-commit-push Pull request 2. Fork Let’s explore GitHub first. GitHub is like the Facebook of programmers. You can see someone’s account, what that person has been working on, find new projects (relatable to a Facebook page), etc. Exploring new repositories is possible by clicking on the ‘Explore’ button in the navigation bar. Searching a specific repository or searching for an account, on the other hand, is possible by simply typing it in the search bar in the navigation bar. Search for the VIB Bioinformatics Core account ‘vibbits’ and find the repository ‘fork-repository’. This repository was made specifically for learning the concept of forking. Do this by clicking the fork button in the upper right corner. The repository has been successfully forked if you see something similar to the figure below. The icon represents a fork, followed by your GitHub account name and the name of the repository. The second line tells us that the upstream repository is the ‘vibbits/forked-repository’. 3. Changes Clone this repository locally, make a branch (e.g. name it yourname) and do some edits in that branch. Add your name, accountname or initials and the date to the participants.txt file. For this exercise we will only edit the participants.txt file. The flow here remains the same: stage-commit-push. After these changes took place, we will have a similar situation In normal circumstances it is possible that the upstream repository has changed in the meantime. The indicator would then note that there are new commits in the upstream (1 commit behind vibbits:main), while the branch/repository itself is one commit ahead. This does not (really) affect the pull request. In any case, the following step is to create a pull request. 4. Pull request The two repositories have diverged during the previous steps. Now its time to create a pull request between these repositories. After clicking the Pull request a new screen pops up that looks very similar to the one seen in Chapter 5 (Branching & merging). Note that moste developers do not really appreciate it if you try to merge your changes straight into the main branch. Usually, they would write some suggestions as to how we can collaborate on a project preferably. Let’s assume that the developers of this repository expect you to merge changes into the dev branch, than it would look something like this: GitHub tells us: It compared the main branch of the forked repository (in my case tmuylder/fork-repository) with the upstream (base) repository vibbits/fork-repository. It’s able to merge these two branches without any conflicting errors It summarizes the changes that have been done in the branch that will be merged into the upstream. If all seems good, we can create the pull request. In the case that there are any conflicting errors, they will need to be solved first. Afterwards we only need to add a message that accompanies the pull request. A brief overview of the pull request is given in the following screen which either allows you to merge the pull request into the upstream repository yourself or which requests the maintainer of the upstream repository to review and merge the pull request. In the latter case, the maintainer will thereafter receive a notification showing the pull request. An overview of all pending pull requests where you are involved in, are consultable on the pull requests tab of the navigation bar. 5. Overview To briefly summarize, the steps that we took were: fork > clone(> branch > edit-stage-commit-push > pull request (> merge) and represent a strategy for collaborating on projects. These strategies are especially useful when we do not have authorisation to change the content of someone else’s project, even though we still have a useful edit/suggestion in mind. What if the upstream repository changed while you were working on your local repository? In this case a pull request should be done in which the receiving branch is your forked repository. Hence, the order of the branches as depicted in the figure above would be swapped. hands_on Exercise Merge upstream changes in your forked repository. This approach is useful if you are working on a project that is prone to lots of changes and you need to keep up to date. Note: This exercise is only possible to be performed if the repository vibbits/fork-repository has changed after you forked it. solution Solution You need to merge any upstream changes into your version, and you can do this with a pull request on GitHub too. This time though you will need to switch the bases of the comparison around, because the changes will be coming from the upstream version to yours. First find the following notification in your repository and click on pull request: In my case, the order is not how it’s supposed to be and the message reads: “There isn’t anything to compare. vibbits:main is up to date with all commits from tmuylder:main.”. Click on switching the base in order to insert the changes from the upstream in your forked repository. A message similar to the following will allow to create a pull request and subsequently merge the changes into your forked repository. Let’s continue with the next session!"},{"title":"05 Lists, Tuples and Sets","url":"/topics/python-programming/tutorials/5_lists_and_tuples/tutorial.html","tags":[],"body":"5.1 Introduction So far we’ve seen variables where you essentially assign a value to a name that you can use in the program. It is also possible to assign groups of values to a name, in Python these are called lists and tuples - variables that contain multiple values in a fixed order. Python also has sets, which are also variables that contain multiple values, but in no particular order. In section 8 we will also discuss dictionaries. By means of a brief summary, already in this stage; there are four collection data types in Python: List is a collection which is ordered and changeable. Allows duplicate members. Use square brackets [] for lists. Tuple is a collection which is ordered and unchangeable. Allows duplicate members. Use normal brackets () for tuples. Set is a collection which is unordered and unindexed. No duplicate members. Use curly brackets {} for sets. Dictionary is a collection which is unordered, changeable and indexed. No duplicate members. Use curly brackets {} for dictionaries (see section 8). They are useful in different circumstances and each data-type has its own advantage. On a small-case example this might not be noticable, however on a larger scale using the right data-type can save you a lot of time. 5.2 Lists and range You can make your own Python list from scratch: myList = [5,3,56,13,33] myList You can also use the range() function. Try this: myList = list(range(10)) myList You should get the following output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. This is a list of integers - you can recognize a list by the square [ ] brackets. Note that Python always starts counting from 0. The command above will give you a series of integers starting from 0 and stopping at the number you defined, however with this number not included in the list. Hence, it stops at 9. You can start from a different number as well: myList = list(range(3,12)) myList or increase the step size (the default is step size is 1): myList = list(range(1,12,2)) myList An important feature of lists is that they are flexible - you can add and remove values, change the order, … . You can do such modifications by calling a method from the list itself. Some examples of methods are: Add elements append() to append an item to the end of the list insert() to add an item at the specified index extend() to extend an item Delete elements remove() to remove the specified item pop() to remove the specified index (or the last item if index is not specified) del keyword removes the specified index clear() method empties the list Sorting: sort() will sort the list in an ordered way reverse() will reverse the order of the list Copy of a list with the copy() method myList = [] # Create an empty list myList.append(5) # Add a single value to the back of the list myList myList.insert(0,9) # Insert a value in the list at index (element position) 0 myList myList.extend([99,3,5]) # Extend the list with another list myList myList[0] # Return the first element in the list (counting starts at zero) myList[2] # Return the third element in the list myRemovedElement = myList.pop(3) # Remove the fourth element in the list and return it print(\"I removed {}\".format(myRemovedElement)) myList myList.sort() # You can sort the elements in a list - this will change their order myList myList.reverse() # Or reverse the order of the list myList You can also select a slice from a list - this will give you a new list: myList = list(range(15)) myListSlice = myList[3:6] myListSlice myListCopy = myList[:] print(myListCopy) print(myList[-4:]) # This will select the fourth-last to the last element in the list There are two other methods you can use on lists: index() returns the index of the first element with the specified value count() returns the number of elements with the specified value myList = list(range(1,15)) myList myList.count(10) # Will count the amount of times the value 10 occurs in this list myList.count(\"A\") # This always works, and will return 0 if nothing is found myList.index(10) # Will give the index of the element with value 10 - in this case 9 because the list index starts at 0. #print(myList.index(\"A\")) # This will crash the program - the value to look for has to be present in the list!!! hands_on Exercise 5.2.1 Take the list [54,56,2,1,5223,6,23,57,3,7,3344], sort it in reverse order (largest value first) and print out the third value. solution Solution # Take the list [54,56,2,1,5223,6,23,57,3,7,3344], sort it in reverse order (largest value first) and print out the third value. myList = [54,56,2,1,5223,6,23,57,3,7,3344] myList.sort() myList.reverse() print(myList[2]) #The first element is at index 0, the third at index 3! 5.3 Tuples Similar to lists are tuples - essentially they are the same, except that a tuple cannot be modified once created. This can be useful for values that don’t change, like (part of) the alphabet for example: myTuple = (\"A\",\"B\",\"C\",\"D\",\"E\",\"F\") myTuple Important to remember is that if you create a tuple with one value you have to use a comma: myTuple = (\"My string\",) myTuple myWrongTuple = (\"My string\") # The brackets here don't do anything. myWrongTuple A tuple is indicated by round brackets ( ). You can interconvert between lists and tuples by using list() and tuple(): myTuple = (\"A\",\"B\",\"C\",\"D\",\"E\",\"F\") myList = list(range(10)) myNewTuple = tuple(myList) myNewList = list(myTuple) print(\"{} and {}\".format(myList, myNewTuple)) print(\"{} and {}\".format(myTuple, myNewList)) You can find out the length (number of elements) in a list or tuple with len(): myTuple = (\"A\",\"B\",\"C\",\"D\",\"E\",\"F\") myTupleLength = len(myTuple) myTupleLength Tuples are faster during iteration procedures due to their immutability. hands_on Exercise 5.3.1 Start with the tuple ('a','B','c','D','e','F'), sort it, take the fourth value out, and print the result. solution Solution # Start with the tuple ('a','B','c','D','e','F'), sort it, take the fourth value out, and print the result. myTuple = ('a','B','c','D','e','F') myList = list(myTuple) myList.sort() #print(myList) print (\"Removing {}\".format(myList.pop(3))) print (\"Result is {}\".format(str(tuple(myList)))) 5.4 Strings Strings are a bit like lists and tuples Strings are really a sequence of characters, and they behave similar to lists: myString = \"This is a sentence.\" myString[0:5] # Take the first five characters myString.count(\"e\") # Count the number of 'e' characters myString.index(\"i\") # Give the index of the first 'i' character You cannot re-assign strings as you do with lists though, the following example does not work: myString = \" This is a sentence. \" print(myString.upper()) # Upper-case all characters print(myString.lower()) # Lower-case all characters print(myString.strip()) # Strip leading and trailing spaces/tabs/newlines print(myString.split()) # Split the line into elements - default is splitting by whitespace characters print(myString.replace(' is ',' was ')) # Replace ' is ' by ' was '. Spaces are necessary, otherwise the 'is' in 'This' will be replaced! A list with all string methods and a full description can be found in the Python documentation, or simply type dir(myString) dir(myString) hands_on Exercise 5.4.1 Ask the user for two words, then check whether they are the same (upper or lower case should not matter),if not check whether they have the same first letter (again case should not matter). If not, then print their length. solution Solution # Ask the user for two words, then check whether they are the same (upper or lower case should not matter),if not check whether they have the same first letter (again case > > should not matter). If not, then print their length. firstWord = input(\"Give first word:\") secondWord = input(\"Give second word:\") print(len(firstWord)) if firstWord.upper() == secondWord.upper(): print(\"Words are the same (ignoring case).\") elif firstWord[0].upper() == secondWord[0].upper(): print(\"Words share the same first letter (ignoring case).\") else: print(\"Word lengths are {} and {}\".format(int((len(firstWord))),int(len(secondWord)))) 5.5 Sets Very useful as well are sets. These are unordered and unindexed (so the order in which you put in elements doesn’t matter), and it is much easier to compare them to each other. Because sets cannot have multiple occurrences of the same element, it makes sets highly useful to efficiently remove duplicate values from a list or tuple and to perform common math operations like unions and intersections. Source: https://www.learnbyexample.org/python-set/ You initialise them by using set() on a list or tuple: mySet1 = set(range(10)) mySet2 = set(range(5,20)) print(mySet1) print(mySet2) mySet.add(5) # Elements in a set are unique - the set will not change because it already has a 5 print(mySet1.intersection(mySet2)) print(mySet1.union(mySet2)) dir(mySet1) The principle of using intersection and union is the same as the Venn diagrams you probably saw in school… You can also make a set out of a string: myString = \"This is a sentence.\" myLetters = set(myString) myLetters # Note that an upper case T and lower case t are not the same! There are more things you can do with sets which we will not go into here, see the Python sets documentation for more information. hands_on Exercise 5.5.1 Which letters are shared between the words “perspicacious” and “circumlocution”? solution Solution # Which letters are shared between the words \"perspicacious\" and \"circumlocution\"? firstWord = \"perspicacious\" secondWord = \"circumlocution\" firstLetterSet = set(firstWord) secondLetterSet = set(secondWord) print(firstLetterSet.intersection(secondLetterSet))"},{"title":"03 Print formatting","url":"/topics/python-programming/tutorials/3_print_formatting/tutorial.html","tags":[],"body":"3.1 Introduction There are several ways to present the output of a program, data can be printed in a human-readable form, or written to a file for future use. Sometimes users want more control over the formatting of output, rather than simply printing space-separated values. There are several ways to format output which we will cover in this section. The following figure (which I shamelessly copied from here) helps to visualize the .format() argument. If you don’t understand it completely, don’t worry, we’ll cover it in this section: Everything between the double quotation marks is what will be printed (thus the print() statement is missing). Between curly brackets you can find lay-out options for the arguments, the arguments themselves are given within the .format() statement. The first number defines the argument that will be printed (Python starts counting at 0), the number behind the colon (:) defines the number of characters that is foreseen for the argument, and lastly the number behind the point (.) is only applicable for floats and defines the amount of decimals that will be printed. E.g.: 1:8.2f will print the first argument with 8 characters/numbers of which two decimals and the type of the argument is a float. If the argument has less than 8 characters/numbers than whitespace will be used. 3.2 Using .format() The following example gives the most basic use form of the .format() statement. print(\"My name is {}.\".format(\"Jane\")) The above doesn’t do anything interesting; you can however put a number in between the curly brackets {} to force the output to take up a number of characters. Try this: print(\"My name is {:>10}.\".format(\"Jane\")) You’ll now see that you force an area of 10 characters to put the name. If the name is shorter, the remaining empty characters will be whitespaces. If the name would be longer, the number will be overruled. Note that the > character in the .format() form can be used to determine the alignment (use for right align and = for centered). There are a number of differences between the old Python (version 8}%\".format(100)) 3.3 Formatting numbers Here are some examples of formatting integers (digits): print(\"This is {:d}.\".format(252)) print(\"This is {:d} and {:d}.\".format(25,30)) Here are some examples of formatting decimal number (floating point): myFloat = 4545.4542244 print(\"Print the full float {},\\ncut off decimals {:5.2f},\\nor determine the characters before the decimal {:10.1f}.\".format(myFloat,myFloat,myFloat)) # Or in old style # print(\"Print the full float %f,\\ncut off decimals %.2f,\\nor determine the characters before the decimal %10.1f.\" % (myFloat,myFloat,myFloat)) 3.4 Special characters For some characters it is necessary to use what are called ‘escape’ codes because you cannot type the character normally from the keyboard. Try this: print(\"The \\ sign\\ncan\\talso\\tbe\\tprinted.\") Here the \\ will print a backslash (however Python might think you are trying to insert a special code and in order to be safe it’s better to type a double \\\\), the \\n will print a new line, \\t a tab character. Escape codes are necessary if you are trying to print a single or double quote: print(\"He said: \\\"Hello\\\".\")"},{"title":"09 Files","url":"/topics/python-programming/tutorials/9_files/tutorial.html","tags":[],"body":"9.1 Introduction More often than not the data you need for your program will come from somewhere else - either from user input or a file. Especially for more complex data, it becomes essential to be able to read in data files, do something with the data, and write out a new file with modified information or a set of analysis results. 9.2 Reading files To read in a file you have to create a file handle. This is a sort of connection to the file that you can use to pull data from it. You create a connection to a file by using the open() function. Whenever you’re done using the file, it’s good practice to close the file handle. # Open the file fileHandle = open(\"data/readfile.txt\") # Close the file fileHandle.close() # Nothing happened... All this does, is creating this connection, the file has not been read. In order to read in a file, there are a couple of possibilities: readline() - read the first line of the file as one string. readlines() - read all of the lines in the file. Each line is one string. The lines are combined as a list of lines (strings). read() - read the whole file as one string. Each method has its advantage. E.g. if you’re searching for the presence of a word or string in a file, given that the file is not too big, you can use read. If you want to process an enormously big file and from each line you need to extract, process and save the information, than it’s better to read line by line with readline within a for-loop. Try to understand the difference of these methods while you go through this section. Given the file readfile.txt in a folder named data: This is the first line. Here is a second one. And there is also a third line. Using read: Note that the three different lines are read in one long string. This is how the read function works. fileHandle = open(\"data/readfile.txt\") fileHandle.read() fileHandle.close() Using readline: Readline reads in the following line. It starts with the first one. When you call the method again, it will print the second line. It’s important to understand this as you can exploit this method in a for-loop to access each line separately. fileHandle = open(\"data/readfile.txt\") fileHandle.readline() fileHandle.readline() fileHandle.close() Using readlines: Instead of reading the lines of a file one by one, you can also do it in one go. As explained above, each line is one string and all of the lines/strings are stored in a list. fileHandle = open(\"data/readfile.txt\") fileHandle.readlines() fileHandle.close() Knowing this we can move on to more complex examples. First make sure to find the PDB file TestFile.PDB in your data folder or download this fake PDB coordinate file for a 5 residue peptide and save it in the data directory. In the example below we will read all the lines in the file (as separated by a newline character), and store them in the variable lines. Each element in this list corresponds to one line of the file! When this is done, we close the file. # Read in the file per line fileHandle = open(\"data/TestFile.pdb\") lines = fileHandle.readlines() # Close the file fileHandle.close() # Print number of lines in the file print(\"There are:\", len(lines), \"lines in the file\") # Loop over the lines, and do some basic string manipulations for line in lines: line = line.strip() # Remove starting and trailing spaces/tabs/newlines print(line) line = lines[10] line = line.strip().split() line[-1] Now you can do many other things with the data in the file. E.g. if you want to count the number of times a carbon element appears in the file. # Open the file fileHandle = open(\"data/TestFile.pdb\") # Read all the lines in the file (as separated by a newline character), and store them in the lines list # Each element in this list corresponds to one line of the file! lines = fileHandle.readlines() # Close the file fileHandle.close() # Initialise the line counter lineCount = 0 # Loop over the lines for line in lines: columns = line.strip().split() if columns[-1] == 'C': # Alternatively, use \"if ' C ' in line:\" print(line, end='') # Using the 'end' argument in the print because the line already contains a newline at the end # otherwise will get double spacing. lineCount += 1 print(\"Number of lines with ' C ': {}\".format(lineCount)) You should find 75 lines - note that in this case, for those who know the PDB format a bit, you’re finding all carbon atoms. Alternatively, you can use the with() statement to open files. The example here above would then become: with open(\"data/readfile.txt\") as fileHandle: for line in fileHandle: print(line) This method is often used as it does not require you to keep track of the open file in your mind, as well as clearer syntax. 9.3 Writing a file Writing a file is very similar, except that you have to let Python know you are writing this time by adding the 'w' parameter in the open() function. Actually Python needs two arguments, however it assumes that if you only give one parameter (the file that it has to read), the other one is 'r' which stands for reading mode. For the sake of the example, we’re writing a new file and call it writefile.txt: f = open('data/writefile.txt','w') f.write('Now we have a new file \\n') f.write('Because Python automatically makes this file and writes some text to it.') f.write('Btw, if you don\\'t specify the newline characters, it will append the string at the end of the last line') f.close() f = open('data/writefile.txt') text = f.read() print(text) f.close() Be careful - if the file exists already it will be overwritten without warning! The file is written to the directory you’re executing the program in - have a look! hands_on Exercise 9.3.1 Read in the file TestFile.pdb, and write out all lines that contain ‘VAL’ to a new file. solution Solution 1 # Read the file f = open(\"data/TestFile.pdb\",\"r\") g = open('data/withval.pdb','w') # Loop over the lines for line in f: if 'VAL' in line: # Alternatively, use \"if ' C ' in line:\" if 'ATOM' in line: g.write(line) f.close() g.close() solution Solution 2 # Open the file fileHandle = open(\"data/TestFile.pdb\") # Read all the lines in the file (as separated by a newline character), and store them in the lines list # Each element in this list corresponds to one line of the file! lines = fileHandle.readlines() # Close the file fileHandle.close() # Track the lines with VAL linesToWrite = [] # Loop over the lines for line in lines: if line.count(\"VAL\"): # Alternatively, use \"if ' C ' in line:\" linesToWrite.append(line) # Write out the lines fileHandle = open(\"data/fileWithVAL.pdb\",'w') for line in linesToWrite: fileHandle.write(line) # Close the file fileHandle.close() solution Solution 3 # Read the file f = open(\"data/TestFile.pdb\",\"r\") # Track the lines with VAL linesToWrite = [] # Loop over the lines for line in f.readlines(): if line.count(\"VAL\"): # Alternatively, use \"if ' C ' in line:\" linesToWrite.append(line) # Write out the lines fileHandle = open(\"data/fileWithVAL.pdb\",'w') for line in linesToWrite: fileHandle.write(line) # Close the file fileHandle.close() 9.4 Advanced file reading and interpretation hands_on Exercise 9.4.1 Read in the TestFile.pdb file, print out the title of the file, and find all atoms that have coordinates closer than 2 angstrom to the (x,y,z) coordinate (-8.7,-7.7,4.7). Print out the model number, residue number, atom name and atom serial for each; the model is indicated by: MODEL 1 lines, the atom coordinate information is in: ATOM 1 N ASP A 1 -10.341 -9.922 9.398 1.00 0.00 N lines, where column 1 is always ATOM, column 2 is the atom serial, column 3 the atom name, column 4 the residue name, column 5 the chain code, column 6 the residue number, followed by the x, y and z coordinates in angstrom in columns 7, 8 and 9. note that the distance between two coordinates is calculated as the square root of (x1-x2)²+(y1-y2)²+(z1-z2)². solution Solution # Open the file fileHandle = open(\"data/TestFile.pdb\") # Read all the lines in the file (as separated by a newline character), and store them in the lines list # Each element in this list corresponds to one line of the file! lines = fileHandle.readlines() # Close the file fileHandle.close() # Initialise some information searchCoordinate = (-8.7,-7.7,4.7) modelNumber = None # Loop over the lines, and do some basic string manipulations for line in lines: line = line.strip() # Remove starting and trailing spaces/tabs/newlines # Only do something if it's not an empty line if line: cols = line.split() # Split the line by white spaces; depending on the format this could be commas, ... # Print the title if cols[0] == 'TITLE': title = line.replace(cols[0],'') title = title.strip() print(\"The title is '{}'\".format(title)) # Track the model number elif cols[0] == 'MODEL': modelNumber = int(cols[1]) # For atom lines, calculate the distance elif cols[0] == 'ATOM': # Set some clear variable names and convert to the right type atomSerial = int(cols[1]) atomName = cols[2] residueNumber = int(cols[5]) x = float(cols[6]) y = float(cols[7]) z = float(cols[8]) # Calculate the distance distance = ((x - searchCoordinate[0]) ** 2 + (y - searchCoordinate[1]) ** 2 + (z - searchCoordinate[2]) ** 2 ) ** 0.5 if distance < 2.0: print(\"Model {}, residue {}, atom {} (serial {}) is {:.2f} away from reference.\".format(modelNumber,residueNumber,atomName,atomSerial,distance)) 9.5 Next session Conclusion"},{"title":"04 Conditions","url":"/topics/python-programming/tutorials/4_conditions/tutorial.html","tags":[],"body":"4.1 Introduction Programs start to become more interesting if you can do different things depending on the input. For this, you have to use conditions, which we will discuss in this section. Decisions will be taken based on a condition. In this perspective, we highlight the importance of understanding booleans True and False, as well as the None-keyword once more. 4.2 If statement The if condition allows you to only execute a bit of code if a (set of) condition(s) is satisfied. Python syntax requires that you put a colon : after the if, and that the block of code that is conditional is indented with the same amount of spaces (or tabs). Python doesn’t really care about the number of spaces or tabs, as long as you’re consistent. Jupyter notebook uses tabs, hence it is best to follow along. Now try this: x = 5 if x == 5: print(\"x is five!\") if x!=5: print(\"x is not five!\") you will see that only the block of code under x == 5 is printed out. You can of course make the conditions more complex and combine them with and and or: x = 5 y = 10 if (y / x) == 2: print(\"y divided by x is 2!\") if y == 10 or x == 2: print(\"x is two or y is ten\") if y == 10 and x == 2: print(\"x is two and y is ten\") print(\"The end\") Here you see that the blocks for the first two conditions (which are True) are executed, but not the third. The last line of code is always printed off - it’s on the same level as the start of the code, and not conditional. 4.3 Indentation Python relies on indentation (whitespace at the beginning of a line) to define scope in the code. Other programming languages often use (curly) brackets for this purpose. The level of indentation is crucial, and Python will immediately give an error if there are inconsistent levels of indentation in the code. Try this: x = 5 y = 10 if (y / x) == 2: print(\"y divided by x is 2!\") print (\"And x is {}!\".format(x)) Note that this can also happen if you start mixing space and tab characters! hands_on Exercise 4.3.1 Write a program where you ask the user for x and y, make sure that y is not zero, and print out x/y. # Modify the code below on the ... locations: xString = input(...) yString = input(...) x = ...(xString) y = ...(yString) if ... : print(\"Error, your y-number is 0\") if ... : print(\"x divided by y = {:.2f}\".format(...)) solution Solution # Write a program where you ask the user for x and y, make sure that y is not zero, and print out x/y. xString = input(\"Give a number: \") yString = input(\"Give another number that is not zero: \") x = float(xString) y = float(yString) if y == 0: print(\"Error, you're y-number is 0\") if y != 0: result = x/y print(\"x divided by y = {:.2f}\".format(result)) 4.4 Elif statement Once you have an if-condition, you can directly follow it up with an elif (else if) condition. This is not the same as another if-statement. An elif is only executed if the previous if (and other preceding elifs) are not True. In the example below the code in section 4.3 is adapted. Now all if-statements are changed by elifs. x = 5 y = 10 if (y / x) == 2: print(\"y divided by x is 2!\") elif y == 10 or x == 2: print(\"x is two or y is ten\") elif y == 10 and x == 2: print(\"x is two and y is ten\") print(\"The end\") Now only the code under the first condition is executed, not the second (the third is not True and is in any case irrelevant). If we switch the conditions around a bit: x = 5 y = 10 if y == 10 and x == 2: print(\"x is two and y is ten\") elif y == 10 or x == 2: print(\"x is two or y is ten\") elif (y / x) == 2: print(\"y divided by x is 2!\") print(\"The end\") The first condition is not True, so the second is evaluated. This one is True, so it is executed, and the text ‘x is two or y is ten’ is printed. For clarity it is often useful to leave some space before and after the (set of) condition(s) - it makes the code easier to ‘read’ afterwards. hands_on Exercise 4.4.1 Write a program where you ask the user for two words. Compare the words; if they are the same, print a message, if the first or second word is ‘Stop’, then also print a message. solution Solution # Write a program where you ask the user for two words. Compare the words; if they are the same, print a message, if the first or second word is 'Stop', then also print a > > message. print(\"Give two words.\") firstWord = input(\"Write a word: \") secondWord = input(\"Write another word: \") if firstWord == secondWord: print(\"These words are the same\") elif firstWord ==\"Stop\" or secondWord == \"Stop\": print(\"You're word was Stop, hence we stopped here\") print(\"The end\") 4.5 Else statement You can also end an if (with or without elifs) with an else condition. The block of code following else is only executed if the previous (set of) conditions are all False. Try this: x = 7 if not (x % 2): print(\"x is divisible by two!\") elif not (x % 3): print(\"x is divisible by three!\") else: print(\"x is not divisible by two...\") print (\"x is {}\".format(x)) You can modify the value of x a bit to see what else can happen. Can you spot a problem with this example? What will happen if x can be divided by both two and three? What can you do to solve this problem? hands_on Exercise 4.5.1 Modify the code above so it prints that it is divisible by two and three when this is the case. solution Solution # If a value can be divided by two and three, only the block of code under the first condition will be executed, so you will not find out whether your value can be divided by three! There are several solutions to this, for example: x = 12 if not (x % 2): print(\"x is divisible by two!\") if not (x % 3): print(\"x is divisible by three!\") elif not (x % 3): print(\"x is divisible by three!\") else: print(\"x is not divisible by two or three...\") print (\"x is {}\".format(x)) # This is not a very elegant solution however, as you are repeating the same bit of code twice to find out whether the value can be divided by three. This one might be slightly better: x = 12 if not (x % 2): print(\"x is divisible by two!\") if not (x % 3): print(\"x is divisible by three!\") if (x % 2) and (x % 3): print(\"x is not divisible by two or three...\") print (\"x is {}\".format(x)) # However you still have to repeat the conditions, which would become very tedious (and error-prone) if you were to try division by many values. The next example is a bit more verbose but cleaner and more 'extendable' for other values: x = 12 xDivisible = False if not (x % 2): print(\"x is divisible by two!\") xDivisible = True if not (x % 3): print(\"x is divisible by three!\") xDivisible = True if not xDivisible: print(\"x is not divisible by two or three...\") print (\"x is {}\".format(x))"},{"title":"5 Branches and merging","url":"/topics/git-introduction/tutorials/5_branches/tutorial.html","tags":[],"body":"1. What’s a branch? The idea of branching is that we can create a copy of the project in which we can add a new feature. This branch is a completely separate version of your project and lives next to your original version. If the new feature is working properly we can merge it back into the project. It’s a great way of testing new changes in some code when you’re not sure whether it will work, and in the meanwhile not messing up the code that you already have. The original repository is now called the master branch, however historically was called the main branch. A new GitHub repository is initialized by default with one branch: the main branch. All the changes in our project that we did so far, have hence always been in this main branch. Remember that when we did git status we read a line saying that we were on the main branch. If we would make a new branch, we can name it however we like (e.g. new-feature). There are two ways of doing this: locally or on the GitHub website. We will first show you the latter (section 2) and afterwards how to do it locally via Git Bash or the Terminal (section 4). A repository can have numerous branches. Branches are ways of organising work on a project: you can have a branch for a new feature, for trying out something new, for exploring an issue - anything at all. It’s a good practice to create a new branch for every new bit of work you start doing, even if it’s a very small one. It’s especially useful to create a new branch for every new feature you start working on. Branches are of course disposable, you can always remove them. 2. Branching on GitHub We can make a new branch on GitHub. Click the button: ‘Main’ In ‘Find or create a branch…’ type new-feature (or any other name) Click ‘Create branch’: new-feature GitHub will now display new-feature. It’s very important to understand that any changes that happen in this branch, will not be influencing the main branch. hands_on Exercise 5 Edit the plot2.R file again, however make sure you’re in the new-feature branch. Add the following lines that will make a new plot. These lines will allow us to investigate the relation between the weight, horsepower and miles per gallon variables of mtcars dataset in R. # Install requirements & plotting of 3D scatterplot install.packages(\"scatterplot3d\") library(scatterplot3d) attach(mtcars) scatterplot3d(wt,hp,mpg, pch=16, highlight.3d=TRUE, type=\"h\", main=\"3D Scatterplot\") solution Solution Edit the file plot2.R by clicking on the pencil icon and add the following lines: Commit your changes with a useful commit message and save by clicking the green ‘Commit changes’-button. Switch back to your main branch and have a look to the plot2.R-file. It shouldn’t contain these changes. 3. Merging branches on GitHub Before exploring how we make branches on our computer locally, we’ll merge the changes in the new-feature branch into the main branch. Branches are merged by making a pull request. In this section we will explain how to do a pull request, often shorted to PR. Whether you are on the main or new-feature branch, doesn’t matter. In both cases you should see the following yellow screen. Alternatively, go to ‘Pull requests’ and find it there. Click on compare & pull requests or go to the section Pull requests and create a New pull request (select the branches you want to incorporate). A new screen pops-up with the following information. The pull request should be interpreted as a request to pull the new branch and all of its changes into the main branch. The base where it would be pulled towards is base: main. The branch where the changes are deriving from is compare: new-feature. Note that GitHub checks the compatibility of the branches: in this case there are no conflicting edits and the branches can be merged together. Give a descriptive title text and if appropriate some additional comment. Underneath the pull request related information, GitHub also gives you a summary of the changes that were done. Each commit from the branch new-feature (i.e. only added these 7 lines in this case) Display of the file and a visual representation of what changed in that commit. Click on Create pull request to finalize the creation of the PR. Note that the the branches are not merged yet, one more comment before we do that! We know that GitHub allows us to collaborate on projects. Here we can find some of the features that GitHub is providing us to start collaborating. We could for example start a conversation here and discuss the PR, select a (couple of) reviewer(s), add assignees who authored, add labels representing what type of edits were done in the branch, etc. Essentially these are valuable for organizing bigger projects; keep track of who’s working on what and who needs to review specific changes, etc. Finally, we verify the merge pull request commit and you give your consent to GitHub to merge both branches by clicking ‘Merge pull request’. It might be possible that in a project with several people, you are not authorized to make changes to the main branch. In this case you will always have to work in a separate branch and someone else will get this last message. He or she will then decide whether this pull request should be merged. 4. Branching locally Besides the possibility of making branches on GitHub, we can also do it locally on our computer. As we’ve made changes to the repository on GitHub, we’ll start with pulling the changes into our local repository. Use git pull in your project folder. There is always an indication in the Terminal or Git Bash of which branch we are in (i.e. main). Here are the most important commands related to making branches and switching between different branches: Listing all the existing branches is possible with git branch -a git checkout -b : will create a new branch and move into this branch. git branch : will create a new branch, but will remain in the current branch (i.e. the main branch in this case) With git checkout we will switch from one branch to the other. Let’s start with listing all the existing branches (4). * main remotes/origin/HEAD -> origin/main remotes/origin/main remotes/origin/new-feature The first branch is our local main branch in which we are currently working (as denoted by the asterisk *). The three others relate to the branches that exist remotely on GitHub. If we want to work on the branch new-feature we will have to import it first with: git checkout new-feature. Git will understand that there is a remote branch with the same name and you want to work on this one. Note that if you use git checkout -b new-feature, you would have created a new branch with the same name as the remote branch. This is error prone and will lead to problems! Hence, it is really important that you switch from branch and not create a new one! 4.1. Example workflow An example workflow is depicted in the figure below and is discussed in the following points. 1. Make a new branch: git checkout -b Git will make a new branch with the name and tell you it switched to the new branch. If you want to change branches, just type git checkout followed by the name of the branch, e.g. git checkout main. 2. Make some changes: Add a new file Edit an existing file 3. Stage changes: Use the following command to simply add all the new or changed files. git add -A 4. Commit staging area: Commit all the staged files with: git commit -m \"some useful commit message\" 5. Push commits to GitHub: git push origin or alternatively: git push --set-upstream origin The git push command is now a bit longer. The first time we want to publish a new local branch on a remote repository (GitHub), we need to be explicit and tell Git to add the to the origin. In Git, the “origin” is used to refer to the GitHub original repository’s URL and makes it much easier to talk about. Next time you want to push your commits from new-branch, you won’t need to be explicit - you can simply do git push, because now new-branch exists on GitHub and both branches know how to commmunicate with each other. hands_on Exercise 6 Make a new branch and make sure you’re in the branch. Rewrite the README.md file so it contains the following text. Once the changes have been committed and pushed to GitHub, create a pull request and merge the changes into the main branch. # Downstream data-analysis R This repository contains all the scripts for the downstream data analysis of my project. solution Solution git checkout -b readme Do the necessary changes git add README.md git commit -m \"changed README file completely\" git push origin readme Find the new branch in your GitHub repository. From there the solution is identical as discussed here above. As a final note on merging branches, we mention here that it is obviously also possible to merge branches on our computer locally. For this, we refer to some further reading materials linked here. 5. Deleting branches 5.1. Via GitHub If a branch is of no more use, we can delete it. To find all the existing branches in GitHub, click on branches in the top left corner of the repository. After successfully merging our changes in the main branch, the old one(s) can be deleted. Click on the waste bin: Go back to the main tab of the repository and find that the branch has been deleted. 5.2. Locally Deleting a branch is as simple as typing the following command: git branch -d If git refuses to do so, there is a forced way to do it as well by using the capital -D parameter. Let’s continue with the next session!"},{"title":"12 Plotting","url":"/topics/python-programming/tutorials/12_plotting/tutorial.html","tags":[],"body":"12. Plotting figures This chapter is based on the materials from this book and this website Matplotlib is a Python 2D plotting library which produces publication quality figures. Although Matplotlib is written primarily in pure Python, it makes heavy use of NumPy and other extension code to provide good performance even for large arrays. We will start with the basics concepts being figures, subplots (axes) and axis. The following line of code allows the figures to be plotted in the notebook results %matplotlib inline matplotlib.pyplot is a collection of command style functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current subplot. What we first have to do is importing the library of course. import matplotlib.pyplot as plt plt.plot([1, 2, 3, 2.5]) plt.ylabel('some numbers') plot() is a versatile command, and will take an arbitrary number of arguments. For example, to plot x versus y, you can issue the command: x_list = list(range(1,10)) y_list = [pow(i, 2) for i in x_list] print(x_list) print(y_list) plt.plot(x_list, y_list) plt.title(\"Title of the plot\") Using the pyplot interphase, you build a graph by calling a sequence of functions and all of them are applied to the current subplot, like so: plt.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3) plt.scatter([0.3, 3.8, 1.2, 2.5], [11, 25, 9, 26], color='darkgreen', marker='^') plt.xlim(0.5, 4.5) plt.title(\"Title of the plot\") plt.xlabel(\"This is the x-label\") plt.ylabel(\"This is the y-label\") # Uncomment the line below to save the figure in your currentdirectory # plt.savefig('examplefigure.png') When working with just one subplot in the figure, generally is OK to work with the pyplot interphase, however, when doing more complicated plots, or working within larger scripts, you will want to explicitly pass around the Subplot (Axes) and/or Figure object to operate upon. def gc_content(file): \"\"\"Calculate GC content of a fasta file (with one sequence)\"\"\" sequence=\"\" with open(file, 'r') as f: for line in f: if line.startswith('>'): seq_id = line.rstrip()[1:] else: sequence += line.rstrip() A_count = sequence.count('A') C_count = sequence.count('C') G_count = sequence.count('G') T_count = sequence.count('T') N_count = sequence.count('N') GC_content = (sequence.count('G') + sequence.count('C')) / len(sequence) * 100 AT_content = (sequence.count('A') + sequence.count('T')) / len(sequence) * 100 print(\"The GC content of {} is\\t {:.2f}%\".format(seq_id, GC_content)) return GC_content, AT_content, A_count, C_count, G_count, T_count, N_count GC_content, AT_content, A_count, C_count, G_count, T_count, N_count = gc_content('../data/gene.fa') print(GC_content) print(AT_content) print(A_count) print(C_count) print(T_count) print(G_count) total_count = A_count + C_count + G_count + T_count A_perc = A_count/total_count*100 C_perc = C_count/total_count*100 G_perc = G_count/total_count*100 T_perc = T_count/total_count*100 height = [A_perc, C_perc, G_perc, T_perc] bars = ('A','C','G','T') plt.bar(bars, height) plt.xlabel('Nucleotide') plt.ylabel('Percentage of occurence (%)') plt.title('Distribution of nucleotides in fasta sequence') plt.show() total_count = A_count + C_count + G_count + T_count A_perc = A_count/total_count*100 C_perc = C_count/total_count*100 G_perc = G_count/total_count*100 T_perc = T_count/total_count*100 height = [A_perc, C_perc, G_perc, T_perc] bars = ('A','C','G','T') #plt.bar(bars, height, color=('green','red','yellow','blue')) plt.bar(bars, height, color=('#1f77b4','#ff7f0e','#2ca02c','#d62728')) plt.xlabel('Nucleotide') plt.ylabel('Percentage of occurence (%)') plt.title('Distribution of nucleotides in fasta sequence') plt.show() # libraries #import numpy as np import matplotlib.pyplot as plt # width of the bars barWidth = 0.3 # Choose the height of the blue bars experimentA = [10, 9, 2] # Choose the height of the cyan bars experimentB = [10.8, 9.5, 4.5] # Choose the height of the error bars (bars1) yer1 = [0.5, 0.4, 0.5] # Choose the height of the error bars (bars2) yer2 = [1, 0.7, 1] # The x position of bars r1 = list(range(len(experimentA))) r2 = [x + barWidth for x in r1] # Create blue bars plt.bar(r1, experimentA, width = 0.3, color = 'blue', edgecolor = 'black', yerr=yer1, capsize=5, label='Experiment A') # Capsize is the width of errorbars # Create cyan bars plt.bar(r2, experimentB, width = 0.3, color = 'cyan', edgecolor = 'black', yerr=yer2, capsize=7, label='Experiment B') # general layout plt.xticks([x + barWidth/2 for x in r1], ['cond_A', 'cond_B', 'cond_C']) plt.ylabel('effect') plt.legend() # Show graphic plt.show()"},{"title":"01 Introduction to Jupyter","url":"/topics/python-programming/tutorials/1_introduction/tutorial.html","tags":[],"body":"1.1 Why Jupyter Jupyter is an interactive code environment that allows you to write code and get immediate feedback from it. It’s one of the most popular environment for Python programming. Especially for training purposes, as it interactively gives you your code and some informative text together. 1.2 Installation The easiest way to install Python and Jupyter is to install Anaconda (Navigator) on your computer. Anaconda Navigator contains several (GUI) applications like Jupyter in which you can run your Python code. As a side note, Anaconda is also a package manager which makes it ideal for reproducibility purposes as well. Nowadays, Jupyter comes in two versions. More often you will hear about Jupyter Notebooks which is the precursor of Jupyter Lab. The latter has a couple of advantages, however for stability reasons we’ll be using Jupyter Notebooks for now. hands_on Installation instructions To install all prerequisites for this course Go to Anaconda, scroll a bit down and select the right distribution system (Windows, MacOS or Linux), and download the Python 3.7 version. Follow the installation instructions. You should be able to find Jupyter Notebooks within the installed apps now. Otherwise, open the Anaconda Navigator & launch a Jupyter Notebook Jupyter Notebooks opens a tab with a list of your folders. Make and/or select a folder in which you want to keep the training materials. Find the training materials on our Github repository: Gentle hands on python Click the button ‘Clone or Download’ and select ‘Download ZIP’. Finally, extract the zipped file within the folder you just selected or created. In Jupyter Notebook you should see the materials now. 1.3 Getting familiar with Jupyter Notebooks a. Make a new notebook Navigate to a folder and click on the right New –> Python 3. A new Notebook now pops up with an empty cell. In this cell you can directly input some Python code. Try out the following: 1+1 Click on the triangle symbol on the top of the notebook or type ‘Shift+Enter’ to run the code. The output will immediately appear on the screen and should look like this. Also, a new cell will have appeared in the notebook. A notebook is actually a set of cells in which you can input code. If you want another cell, you can click the ‘+’ symbol on top of the notebook. Other interesting symbols up there are the stop symbol and the reload symbol. Whenever your code is stuck, you can stop it right there, or whenever you want to restart in a clean and fresh environment, you hit that restart button. b. Code or Markdown There are two modes that a cell can have. A cell is by default in Code modus. This means that the environment expects a Python code as input and it will interpret it and give you some output upon running that cell. The Markdown mode is a kind of text modus. In here you can type any kinds of text and edit it so headers, bold or italic texts, quotes, images are possible to integrate. It’s called rich text. E.g. If you double click this text, you will see the Markdown code of this text. c. Command or Edit mode To switch between these modes, hit ‘Esc’ or ‘Enter’. When you hit ‘Enter’, you’ll get into the Edit mode, the cell will have a blue border around it and you’re free to edit the content of that cell (both in python code or markdown code). If you hit ‘Esc’, you’re cell will be in the Command mode and you can use shortcuts to edit your notebook: a (above): add a new cell above b (below): add a new cell below dd: remove the cell z: undo the previous action these are just a few of them. The blue bar on the left of your cell indicates which cell is selected. In command mode, you can move through your cells with the up and down arrow keys. Lastly, within the command mode, type ‘y’ to change the cell to a Python code cell and type ‘m’ to change the cell to a Markdown code cell. d. Running a cell To stress the importance of the ‘stop’ button on top of this notebook, run the following code below. While it is running, the code has an asterisk which means it’s still being executed and your notebook won’t be able to process any other code in another cell. In order to stop it, because it’s an infinite loop, hit the stop button or type ‘ii’ in command mode. import time while True: print(\"Hello\") time.sleep(3) 1.4 Examples The above will suffice for the Jupyter environment introduction. We will dive into our first examples before diving into the first chapter of our Python adventure. A program needs information (input) to run, and then needs to export its results so that you know what happened (output). The easiest way to do this is to send a ‘text message’ to the screen; this is possible with the print command which we will introduce here. In this section we also discuss some basics of Python syntax, and the errors that occur if you don’t get it right. a. Let’s do some math Python is very intuitive and flexible in a way that there is no need of special colons, nor do you have to take spaces into account. Just note that Python is indent-sensitive, but we will get back to this. 1+1 2 - 5 3 * 4 10/2 b. Writing a message The print command allows you to write data to the console screen. Try the following example: # Print 'Hello world' to the screen print(\"Hello world\") Notice that lines starting with a # symbol are not displayed, nor evaluated by Python. They usually contain extra information concerning the code. # What happens if you leave out the quotation marks? print(Hello world) You should get the following error: SyntaxError: invalid syntax. This is because Python doesn’t understand what Hello and world mean. c. Writing numbers You can also print out numbers as text messages to the screen. You do not need quotation marks in this case; just the number is enough. If your number does not have a decimal point (.) in it, it’s called an integer, if it does have a decimal point, it is a float. # Print an integer and a floating point print(5) print(3.1415) Note In Python, programs often start with: #!/usr/bin/python This line is called the ‘Shebang’ and tells the operating system where it can find the Python language interpreter so it can run the program without you having to specify where Python is. With Jupyter Lab/Notebooks we already have a Python environment so we do not need to redefine it every time. 1.5 JupyterLab With recent improvements, the environment grew a little bit more powerful to a full interface, called JupyterLab. You can see all of the files that are within a folder within a file explorer, you can open a Terminal window which is a Linux machine where you can install any packages that you would need. You can also make a text file or edit text files that are in your folder. However, the most simplest is still to open a Python console where you directly insert python code."},{"title":"08 Dictionaries","url":"/topics/python-programming/tutorials/8_dictionaries/tutorial.html","tags":[],"body":"8.1 Introduction So far we’ve seen variables that store one value or a series of values (see section 5: lists, tuples and sets). There is another way of storing information where you associate one value with another value; in Python this is called a dictionary. Dictionaries provide a very useful way of quickly connecting different values to each other. 8.2 Dictionary creation & usage It is best to think of a dictionary as a set of key:value pairs, with the requirement that the keys are unique (within one dictionary). Dictionaries are initiated by using curly brackets {}, and each pair of key:values is separated with a comma. This is how a dictionary would look like: myDictionary = {'A': 'Ala', 'C': 'Cys', 'D': 'Asp'} myDictionary You can recall values by using square brackets [ ] with the name of the key, or use the get()-method. myDictionary['A'] myDictionary.get('C') If you would like to add a new pair of key-value: myDictionary['E'] = 'Glu' myDictionary Note however that keys are unique and if you try to add a key:value-pair with a key that already exists in the dictionary and a different value, it will overwrite the value. myDictionary['A'] = 'Glu' myDictionary So keys are unique, values are not! Dictionaries, like lists, have several useful built-in methods. The most frequently used are listed here below: keys() to list the dictionary’s keys values() to list the values in the dictionary get() call the value of a specified key pop() to remove the specified key and its values Listing the keys within a dictionary: myDictionary = {'A': 'Ala', 'C': 'Cys', 'D': 'Asp', 'E': 'Glu'} myDictionary.keys() Python tells us that the list is still in a dictionary-keys data structure type. If you would like to extract the keys for further processing, it’s probably better to transform them into a list: list(myDictionary.keys()) Similarly for the values of a dictionary: list(myDictionary.values()) We’ve already exploited the get method, with pop we can remove a key-value pair: myDictionary.pop('E') myDictionary If you try to access a key that doesn’t exist, Python will give an error: myDictionary = {'A': 'Ala', 'C': 'Cys', 'D': 'Asp', 'E': 'Glu'} myDictionary['B'] You should therefore always check whether a key exists: # Newlines don't matter when initialising a dictionary... myDictionary = { 'A': 'Ala', 'C': 'Cys', 'D': 'Asp', 'E': 'Glu', 'F': 'Phe', 'G': 'Gly', 'H': 'His', 'I': 'Ile', 'K': 'Lys', 'L': 'Leu', 'M': 'Met', 'N': 'Asn', 'P': 'Pro', 'Q': 'Gln', 'R': 'Arg', 'S': 'Ser', 'T': 'Thr', 'V': 'Val', 'W': 'Trp', 'Y': 'Tyr'} if 'B' in myDictionary.keys(): print(myDictionary['B']) else: print(\"myDictionary doesn't have key 'B'!\") However, it’s much cleaner if you use the get() method as it doesn’t return an explicit error if a key doesn’t exist in your dictionary. Instead it will return a None-value. type(myDictionary.get('B')) hands_on Exercise 8.2.1 Use a dictionary to track how many times each amino acid code appears in the following sequence: SFTMHGTPVVNQVKVLTESNRISHHKILAIVGTAESNSEHPLGTAITKYCKQELDTETLGTCIDFQVVPGCGISCKVTNIEGLLHKNNWNIED NNIKNASLVQIDASNEQSSTSSSMIIDAQISNALNAQQYKVLIGNREWMIRNGLVINNDVNDFMTEHERKGRTAVLVAVDDELCGLIAIADT Tip: use the one-letter code as key in the dictionary, and the count as value. solution Solution # Use a dictionary to track how many times each amino acid code appears in the following sequence: # SFTMHGTPVVNQVKVLTESNRISHHKILAIVGTAESNSEHPLGTAITKYCKQELDTETLGTCIDFQVVPGCGISCKVTNIEGLLHKNNWNIEDNNIKNASLVQIDASNEQSSTSSSMIIDAQISNALNAQQYKVLIGNREWMIRNGLVINNDVNDFMTEHERKGRTAVLVAVDDELCGLIAIADT # Tip: use the one-letter code as key in the dictionary, and the count as value. mySequence = \"SFTMHGTPVVNQVKVLTESNRISHHKILAIVGTAESNSEHPLGTAITKYCKQELDTETLGTCIDFQVVPGCGISCKVTNIEGLLHKNNWNIEDNNIKNASLVQIDASNEQSSTSSSMIIDAQISNALNAQQYKVLIGNREWMIRNGLVINNDVNDFMTEHERKGRTAVLVAVDDELCGLIAIADT\" # First way to do this, using sets (condensed) aminoAcidCount = {} myUniqueAminoAcids = set(mySequence) for aaCode in myUniqueAminoAcids: print(\"Amino acid {} occurs {} times.\".format(aaCode,mySequence.count(aaCode))) aminoAcidCount[aaCode] = mySequence.count(aaCode) solution Solution # Another way to do this, a little bit more elaborate and using the myDictionary as a reference for iteration mySequence = \"SFTMHGTPVVNQVKVLTESNRISHHKILAIVGTAESNSEHPLGTAITKYCKQELDTETLGTCIDFQVVPGCGISCKVTNIEGLLHKNNWNIEDNNIKNASLVQIDASNEQSSTSSSMIIDAQISNALNAQQYKVLIGNREWMIRNGLVINNDVNDFMTEHERKGRTAVLVAVDDELCGLIAIADT\" myDictionary = { 'A': 'Ala', 'C': 'Cys', 'D': 'Asp', 'E': 'Glu', 'F': 'Phe', 'G': 'Gly', 'H': 'His', 'I': 'Ile', 'K': 'Lys', 'L': 'Leu', 'M': 'Met', 'N': 'Asn', 'P': 'Pro', 'Q': 'Gln', 'R': 'Arg', 'S': 'Ser', 'T': 'Thr', 'V': 'Val', 'W': 'Trp', 'Y': 'Tyr'} lengthDict = len(myDictionary.keys()) for aa in range(lengthDict): aaCode = list(myDictionary.keys())[aa] aaCount = mySequence.count(aaCode) print(\"Amino acid {} occurs {} times.\".format(aaCode,aaCount)) 8.3 A practical example of dictionaries An practical example of dictionaries can be found in Biopython. Imagine that we want to extract some information from a GenBank file (NC_005816) # Imports the SeqIO object from Biopython from Bio import SeqIO # Reads in (just one record of) the GenBank file record = SeqIO.read(\"data/NC_005816.gb\",\"genbank\") print(record) The SeqRecord object (which we see here) has an id, name and description as well as a sequence. For other (miscellaneous) annotations, the SeqRecord object has a dictionary attribute annotations. Most of the annotations information gets recorded in the annotations dictionary. print(record.id) print(record.name) print(record.description) #print(record.seq) record.annotations record.annotations['organism'] record.annotations['source'] (In general, organism is used for the scientific name (in Latin, e.g. Arabidopsis thaliana), while source will often be the common name (e.g. thale cress). In this example, as is often the case, the two fields are identical.) record.annotations['accessions'] # This could be a list of values, hence the list. 8.4 More with dictionaries As mentioned here above, the value associated with a key can consist of a list with values (instead of one single value). In the example below we save the information of an experiment in a dictionary. The key that saves the date information contains a list of fictive dates (01-01-2020 and 02-01-2020): TriplicateExp1 = {'name': 'experiment 1', 'pH': 5.6, 'temperature': 288.0, 'volume': 200, 'calibration':'cal1', 'date':['01-01-2020','02-01-2020']} TriplicateExp1 For the keys, however, the data structures should be immutable (so tuples are OK, lists are not). Recall that keys have to be unique; if you add a key that already exists, the old entry will be overwritten: dates = ('date1','date2') # tuple TriplicateExp1[dates] = ['01-01-2020','02-01-2020'] TriplicateExp1 It is also possible to have a so-called nested dictionary, in which there is a dictionary within a dictionary. Here we make two more dictionaries with information about the triplicate experiment. The information of each experiment is thus assembled in a separate dictionary. Then, the three dictionaries are combined into one dictionary. TriplicateExp2 = {'name': 'experiment 2', 'pH': 5.8, 'temperature': 286.0, 'volume': 200, 'calibration':'cal1', 'date':'03-01-2020'} TriplicateExp3 = {'name': 'experiment 3', 'pH': 5.4, 'temperature': 287.0, 'volume': 200, 'calibration':'cal1', 'date':'04-01-2020'} Triplicate = { 'exp1':TriplicateExp1, 'exp2':TriplicateExp2, 'exp3':TriplicateExp3 } Triplicate"},{"title":"02 Variables and Operators","url":"/topics/python-programming/tutorials/2_variables_and_operators/tutorial.html","tags":[],"body":"2.1 Introduction Just printing things is not that interesting, what you really want to do with a computer program is manipulate data. This is why variables are so important - they allow you to assign information to a name that you can re-use later on. In this section we will introduce the basic types of variables and how you can manipulate them. Just to get started, we give an overview of the different built-in data types that are present in Python and which you can assign to a variable. Although this variety of data types exist, not all of them will be discussed in this course. Text type: str Numeric types: int, float, complex Sequence types: list, tuple, range Mapping types: dict Set types: set, frozenset Boolean types: bool Binary types: bytes, bytearray, memoryview In this section, we will cover the text type, numeric types (complex are out of scope) and booleans. Operators can be anything from: Arithmetic: additions, substractions, multiplications, divisions, remainders and power Comparison: equal to, not equal to, greater than, less than, etc. Logical: AND, OR and NOT used for conditional statements Identity: is, is not Note: This section doesn’t really include any exercises. Try to follow and code along while we scroll through the examples so you start to have a feeling of it. 2.2 Strings We already saw strings in the previous section. You can assign a string to a variable like this: # Assign the sequence AGAATCGATACGA to a variable and print the variable. mySequence = \"AGAATCGATACGA\" print(mySequence) What happens here is that you assign a value: “AGAATCGATACGA” to a variable: mySequence and then print it out. You can now keep on using mySequence throughout your program. Note that mySequence is not quoted because it is now part of the program, try this for example: # Repeat the above, but this time put the variable in quotation marks when you put in the print statement and see what happens mySequence = \"AGAATCGATACGA\" print(\"mySequence\") You will now still assign the value “AGAATCGATACGA” to the variable mySequence, but because of the quotes you then print off the string “mySequence”, not the variable. You can assign strings in the following ways: myString1 = \"Hello world!\" myString2 = 'Hello sun!' myString3 = \"\"\"Hello universe.\"\"\" print(myString1) print(myString2) print(myString3) The single and double quotes are essentially the same. If you use triple double quotes - “”” - you can assign a string over multiple lines. # Try assigning a string over multiple lines without using the triple double quotes and see what happens. myString = \"Hello universe.\" This will give a SyntaxError, as Python ‘reads’ each line separately, and it doesn’t find the ending (on the first line) and starting (on the second line) quote. Using the escape codes, you can however do the following: # Try to print two words in two different lines without using three \"\" marks. myString = \"Hello\\nuniverse.\" myString 2.3 Strings from user input Python provides a very simple way to get user input. This input is always returned as a string, so try the following: # Use input to ask for a sequence string, then print the input sequence mySequence = input(\"Give me a sequence:\") print(mySequence) 2.4 Integers Integers are non-decimal numbers. Python will recognize numbers in the code automatically, so you can do: # Assign integer 5 to a variable myInteger myInteger = 5 print(myInteger) As described in the introduction, you can also do standard mathematical operations on integers. Mathematical operations are even possible within a print statement. 5 + 5 # Addition 5 - 8 # Subtraction 2 * 5 # Multiplication 4 / 2 # Division 5 % 2 # Modulus, remainder of division 2 ** 3 # Power It doesn’t matter if you use variables or integers for this: x = 5 y = 2 x + 5 # Addition x - 8 # Subtraction y * x # Multiplication 4 / y # Division 5 % y # Modulus, remainder of division y ** 3 # Power In order to print an integer inside a string, you could simply use the following expression in which the string is separated from the integer with a comma. firstResult = 5 * 4 print(\"The result is\", firstResult,\".\") However, there is another way using the .format() method. The format method allows you to change the lay-out of the output that it prints. We will use it a lot during this course, here you see it in the most simplest form. The variable that you want to print is given within the rounded brackets of the format method, and the location in the string to where it prints is given with curly brackets: firstResult = (5 * 4) print(firstResult) print(\"The result of the first calculation is {}.\".format(firstResult)) secondResult = (5 * (4 + 3) - 2) print(secondResult) print(\"The result of the second calculation is {}.\".format(secondResult)) Note here the precedence of operations; * and / take precedence over + and -. You can use () to change the results. 2.5 Floats Floats (floating point numbers) are decimal numbers that behave in the same way as integers, except that they are more accurate # Assign float 5.5 to the myFloat variable myFloat = 5.5 myFloat type(myFloat) Mathematical operations are the same: 5.2 + 4.8 # Addition 5.2 - 8.3 # Subtraction 2.0 * 5.11212 # Multiplication 4.2 / 2.7 # Division 5.4 % 2.0 # Modulus, remainder of division 4 ** 0.5 # Power Also floats can be incorporated in a string with the .format() statement. You can determine the number of characters before and after the decimal point as well, however we will cover this in the next section. myFloat = 4545.4542244 print(\"Print the full float {},\\ncut off decimals {:.2f},\\nor determine the characters before the decimal {:10.1f}.\".format(myFloat,myFloat,myFloat)) Note here that we put three formatting characters in the string; we then also need three values to print out. 2.6 Floats, integers and strings You can also force a conversion between the different value types float, integers and strings with the str(), int() and float() conversions: # Use the int() and float() statements to switch the value types and print out the values. Do you notice any differences? myFloat = 4.5 myFloat int(myFloat) # Note that it will print the result of the operation; myFloat remains an integer! myInteger = 5 myInteger myOtherFloat = float(myInteger) myOtherFloat The same is possible to convert between strings with str(), you can also convert strings back to integers and floats but only if the content of the string is an integer or float: # Convert a float and an integer to a string with the str() statement myFloat = 4.5 myFloatString = str(myFloat) myInteger = 5 myIntegerString = str(myInteger) print(\"My strings are {} and {}\".format(myFloatString,myIntegerString)) print(\"My string converted to integer is {}\".format(int(myIntegerString))) print(\"My string converted to float is {}\".format(float(myFloatString))) hands_on Exercise 2.6.1 Write a program where you ask for a number, convert it to an integer, and print out in a formatted string what your number is. solution Solution myFloatString = input(\"Give me a number:\") myInteger = int(float(myFloatString)) print(\"My number in integer form is {}\".format(myInteger)) You can also add, substract, divide and multiple a variable by a number or other variable directly. These are the so-called assignment operators. myFloat = 6 myString = \"ABC\" myFloat += 5 # Same as myFloat = myFloat + 5 print(myFloat) myString += \"DE\" # Addition works for strings as well print(myString) myFloat -= 5 # Same as myFloat = myFloat - 5 print(myFloat) myFloat /= 2 # Same as myFloat = myFloat / 2 print(myFloat) myFloat *= 2 # Same as myFloat = myFloat * 2 print(myFloat) Finally, you can check what data type a variable is by using type(): myInteger = -6 myFloat = 5.22 myString = \"Text!\" print(myInteger, type(myInteger)) print(myFloat, type(myFloat)) print(myString, type(myString)) Note here that you can print multiple values by using a comma in between the values. hands_on Exercise 2.6.2 See what happens if you try to print a float as an integer, and an integer as a string. solution Solution myFloat = 11.4 myIntFloat = int(myFloat) print(\"My float as integer {}\".format(myIntFloat)) #This works myInt = 12 print(\"My integer as string {}\".format(str(myInt))) #This works as well... but: myString = \"Hello\" print(\"My string as float {}\".format(float(myString))) #will fail and give a TypeError - Python cannot convert \"aa\" into a float. 2.7 Booleans Finally, there are the boolean variables True and False. Python returns booleans when comparing values. In the code below python checks whether the comparison is TRUE, when this is the case it will print out the boolean True. In order to do a comparison, we use comparison operators like ==, >, =, != myBoolean = True myBoolean type(myBoolean) myInteger = 5 myInteger == 6 # This means 'is myInteger equal to 6?' myInteger 6 # This means 'is myInteger greater than 6?' myInteger = 6 # This means 'is myInteger greater or equal to 6?' myInteger != 6 # This means 'is myInteger not equal to 6?' Similarly to comparison operators, you can also use is and not which are the identity operators: myInteger = 5 myInteger is 6 # Same as == myInteger is not 6 # Same as != not myInteger > 6 # Same as 2 # Both have to be True for the result to be True x != 5 or y > 2 # Only one has to be True for the result to be True 2.8 Nothing Finally, we highlight the None value which is comparable to other program’s null values. In the code below we show that None, which you could interpret as nothing, is still something else than the value 0 or e.g. an empty string. myNothing = None myNothing type(myNothing) type(None) 0 == None \"\" == None However, the opposite of None is still True. not None Really 0 is still an integer, “” a string, so None is really nothing:"},{"title":"10 Functions","url":"/topics/python-programming/tutorials/10_functions/tutorial.html","tags":[],"body":"10.1 Introduction So far we’ve been writing ‘sequential’ code, basically following the flow of the code from the top to the bottom of the program. Sometimes, however, you want to re-use code elsewhere without copy/pasting a bit of code. You can do this with functions; a function holds a block of code that can be called from other places. Functions are essential for larger projects and code maintenance - if there’s a problem with that piece of code, for example, you only have to fix it in one place. 10.2 Functions We’ve already been using built-in Python functions, for example abs() or len(). However, in this section we will build our own functions. Generally, the syntax when calling a function is the name of the function followed by round brackets ( ). When you’re writing your own function, in essence it would look like this: def name_function(): \"Some information about the function\" print(\"This is a very simple function\") Information is given to a function by means of an argument and this is passed on in the rounded brackets. In the example above an argument is not defined, hence whenever you call the function it will print the same text. Arguments are defined within the parenthesis and are separated by commas in case there are multiple arguments. Before exploiting functions with arguments, let’s have a look to an example with no arguments that prints the same text always when you call the function. def silly_function(): \"This is some information about the silly function that will print out some silly text\" text = \"Some silly text\" print(text) Notice that nothing happened now. This is because we’re not calling the function, we just defined it. In order to call the function, we use the following expression: silly_function() Information about the function can be retrieved by using the help() function. help(silly_function) The following code is an example of a function that will take some value as an argument and return the absolute value: def myAbsFunc(someValue): \"myAbsFunc takes a number as input and will return the absolute value\" if someValue meanValueList2: outputText = \"List1 has a higher average ({:.2f}) than list2 ({:.2f}).\".format(meanValueList1,meanValueList2) else: # No need to compare again, only possibility left outputText = \"List2 has a higher average ({:.2f}) than list1 ({:.2f}).\".format(meanValueList2,meanValueList1) return outputText valueList1 = [4,6,77,3,67,54,6,5] valueList2 = [5,5,76,5,65,56,4,5] compareMeanValueOfLists(valueList1,valueList2) You can call functions within functions, or basically anywhere in your code, even in conditions, …: if getMeanValue(valueList1) > 26 : print(\"The mean value of list 1 is greater than 1.\") There are several ways to solve this problem, however it might be easier to do it with zip() ;). hands_on Exercise 10.2.1 The Hamming distance between two strings of equal length is the number of positions at which the corresponding character are different. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. The Hamming distance between: “karolin” and “kathrin” is 3. Write a function called “hamming_distance”: which accepts two strings, and raises an error if the lengths are unequal. Furthermore the function will return an integer that represents the number of mismatches between the two sequences. solution Solution 1 # string1 and string2 should be the same length. def hamming_distance(string1, string2): \"\"\"Return the Hamming distance between equal-length sequences.\"\"\" if len(string1) != len(string2): raise ValueError(\"Undefined for sequences of unequal length.\") # Start with a distance of zero, and count up distance = 0 # Loop over the indices of the string L = len(string1) for i in range(L): # Add 1 to the distance if these two characters are not equal if string1[i] != string2[i]: distance += 1 # Return the final count of differences return distance seq1 = \"GATCATAGA\" seq2 = \"CATCATACA\" print(hamming_distance(seq1,seq2)) solution Solution 2 # string1 and string2 should be the same length. def hamming_distance(string1, string2): \"\"\"Return the Hamming distance between equal-length sequences.\"\"\" assert len(string1) == len(string2), \"Undefined for sequences of unequal length.\" # Start with a distance of zero, and count up distance = 0 # Loop over the indices of the string for s1,s2 in zip(string1,string2): if s1 != s2: distance +=1 return distance # Return the final count of differences return distance seq1 = \"GATCATAGA\" seq2 = \"CATCATACA\" print(hamming_distance(seq1,seq2)) There are several ways to solve this problem, however it might be easier to do it with the zip() function. hands_on Exercise 10.2.2 Write a function that calculates the GC content of the sequence in a fasta file. For this example you can use this fasta file which contains the genetic sequence of a bone gla protein. The function must accept a fasta file as input file and will print the following: The GC content of HSBGPG Human gene for bone gla protein (BGP) is 63.53% The method .startswith() might help. The function should read the lines of the fasta file and if it starts with a ‘>’ define the text that comes afterwards as the sequence ID. The other lines are part of the sequence. After reading through the lines, you can easily define the GC content by counting the bases and taking the average. solution Solution 1 # solution one def gc_content(file): \"\"\"Calculate GC content of a fasta file (with one sequence)\"\"\" sequence=\"\" with open(file, 'r') as f: for line in f: if line.startswith('>'): seq_id = line.rstrip()[1:] else: sequence += line.rstrip() GC_content = (sequence.count('G') + sequence.count('C')) / len(sequence) * 100 print(\"The GC content of {} is\\t {:.2f}%\".format(seq_id, GC_content)) gc_content('data/gene.fa') solution Solution 2 # solution two - very similar to one. def gc_content(file): f = open(file, 'r') sequence=\"\" for line in f.readlines(): if line.startswith('>'): seq_id = line.rstrip()[1:] else: sequence += line.rstrip() GC_content = (sequence.count('G') + sequence.count('C')) / len(sequence) * 100 print(\"The GC content of {} is\\t {:.2f}%\".format(seq_id, GC_content)) gc_content('data/gene.fa') 10.3 Flexibility in functions In the functions so far we’ve been using values (arguments) that are passed in and are required for the function to work. If you’re not sure how many arguments the user will give, you can use an asterisk *. However, make sure that your code is flexible to access the number of arguments that the user is giving as input. In the example below we use the * asterisk to define a flexible number of arguments, and we use a for-loop to access each argument: def MeanValue(*valueList): \"\"\" Calculate the mean (average) value from a list of values. Input: list of integers/floats Output: mean value \"\"\" meanValues = [] for eachList in valueList: meanOfList = sum(eachList)/len(eachList) meanValues.append(meanOfList) return meanValues MeanValue([1, 2, 3], [4,5,6]) MeanValue([1, 2, 3], [4,5,6], [7, 8, 9]) A second way of making flexible functions is by using keywords in a function; these are not required for the function to work because they are given a default value in the function definition. You can then set these keywords if necessary; consider the following example. By default the parameter sortedList is False which means that Python will not make a sorted list in the function below, unless you explicitly ask it by setting the parameter to True. def MeanValue(*valueList, sortedList = False): \"\"\" Calculate the mean (average) value from a list of values. Input: list of integers/floats Output: mean value \"\"\" meanValues = [] for eachList in valueList: meanOfList = sum(eachList)/len(eachList) meanValues.append(meanOfList) if sortedList == False: print('I calculated all the mean values of your lists, however did not sort them') else: meanValues.sort() print('I calculated the mean values and also sorted them') return meanValues valueList1 = [4,6,77,3,67,54,6,5] valueList2 = [5,5,76,5,65,56,4,5] valueList3 = [5,9,75,8,65,34,4,4] MeanValue(valueList1, valueList2, valueList3) MeanValue(valueList1, valueList2, valueList3, sortedList = True) Using these keywords makes the function a lot more flexible - you can make the function do things (or not) depending on them."},{"title":"10 Functions extra","url":"/topics/python-programming/tutorials/10_functions_extra/tutorial.html","tags":[],"body":"Extra exercises on functions This chapter contains some extra exercises on functions. In the end, practice makes perfect… hands_on Exercise 10.2.1 Download this matrix file (Matrix.txt) and save it in your directory. Then write a function to read a matrix file in this format, reorder the rows by the values in the given column, and printing out the result. The function should take as argument a file name and a column number. solution Solution def sortMatrixByColumn(fileName,columnNumber): # # Read the tab-delimited file and store the values # fin = open(fileName) lines = fin.readlines() fin.close() # # Convert the data from the file into a Python list # matrix = [] for matrixRow in lines: # Tab-delimited, so split line by \\t - this will give a list of strings matrixColumns = matrixRow.rstrip().split(\"\\t\") # Add a row to the matrix matrix.append([]) # Add the columns, but convert the strings from the file into a float for matrixValue in matrixColumns: matrix[-1].append(float(matrixValue)) # # Now sort by column - but have to track the row number as well! # selectedColumnValues = [] for rowNumber in range(len(matrix)): selectedColumnValues.append((matrix[rowNumber][columnNumber],rowNumber)) selectedColumnValues.sort() # # Now print out the new matrix - the column value is now not interesting # we want the row number!! # for (columnValue,rowNumber) in selectedColumnValues: columnValueStrings = [] for value in matrix[rowNumber]: columnValueStrings.append(\"{:.3f}\".format(value)) print(\"\\t\".join(columnValueStrings)) sortMatrixByColumn(\"data/matrix.txt\",3) hands_on Exercise 10.2.2 Modify the program to read in the TestFile.pdb file by using separate functions to get the title, dissect the information from the ATOM line and to calculate the distance to the reference distance solution Solution def getTitle(line,cols): # Gets the title title = line.replace(cols[0],'') title = title.strip() return (\"The title is '%s'\" % title) def getAtomInfo(cols): # Get relevant information from an ATOM line and convert to the right type atomSerial = int(cols[1]) atomName = cols[2] residueNumber = int(cols[5]) x = float(cols[6]) y = float(cols[7]) z = float(cols[8]) return (atomSerial,atomName,residueNumber,x,y,z) def calculateDistance(coordinate1,coordinate2): # Calculate the distance between two 3 dimensional coordinates return ((coordinate1[0] - coordinate2[0]) ** 2 + (coordinate1[1] - coordinate2[1]) ** 2 + (coordinate1[2] - coordinate2[2]) ** 2 ) ** 0.5 # Open the file fileHandle = open(\"data/TestFile.pdb\") # Read all the lines in the file (as separated by a newline character), and store them in the lines list # Each element in this list corresponds to one line of the file! lines = fileHandle.readlines() # Close the file fileHandle.close() # Initialise some information searchCoordinate = (-8.7,-7.7,4.7) modelNumber = None # Loop over the lines, and do some basic string manipulations for line in lines: line = line.strip() # Remove starting and trailing spaces/tabs/newlines # Only do something if it's not an empty line if line: cols = line.split() # Split the line by white spaces; depending on the format this could be commas, ... # Print off the title if cols[0] == 'TITLE': print(getTitle(line,cols)) # Track the model number elif cols[0] == 'MODEL': modelNumber = int(cols[1]) # For atom lines, calculate the distance elif cols[0] == 'ATOM': (atomSerial,atomName,residueNumber,x,y,z) = getAtomInfo(cols) # Calculate the distance distance = calculateDistance((x,y,z),searchCoordinate) if distance < 2.0: print(\"Model {}, residue {}, atom {} (serial {}) is {:.2f} away from reference.\".format(modelNumber,residueNumber,atomName,atomSerial,distance))"},{"title":"01 Downloading NGS data from the internet","url":"/topics/ngs-intro/tutorials/download-ngs-data-internet/tutorial.html","tags":[],"body":"Introduction NGS data repositories First of all, you need data to analyze. You can generate your own data but there’s a lot of NGS data available on the internet. The main repositories for NGS data: { class=”wikitable”           align=”center” style=”background:#f0f0f0;” ’’’’’’         align=”center” style=”background:#f0f0f0;” ’'’NCBI - US’’’         align=”center” style=”background:#f0f0f0;” ’'’EBI - Europe’’’                 Close-by so faster downloads   ’'’Gene expression database’’’   [http://www.ncbi.nlm.nih.gov/geo/ GEO]   [http://www.ebi.ac.uk/arrayexpress ArrayExpress]   Contain processed NGS data, no raw data   ID starts with G   ID starts with E-   ’'’NGS sequence database’’’   [http://www.ncbi.nlm.nih.gov/sra SRA]   [http://www.ebi.ac.uk/ena ENA]   Contain raw NGS data   ID starts with SR   ID starts with ER       ENA IDs also used by SRA   SRA IDs also used by ENA       stores reads in sra format   stores reads in fastq format   }         Both GEO and SRA use multiple types of IDs, ordered according to a certain hierarchy: {|class=”wikitable” | align=”center” style=”background:#f0f0f0;”|’'’GEO ID’’’ | align=”center” style=”background:#f0f0f0;”|’'’points to’’’ | align=”center” style=”background:#f0f0f0;”|’'’definition’’’ |- |ID starts with GSE||experiment||Data of a full NGS experiment consisting of multiple samples The samples belong to different groups that are to be compared e.g. treated and control samples |- |ID starts with GSM||sample||Data of one single sample |- | align=”center” style=”background:#f0f0f0;”|’'’SRA ID’’’ | align=”center” style=”background:#f0f0f0;”|’'’points to’’’ | align=”center” style=”background:#f0f0f0;”|’'’definition’’’ |- |ID starts with SRP||study||Studies have an overall goal and may comprise several experiments. |- |ID starts with SRX||experiment||An Experiment describes what was sequenced and the method used. Info on the source of the DNA, samples, sequencing platform and the processing of the data. |- |ID starts with SRR||run||Data of a particular sequencing experiment. Experiments may contain many runs depending on the number of instrument runs that were needed. |} There are two other resources of NGS data: [https://insilicodb.org/ In Silico DB] from the ULB https://insilicodb.org/ [http://www.illumina.com/science/data_library.ilmn Illumina’s NGS data library] http://www.illumina.com/science/data_library.ilmn If you have an article describing an NGS dataset that is of interest to you, you should search in the article for a sentence mentioning the ID of the data in one of these databases. Metadata of NGS data sets You do not only need the data, you also need extra inforrmation to be able to do the analysis. For instance, you need to know where each sample comes from: in clinical datasets it is important to know if the reads are coming from a patient or from someone in the control group… This kind of information is called metadata and is stored together with the actual data. Exercise 1: Downloading a data set for the introduction training For the introduction training we will use a data set containing short Illumina reads from Arabidopsis thaliana infected with a pathogen, Pseudomonas syringae, versus mock treated controls. The data set is described in the article of Cumbie et al., 2011. The authors provide an ArrayExpress ID (E-GEOD-25818) in the section Analysis of a pilot RNA-Seq experiment, but this ID points to Affymetrix microarray data and not to NGS data: Go to the ArrayExpress home page Find the description of the experiment with ArrayExpress ID E-GEOD-25818 ? |- | Type the ID in the search box on the ArrayExpress home page Click Search You see that the experiment is stored as a Transcription profiling by array experiment (red) and that Affymetrix GeneChip Arabidopsis Genome [ATH1-121501] is the platform that was used (green). Click the Click for detailed sample information and links to data link (blue) You see that you will download .CEL files, the file type for storing raw Affymetrix microarray data. } Fortunately I could find the data in NCBI’s SRA database, so we know the SRA ID. Since the connection with NCBI is too slow, we will do the download from ENA using the SRA ID. Go to the EBI website. Download the data set with SRA ID SRR074262 from ENA ? Type SRR074262 in the search box Click Search Since we are using an SRA run ID as a search term, we do a very specific search so the search returns a single SRA record: Click the SRA run ID on the results page to go to [http://www.ebi.ac.uk/ena/data/view/SRR074262&display=html the ENA record containing the actual data of the run] Scroll to the table at the bottom of the page Click the link in the Fastq files (ftp) column (red): } It can take some time to download the file since it’s very big. Firefox will give you an estimate on how long it’s going to take. If it takes too long, cancel the download and use the file that is already present on the BITS laptops in the /Documents/NGSdata folder as SRR074262.fastq. In a normal analysis we would of course download all 6 data files of this study. It’s only because of time limits that we will only use a single sample during the training. If you are analyzing the 6 samples you need to take a look at the metadata to know which samples represent controls and which samples represent the treatment (in this case treatment with a plant pathogen). In ENA and SRA, annotation is found in the record of the NGS study. Go to the ENA record of the study the downloaded sample belongs to and look at the grouping of the samples. |- | Click the SRA ID of the study the downloaded sample belongs to (green) to access the record of the study: Click the Select columns link on the Read files tab to visualize all the fields with metadata that you can visualize. Deselect the fields that you are not interested in and select the fields you want to view; If you are interested in the grouping of the samples you need to select Library name(red): This adds a column called Library name in the table containing the grouping annotation of the samples. If you want to know exactly what the names mean, you have to consult [http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0025279 the paper that describes the analysis of the data set]. In the RNA preparation and sequencing section you see that hrcC means infected with the pathogen, while MgCL2 represent the control treatment The sample that we have downloaded for the introduction training thus comes from the group of infected samples. |} Exercise 2: Downloading a data set for the ChIP-Seq training Exercise created by Morgane Thomas-Chollier For the ChIP-Seq training, we are going to use the data set that is described in [http://www.ncbi.nlm.nih.gov/pubmed/23818864 the article of Myers et al., 2013] http://www.ncbi.nlm.nih.gov/pubmed/23818864. The article contains the following sentence at the end of the Materials and Methods section: “All genome-wide data from this publication have been deposited in NCBI’s Gene Expression Omnibus (GSE41195).” In this case GSE41195 is the ID of the experiment in the GEO database. Go to [http://www.ncbi.nlm.nih.gov/geo/ the GEO home page] Download the data of the experiment with GEO ID GSE41195 ? |- | Type the ID in the search box on the GEO home page Click Search This redirects you to the GEO record of the experiment. In the Experiment type section you can see that this GEO record is a mixture of expression analysis and ChIP-Seq. Scroll to the bottom of the page: You can see that the data of the ChIP-Seq experiment have their own identifier: GSE41187 Click the ChIP-Seq data identifier. This brings us on the GEO record of the ChIP-Seq experiment. Scroll down to the Samples section: Note that GEO contains the grouping annotation here in the Samples section. For time’s sake, we will focus on a single sample: FNR IP ChIP-seq Anaerobic A Click the ID GSM1010219 of that sample to go to the GEO record of the sample Scroll to the bottom of the page to the Relations section: GEO only contains processed NGS, no raw data. The corresponding raw data is stored in the SRA database. In the Relations section you can find the SRA ID of this data set. For the training we need a fastq file containing raw data. Copy the SRA ID of the ChIP-Seq experiment (SRX189773) } Again, it will take too long to download the data from NCBI. So we will do the download from EBI. Go to [http://www.ebi.ac.uk/ the EBI website]. Download the data with SRA ID SRX189773 ? |- | Type the ID in the search box on the EBI home page Click Search This returns two results: a link to the record of the experiment and a link to the record of the run: Click the record of the full experiment (red) The table at the bottom of the page contains a column called Fastq files (ftp) Click the link in this column to download the data in fastq format } It took only a few minutes to download the data on my laptop at work, but the internet connection at work will be faster than the one in the training room. Firefox will give you an estimate of the time it takes for the download. If it is too long, cancel the download and use the file that has already been downloaded and is available on the BITS laptops: on Windows: in the /Documents/NGSdata folder as SRR576933.fastq In Linux: in the /home/bits/NGS/ChIPSeq folder as SRR576933.fastq ChIP-Seq always compares the ChIP sample to a control sample, consisting of genomic DNA isolated from cells that were cross-linked and fragmented under the same conditions as the ChIP sample or of DNA fragments isolated in a “mock” ChIP reaction using an antibody that reacts with an irrelevant, non-nuclear protein. In this data set, control samples consist of full genomic DNA. To download a control sample, we should redo the same steps starting from the GEO record of the ChIP-Seq experiment and click the GEO sample ID of the anaerobic INPUT DNA sample… However, the fastq file is available in the same data folders (SRR576938.fastq) Downloading data sets via Linux command line See Linux command line training pages Downloading data sets via R Exercise created by Stephane Plaisance Once you know the SRA or ENA ID of the data set you can download the data and the metadata automatically via an R script. See [http://wiki.bits.vib.be/index.php/NGS_RNASeq_DE_Exercise.1#Obtain_data_and_metadata_from_ENA_using_R the exercises of the RNA-Seq training] to learn how to do this."},{"title":"06 Loops","url":"/topics/python-programming/tutorials/6_loops/tutorial.html","tags":[],"body":"6.1 Introduction Another important feature of computer programs is that they can do the same thing over and over again with different information. This is possible by using loops in your code; essentially a loop is executed until it runs out of data or the code decides to break out of it. 6.2 For loop Now that we have these variables that can hold multiple elements (previous exercise), it would be useful to be able to loop over them one by one. This is possible with the for loop: # Make a list of integers from 0 to 9 with steps of 1 (0, 1, 2, ..., 9) myList = range(10) # for each value (myElement) in this list (myList); do the following: for myElement in myList: # Print that value print(\"Counting {}\".format(myElement)) In the first iteration myElement will take up the first value of myList and perform the code that is indented (in this cas it will print counting 0), then it will go back to the start of the loop and take up the second value of myList (which is 1) and perform again the code that is indented (counting 1), etc. Note that again we have to use indentation, as there is a block of code that is only relevant within the for loop. Python will always need a list, tuple, set or dictionary to iterate through and it will work exactly the same with tuples (see example below). The iterator will always take up the value of the list/tuple/set/dict! myTuple = (\"A\",\"B\",\"C\",\"D\",\"E\",\"F\") for myElement in myTuple: print(\"Letter {}\".format(myElement)) Because you can access individual elements within a list or tuple, you can also count the element index in the list or tuple, so that you know both index and value. If you want to iterate over a list of letters, in this case it’s in a tuple type, you’ll first have to find the length of the list and then use range to make a list of integers that can be used as an index. myTuple = (\"A\",\"B\",\"C\",\"D\",\"E\",\"F\") myTupleLength = len(myTuple) for tupleIndex in range(myTupleLength): myElement = myTuple[tupleIndex] print(\"Letter {} is at position {}\".format(myElement,tupleIndex + 1)) # We have to add 1 to the index here because Python starts at zero... Python has a built-in function enumerate() which eases this task for you as a programmer. For the tuple which we defined above, you could make the following table with indeces and accompanied values: | index | value | |---|---| | 0 | A | | 1 | B | | 2 | C | | 3 | D | | 4 | E | | 5 | F | enumerate() mimics this table and you can use it in this way which immediately gives you the indeces: myTuple = (\"A\",\"B\",\"C\",\"D\",\"E\",\"F\") for line in enumerate(myTuple): print(line) The enumerate function has some similarities with dictionaries, especially in how to access a value. Don’t worry if you’re confused with the squared brackets, we’ll cover this in Chapter 8. myTuple = (\"A\",\"B\",\"C\",\"D\",\"E\",\"F\") for line in enumerate(myTuple): print(\"Letter {1} is at position {0}\".format(line[0]+1, line[1])) # For the sake of exercising I switched the format positions for once. Intermezzo: Before starting with exercises, we want to highlight the if-conditions from chapter 4 again, especially the fact that Python interprets the integer 0 to be False, the integer 1 is interpreted as True and any other integer different than 0 and 1 is considered to be not False (it’s also not True though) a = 0 a == False b = 1 b == True c = 2 c == True c == False c != True c != False Why is this important to know? We see sometimes code similar to the one below in which an arithmetical operation is evaluated in an if statement. If the result of this arithmetical operation is an integer like 2, 3, etc. we know now how we can deploy this knowledge to evaluate the statement. c = 2 if c != False: # print(\"C is equal to\", c) a = 0 if a: print(\"A is equal to 0\") b = 1 if b: print(\"B is equal to\", b) if not a: print(\"A is still equal to 0\") Now we want to find out if a number is divisible by another number. In the code below, we will iterate over each value in the list of numbers. If the remainder after division is 0 (comparison is True), we print the number out. myNumbers = range(1,50) myDivider = 17 for myNumber in myNumbers: if not (myNumber % myDivider): # Nothing left after division, so number is divisible. print(\"Number {} cannot be divided by {}!\".format(myNumber,myDivider)) Here we now have two levels of code besides the main one; the if is checked for every value, but the print is only executed for numbers divisible by myDivider. You can also control the loop by using continue and break. They alter the flow of a normal loop: myNumbers = range(1,100) for myNumber in myNumbers: if myNumber == 5: continue # This means that the code within the for loop will be ignored if myNumber is equal to 5, we 'jump back' to the start and use the next number (6) print(myNumber) if myNumber == 8: break # This means we will exit the loop alltogether, all other values after this one will not be dealt with. hands_on Exercise 6.2.1 Write a program where you print out all positive numbers up to 1000 that can be divided by 13, or 17, or both. The output should be printed as : Number 13 is divisible by [13]. If you want a little more challenge, the output should be printed as Number 884 is divisible by 13, 17 solution Solution 1 myNumbers = range(1,100) # should be 1001 myDividers = (13,17) # We will loop over these in the loop itself, so it's easy to add new numbers to this for myNumber in myNumbers: validDividers = [] # In this list we will put all the valid dividers for myDivider in myDividers: if not (myNumber % myDivider): validDividers.append(myDivider) if validDividers: # This means that the list has to have values in it print(\"Number {} is divisible by {}\".format(myNumber,validDividers)) solution Solution 2 # Extra: The output is not very nice here as you print off the list with the square brackets, you could try the following bit of code under the if validDividers: condition: myNumbers = range(1,100) #should be 1001 myDividers = (13,17) # We will loop over these in the loop itself, so it's easy to add new numbers to this for myNumber in myNumbers: validDividers = [] # In this list we will put all the valid dividers for myDivider in myDividers: if not (myNumber % myDivider): validDividers.append(myDivider) if validDividers: # This means that the list has to have values in it # First make strings out of the integers; this is valid Python syntax where you make a list out of a list validDividerStrings = [\"{}\".format(validDivider) for validDivider in validDividers] # Now you can join the elements of a list (if they are strings) together using the .join() method for a string: validDividerString = ', '.join(validDividerStrings) print(\"Number {} is divisible by {}\".format(myNumber,validDividerString)) ######### Or as an alternative for the nice printing: #if len(validDividers) == 1: # print(\"number is div by {}\".format(validDividers[0])) #elif len(validDividers) == 2: # print(\"number x is div by {}, {}\".format(validDividers[0],validDividers[1])) hands_on Exercise 6.2.2 Write a program where you find, for each positive number up to 50, all numbers that can divide each number. E.g. 16 can be divided by 1, 2, 4, 8 and 16. 17 can be divided by… It’s fine if you print the output like this: Number 1 can be divided by 1! Number 2 can be divided by 1! Number 2 can be divided by 2! Number 3 can be divided by 1! However, you can also try to print the output like this: Number 4 can be divided by 1, 2, 4! solution Solution 1 # Write a program where you find, for each positive number up to 50, all numbers that can divide each number. E.g. 16 can be divided by 1, 2, 4, 8 and 16. 17 can be divided by... myNumbers = range(1,5) #should be 51 for x in myNumbers: dividers = [] for y in range(1,x+1): if not (x % y): dividers.append(y) for divider in dividers: print (\"Number {} can be divided by {}!\".format(x,divider)) solution Solution 2 # The output is again not very nice here, you can replace the last two lines by this for nicer output: myNumbers = range(1,5) for x in myNumbers: dividers = [] for y in range(1,x+1): if not (x % y): dividers.append(y) #for divider in dividers: dividerList = \", \".join([str(divider) for divider in dividers]) print (\"Number {} can be divided by {}!\".format(x,dividerList)) 6.3 While loop A while loop is dependent on a condition, as long as this condition is evaluated as True the loop will continue. Its structure is very similar to the for-loop we saw here above. result = 0 while result < 10: # add 1 to the result result += 1 print(result) This is an endless loop: FYI, if you execute this, you’ll end up in an enternal loop. To break the loop, press stop button. while True: print(\"Endless...\") While loops are more flexible than for loops, as you can make them end whenever necessary depending on code within the loop itself: baseValue = 2 powerValue = 1 powerResult = 0 while powerResult < 1000: powerResult = baseValue ** powerValue print(\"{} to the power {} is {}\".format(baseValue,powerValue,powerResult)) powerValue += 1 # Add one to itself - this kind of step is crucial in a while loop, or it will be endless! Note that the last value printed is greater than 1000, the while condition is only checked at the start of the loop. You should check where the first result is calculated as this may impact the result! Here we changed the order of calculating the value. We initialized the loop and put the calculation at the very end: baseValue = 2 powerValue = 1 powerResult = 0 powerResult = baseValue ** powerValue while powerResult < 1000: print(\"{} to the power {} is {}\".format(baseValue,powerValue,powerResult)) powerValue += 1 # Add one to itself - this kind of step is crucial in a while loop, or it will be endless! powerResult = baseValue ** powerValue hands_on Exercise 6.3.1 Try to reproduce a for-loop (the example of numbers divisible by 17) by using a while-loop. solution Solution # Try to reproduce a for-loop (the example of numbers divisible by 17) by using a while-loop. myNumber = 1 myDivider = 17 while myNumber <= 50: if not (myNumber % myDivider): # Nothing left after division, so number is divisible. print(\"{} is divisible by {}\".format(str(myNumber),str(myDivider))) myNumber += 1 hands_on Exercise 6.3.2 Write a program where you start with a list of numbers from 1 to 100, and you then remove every number from this list that can be divided by 3 or by 5. Print the result. Tip: you have to make a copy of the original list here, otherwise Python will get ‘confused’ when you remove values from the list while it’s looping over it. Use [:] for this purpose. solution Solution # Write a program where you start with a list of numbers from 1 to 100, and you then remove every number from this list that can be divided by 3 or by 5. Print the result. # Tip: you have to make a copy of the original list here, otherwise Python will get 'confused' # when you remove values from the list while it's looping over it myNumberList = list(range(1,101)) for number in myNumberList[:]: if not (number % 3) or not (number % 5): myNumberList.pop(myNumberList.index(number)) print(myNumberList) hands_on Exercise 6.3.3 Write a program where you ask the user for an integer (whole number), and keep on asking if they give the wrong input. Check whether the number can be divided by 7, and print the result. solution Solution # Write a program where you ask the user for an integer (whole number), and keep on asking if they give the wrong input. Check whether the number can be divided by 7, and print the result. myNumberList = range(1,101) # Keep on checking until you have a number, prime the while loop as well isNumber = False while not (isNumber): inputString = input(\"Give a number:\") if inputString.isdigit(): isNumber = True number = int(inputString) else: print(\"Incorrect, not a whole number, try again.\") if not (number % 7): print(\"{} can be divided by 7!\".format(number)) else: print(\"Number not divisible by 7\") 6.4 Iterating through two files at the same time Python has a built-in function which allows you to iterate through multiple e.g. lists or strings at the same time. For two strings, it would look like this: x = 'abcde' y = 'fghij' count = 0 for i,j in zip(x,y): count += 1 print(\"Iteration: {}. The value i is {}, and the value j is {}\".format(count, i, j)) And the principle is practically the same for three (or more) strings. x = 'abcde' y = 'fghij' z = 'klmno' count = 0 for i,j,k in zip(x,y,z): count += 1 print(\"Iteration: {}. The value i is {}, the value j is {} and the value k is {}\".format(count, i, j, k))"},{"title":"Share: Data selection and preservation","url":"/topics/data-management-plans/tutorials/share-preserve/tutorial.html","tags":[],"body":"Introduction Introduction to data selection and preservation Research should be transparent and you should always be able to revert back to your data if necessary and be able to show others how you came to your results. Therefore, your research data with all information reasonably necessary for verification needs to be preserved. With well-managed and preserved research data, you can defend yourself against allegations of mistakes. You can also prevent wrong conclusions from further spreading into the scientific community if there really are mistakes. Long term data preservation Research data can be preserved for different reasons such as verification and/or possible reuse. It can be your own wish or that of your university, funder or journal. Verification TODO: adapt this part The Netherlands Code of Conduct for Academic Practice (VSNU) states that raw data from research must be kept available for a minimum of ten years. This statement is also included in the Utrecht University Policy framework for research data: “Archived research data are to be retained for a minimum of ten years, commencing from the date that the research results are published.” Reuse It may be worthwhile to make (part of) your data available for a longer period of time and/or for a wider audience. Data which are suitable to keep for reuse are interpretable data on which new research can be based, independent of the publication. On the one hand, making research data reusable will need extra effort. On the other hand, possible reuse, even by your future self, might bring you lots of benefits and credits. Consider if your data is worth the effort of making it reusable or if preserving and archiving for verification is enough. Reuse is explained more in depth in the next part of this course: ‘Availability for reuse’. In this part we will focus on selection and preservation of research data for verification purposes. Data package Keeping data for verification serves the specific goal of having transparent, reproducible research. Alternatives to preserving raw data If preserving your raw data poses problems, alternatives can also ensure verfication. For instance, transcripts of recorded interviews could hold all important information and may be less privacy-sensitive, so it is reasonable to preserve those instead of the recordings themselves. Also, if raw data is very large, preserving your data only in some processed form could be an alternative. Combined with, for instance, a demonstrable quality check on the processing. The contents of your data package TODO: add image for illustration/zenodo? Others should be able to understand what you did. It is not enough to just provide data. Instead you should preserve a package with everything included that is necessary to reproduce your results. Think of including the following: Primary (raw) data; Secondary (processed) data; Protocols; Computer code/scripts; Lab journals; Metadata and/or codebooks describing the data; An overview of what the contents of the data package stating what file contains what information, and how these are related. The data should contain a reference to any publication which is based on the data. To make understanding your data less dependent on information in the publication, you can also add information on: Collection methods; Procedures; Experimental protocol; Your research question; Stimuli used; Sample descriptions. This is especially practical if the data package can be found and used on its own account. This is the case if it is published in a data repository or data journal as a data package for reuse. Do not forget to explicitly state who is responsible for the content of the data package, who is to be contacted in case of a request for access, and under what conditions access is granted. Where to preserve what type of data? During your research, you generate research results that can be made available for others. A paper or publication is the most traditional way of making results available, but it is by no means the only way. A relatively new way of making results available is using a public data repository. As you have just learned, preserving your data may serve the purpose of verification or reuse. Public data repositories cater to both needs. In addition, they handle requests to view or use your data which means you do not have to take care of such requests yourself. In the example below, you find a workflow for experimental research. What information can be made available in what place? Drag the items on the right to the correct place in the figure. Please note that some items can be used more than once. TODO: add H5P quiz and PDF solution? Accounting for data of others If you are permitted to use data from other parties, you will have to account for those as well if your research is to be verifiable and reproducible by others. You may recognise this from chapter 1 of this course: Data collection: Discover existing data, weblecture ‘Assessing usefulness of research data of others’ (5 of 10). You have the following options: If the used data is preserved correctly somewhere for the coming ten years, refer to the data repository in question. If it is not taken care of, contact the responsible persons, negotiate correct preservation in a data repository for ten years, and refer to that repository. If this isn’t possible, try to arrange a local copy that you preserve yourself; If this isn’t allowed, you will not be able to present the data in case of questions. Therefore, you should question yourself whether you can actually use the data. Figure 1: Preserve for 10 years Accounting for data of others on websites If you find interesting information on a website that you want to refer to, it is possible that this information will not be future proof. The link or web address might change over time (link rot). Or the information on a website is updated, changed or replaced with other content (content drift). It is possible to archive web pages on a web archive like the Internet Archive. You can capture a web page as it appears now for use as a trusted citation in the future (save a page). You will get an alternative link, pointing to the archived, static version of the page. Use this alternative link as a reference to the online information. How to preserve your data correctly In order for the data to survive for the long term, an active preservation regime has to be applied. The bad news is, data automatically gets lost over time. There are five main ways your data can be lost: Digital sources degrade over time (‘bit rot’); File formats and software become outdated; The media on which your data is stored becomes outdated or defective; Disaster strikes the storage location; The person that understands the data finds another job or data simply becomes forgotten. In this video below you will learn how to minimise the risk of losing data. You are also given good preservation practices. Match the solutions to the data loss From the weblecture you learned how to prevent data loss. Can you recall all applicable active regimes, as explained in the weblecture? Below you see a list of solutions to prevent data loss. Underneath that list you see a list of risks for data loss. Please add the number of each solution to the correct risk. Solutions to prevent data loss Have multiple copies. Use a checksum to identify faulty copies Use preferred file formats that can be opened by a wide range of software. Update the file format to a current one. Move data to fresh media well before the media’s expiration date. Have multiple copies. Move data to fresh media well before the media’s expiration date. Document your data well. Advertise the content in a data catalogue. TODO: add quiz text solution Write your data management plan for your data preservation Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module on data selection and preservation. You should be able to complete the following questions in the section ‘Data selection and preservation’: Which data should be preserved and/or shared? How and where will you keep your data for the long term?"},{"title":"02 OTU creation using LotuS","url":"/topics/metagenomics/tutorials/OTU_creation/tutorial.html","tags":[],"body":"OTU creation using LotuS In this tutorial we will create a genus abundance table from two 454 sequencer runs using a pipeline called LotuS. A genus abundance table contains counts of different genera in a several of samples – Rows are the different genera and columns the samples. As a simple example, take a look at this table:             Genus bl10 bl11 bl12 bl128 bl13 Bacteria 24 52 39 63 181 Bacteroides 169 27 7 42 6 Porphyromonadacea 370 346 621 565 224 This table tells us how often we observe unclassified Bacteria, Bacteroides and unclassified Porphyromonadaceae in the 5 samples bl10, bl11, bl12, bl128 and bl13. A matrix like this will be used for the next tutorial on numerical ecology and created from raw sequence data within this tutorial. The data In a recent experiment, we sequenced 73 samples in two 454 runs, the raw fasta and quality files are in /home/VIBTrainingX/metagenomics/ on the bits server. For each run we have a fasta (.fna) and quality (.qual) file. Go to this directory using the command cd and become aware of the files required from the experimenter (command ls). You can always take a look at files and their contents using viewing commands like less. The sequence files were multiplexed before the experiment, that is a small nucleotide sequence – the barcode - was attached to each read, specific for each experiment. A mapping file is typically used, containing the link between a sequence barcode and the name of the experiment and is essential to demultiplex the fasta files. The tools LotuS is actually a set of tools that were installed in the /opt/ folder. First go to the lotus website and familiarize yourself with the basic documentation. To start the exercises, go to the directory where Lotus is installed. cd /opt/lotus-1.62/lotus_pipeline/ From this directory you can run all the tools. To reach all data files (e.g. input files) you have to provide the path to the files: ~/metagenomics/ The analysis Creation of Mapping file. An Excel is provided, with some basic experiment annotation. The fwd primer is given as ACTYAAAKGAATTGACGG, but if you search for the primer sequence in the reads (in one of the .fna files) you will not find it because you need to reverse translate the primer sequence first using [http://www.bioinformatics.org/sms/rev_comp.html this tool]. So you see annotation provided by the provider is not always correct. Lotus needs experiment annotation to map input files to barcodes. Based on the documentation on [http://psbweb05.psb.ugent.be/lotus/documentation.html#MapFile the Lotus website], create a mapping file for this experiment. This means that you need to replace the column headers of the Excel file to terms that are accepted by Lotus and that you have to indicate that there is a .fna and a .qual file for each run. The header line should be preceeded by a #. The mapping file should at least contain four columns with the following headers: SampleID BarcodeSequence fnaFile qualFile Save the file as a tab-delimited text file. You can always test the validity of your mapping file with the command ./lotus.pl -check_map [your mapping file] If you have fastq files as input the fnaFile and qualFile columns would be replaced by one fastqFile column. Changing the data format of the input files. Sometimes you need to transcribe data from one format to another. For instance we could transform the fasta files (.fna) to fastq files (.fq). This can be done with the program sdm, that is part of the LotuS pipeline. Take a look at the sdm help by typing ./sdm and exploring the options, e.g. ./sdm -help_commands Then, using command line arguments, transcribe the fasta + qual files of the Anh experiment into fastq files. Take a look at output and log files created by sdm. hands_on Hands-on: Exercise 1 question Question How to transform fasta + qual files into fastq files ? solution Solution sudo ./sdm -i_fna ~/metagenomics/Anh.1.fna -i_qual ~/metagenomics/Anh.1.qual -o_fastq t1.fq In the lotus_pipeline folder the fastq file t1.fq was generated, to take a look at the file use head t1.fq Do the same for the t1.log file: you see that sdm is not only used to transform fasta into fastq files but it is also capable of doing quality filtering on the raw reads files. Setting up a quality filter of the input sequence files. Since we want to make sure the quality filtering of the input file is strict, LotuS offers several quality filtering options. Quality settings are different for different data formats, that´s why Lotus offers a file with specific settings for each platform. Since we have 454 data we will take a look at the file sdm_454.txt. less sdm_454.txt Read the comments (line starting with “#”) to each option and think which quality filtering options might be important in order to create OTUs from the raw sequences. (Hint: an OTU is a clustering of similar sequences with the aim to have one cluster of sequences for each species that was originally present in the samples. Take into account that sequencing machines make errors and that PCR amplification of the 16S rDNA is similarly with errors). Think about a set of parameters, including the statistical information from step 2, and save these in your copy of sdm_options.txt for later use. Check the sdm quality filter settings. Some of the default filter settings are: MinSeqLength=250 : Only use reads of at least 250 nt long after processing (remember we are working with long reads from 454 sequencing) TruncateSequenceLength = 250 : Cut all reads after 250 nt QualWindowWidth = 50 and QualWindowThreshold = 25 : Remove all reads where average quality is = 0,5 maxHomonucleotide = 8 : Remove all reads with a homonucleotide run (repeat of same nt) >= 8 RejectSeqWithoutFwdPrim = TRUE : Remove all reads that do not contain the forward primer Demultiplexing and quality filter the input files. For this step you will need the mapping file from Step 1 and the file with the quality filtering settings for 454 data. Use the sdm command to demultiplex and filter all input files at the same time into a local folder ‘‘demultDir’’. First create the folder where the demultiplexed files will be written in ~/metagenomics/: mkdir ~/metagenomics/demultDir Since the mapping file contains information on all files, you have to provide an input path to the folder that contains the input files (.fna + .qual) to sdm. hands_on Hands-on: Exercise 2 question Question How to demultiplex and quality filter files ? solution Solution ./sdm -i_path ~/metagenomics/ -o_fastq t1.fq -o_demultiplex ~/metagenomics/demultDir/ -map ~/metagenomics/map.txt -options sdm_454.txt Input is the folder containing the .fna and .qual files. The demultiplexing will fill the demultDir folder with fastq files. Discuss the output files and what each of these represents. In this experiment multiple samples were sequenced in the same lane. Two lanes were used, each containing 37 samples. After sequencing, this results in two files with reads. To know which sample a read comes from, unique bar codes are incorporated into the adapter sequences. One specific bar code for each sample. In this step reads from different samples (but from the same lane thus in the same fasta file) are split over separate fastq files, one for each sample. Mapping file creation when sequence provider provides demultiplexed files. Now that you have demultiplexed the files into a single folder, you might be aware that sequence providers often deliver files in this format: already demultiplexed into single files. In this case slight modifications to the mapping file are enough to change the input from non-demultiplexed large file(s) to demultiplexed-many-small-files. Note that lotus has a special script that creates the mapping file for you in this case. The script is autoMap.pl. It is used to link SampleIDs to demultiplexed files. Run autoMap. ./autoMap.pl ~/metagenomics/demultDir/ ~/metagenomics/automap.txt 1,1 Running Lotus. We will run Lotus on the demultiplexed files. Use the mapping file you generated in Step 5 and the settings file sdm_454.txt. Use the utax taxonomy to assign a phylogeny to the derived OTUs. Run lotus from out the /opt/lotus_pipeline/ and save the output in the folder ‘‘testR’’ hands_on Hands-on: Exercise 3 question Question How to run lotus solution Solution sudo ./lotus.pl -i ~/metagenomics/demultDir/ -o ~/metagenomics/testR/ -m ~/metagenomics/automap.txt Input is the folder containing the .fna and .qual files. The demultiplexing will fill the demultDir folder with fastq files. In case you haven’t done any quality filtering yet, you can still do it now. The command would then be: sudo ./lotus.pl -i ~/metagenomics/demultDir/ -o ~/metagenomics/testR/ -m ~/metagenomics/automap.txt -s sdm_454.txt Peek at the file hiera_RDP (using head). The file maps eachg OTU to a genus. Peek at the file OTU.txt (using head). The first line contains the number of reads that represent OTU_1 in each sample. Peek at the file otus.fa (using head). It contains the reads representing each OTU. You can use this file to blast the sequences to check if they are really from the OTU they were assigned to. Go to the folder higherLvl. This contains the data that we are going to use in the Ecology analysis. Go to the folder LotuSLogs. This contains the settings of the analysis. For instance, peek a the file demulti.log: it shows how many reads were rejected… The file citations.txt contains the references for reporting your LotuS results. Using a different taxonomy assignment on a finished run. In this step we want to reassign the taxonomy to a LotuS run, but keep exactly the same OTUs. In this exercise, assign the OTUs to the Silva taxonomy. This option is useful, if e.g. you want to keep your work on a given OTU set (as well as the phylogenetic tree), but want to try out what would have happened if you had used e.g. Silva as reference database instead of utax. hands_on Hands-on: Exercise 4 question Question How to reassign the taxonomy with Silva as reference database? solution Solution sudo ./lotus.pl -i ~/metagenomics/demultDir/ -o ~/metagenomics/testR2/ -m ~/metagenomics/automap.txt -s sdm_454.txt -refDB SLV -redoTaxOnly 1 Input is the folder containing the .fna and .qual files. The demultiplexing will fill the demultDir folder with fastq files. Using a custom database. The research of honey bee gut communities have very specific taxonomic names for already known bacteria. In order to accomodate for their naming sheme, we will use a very specific database that contains 16S sequences of bacteria mostly found in the honey bee gut. Download the bee taxonomy in tax format and [http://psbweb05.psb.ugent.be/lotus/packs/DB/beeTax/beeTax.fna bee taxonomy in fna format]. Use the two provided files (fna, tax) to again redo the taxonomy, but this time assigning first using the honey bee DB and secondly everything with low hit should be assigned with the SILVA database. hands_on Hands-on: Exercise 5 question Question Use honey bee taxonomy database ? solution Solution sudo ./lotus.pl -i XX -o ~/metagenomics/testR3/ -redoTaxOnly 1 \\ -m ~/metagenomics/LASI_Spring_2_bees_barn_3_map_lts_5.txt \\ -refDB ~/metagenomics/beeTax.fna,SLV -tax4refDB ~/metagenomics/beeTax.tax Input is the folder containing the .fna and .qual files. The demultiplexing will fill the demultDir folder with fastq files. Get everything assigned! In this step we want to assign every OTU sequence to a database target – and we don’t care about false positive assignments! Of course this is per se wrong, but in some cases you just want to know what the best hit would be, even if it is only 90% similar to your OTU sequence. LotuS provides several options that allow tweaking towards more lenient assignments. Find all options related to this and try to create the most extreme case with these options, by reassigning the taxonomy again as in the previous step. Try a different sequence clustering algorithm. Now rerun lotus, but try to optimize for a lot of small, hard defined OTUs (that might correspond to something like strain level). Which clustering algorithm might be suitable? Which clustering cutoffs make sense? For this specific run, use the first mapping file you created (step 1) and the non-demultiplexed input files. Save this output in the folder ‘‘testR4’’ Your own best run. Now that you have run LotuS with various databases and options, go back and look at the output folder of the different runs, look at the statistics provided in the ‘‘LotuSLogS’’ subfolder. Based on this, tune the sdm filtering parameter file from step 3 (again), choose the database you think best suited/most interesting, and choose a clustering algorithm. With this create run the sample set again, saving the output in folder ‘‘testrun1.3’’. This output folder you can use in the following session on numerical ecology. If LotuS run has finished, go to the specified output folder and copy the genus.txt from the output folder to your home folder: cp testrun1.3/ higherLvl/genus.txt ~ Using Illumina data as input. In all the analysis before we were using 2 x 454 runs from an outdated next generation sequencing technology. For the next exercise we will look at the output of an Illumina miSeq sequencing platform, that is still being used a lot nowadays. Set up the mapping file, using [http://data.bits.vib.be/pub/trainingen/metagenomics/Miseq.xlsx the provided Miseq.xlsx file]. Run LotuS, after you set up a custom sdm configuration file and using a combination of parameters you learned about in previous steps. This run might take some time longer to finish. Be sure you set it to use all the cores of your computer and let it run over the lunch break. Congratulations, now you know how to process raw sequence files to meaningful summary tables, that can be directly analyzed in R or even Excel! In the next tutorial this matrix will be analyzed with the help of R, after the lunch break."},{"title":"01 Tools and Data files","url":"/topics/metagenomics/tutorials/Introduction/tutorial.html","tags":[],"body":"Tools Lotus pipeline LotuS offers a lightweight complete 16S/18S/ITS pipeline to Demultiplex and filter fasta or fastq sequences Denoise, remove chimeric sequences and cluster sequences into very high quality OTUs that perform at a similar level to mothur / dada2 Determine taxonomic origin of each OTU using >5 spezialized and general purpose database or statistical algorithms Construct OTU, genus, family, class, order and phylum abundance tables in .txt or .biom format Reconstruct OTU phylogenetic tree More information at LotuS home page usearch Download usearch version 8 and copy the executable in a folder e.g. /usr.bin/tools/ which you can reach (you might to be superuser for this) Make executable: sudo chmod +x /usr/bin/tools/usearch8.1.1861_i86linux32 Create a symbolic link into the folder where Lotus will search for it: sudo ln -s /usr/bin/tools/usearch8.1.1861_i86linux32 /usr/bin/tools/lotus_pipeline/bin/usearch_bin ` R package You also need R with the vegan package installed."},{"title":"03 Ecology Analysis using vegan","url":"/topics/metagenomics/tutorials/Ecology_Analysis_using_vegan/tutorial.html","tags":[],"body":"Ecology Analysis using vegan In this exercise we will look at a data matrix of 16S rRNA counts in 74 samples. This dataset is the microbiota composition of 74 mice from 5 different mice strains. The original research aim was to define the effect that the mouse genome has on the microbiota and what the effect of living in the same cage would be. However, we found much stronger trends in the data, and these we will look at in this exercise. The 454 data was already compiled into a matrix with genus abundance per sample in a previous step. This matrix is called a feature abundance matrix, or abundance matrix for short. We will do an ecology-oriented analysis of the data, in later steps also taking metadata (experimental, environmental or clinical data that was collected for each sample, independent of the DNA) into account. The aim of this tutorial is to get an idea of the very basic steps of ecological data analysis using the programming language R. The gene abundance table (Genus.txt) can be found in the folder /home/VIBTrainingX/NGS/metagenomics/higherLvl folder on the server. Those who are working on their own laptop can download it from the lotus website. Set the folder with the provided files as your working directory in R using setw. This way required files can be easily loaded. To find out how to use this command, you can type ?setwd() to open the help. If there are other R-commands that you want to know more about, you can open the R-help for that command by entering in the R-prompt ?command. This will be very useful when working with R, make sure to use this a lot as you can only learn more :o). hands_on Hands-on: Exercise 1 question Question How to set the working directory in R solution Solution setwd(\"dir_to_data\") Load the provided data into the matrix M (Genus.txt, actual genus abundance data), using the read.delim command, saving the loaded table as M. Make sure, the row names are correctly read in. As R reads the matrix as an object of class data.frame, we convert M from a data.frame to a matrix M=as.matrix(M). This is important for some of the following calculations, where we need a matrix class object. hands_on Hands-on: Exercise 1 question Question How to read in data as matrix ? solution Solution # read in data as matrix M = read.delim(file=\"Genus.txt\",row.names=1) M = as.matrix(M) The matrix you loaded represents the number of 16S sequences assignable to each genus, which we could find in the samples. Also note that not all genera are real genera, but partly assigned unknown sequences. With these groups we do not know if this is a single genus or in fact several genera or in extreme cases even several classes, that just all fall under the same phylum tag. What are the advantages and disadvantages of keeping such undefined groups in the data? Use the function edit(M) to better view the abundance matrix. Let’s look at some basic features of the abundance matrix. The summary(M) command is a good start, but also look at total row and column counts (colSums, rowSums command). To see how the genera are distributed within each sample, we will plot a sample-wise density plot.We will be using a combination of the density, lines and lapply functions, to draw the densities of values found in each sample. Let’s start with looking at the density of the first sample. In R you can access specific columns by writing the matrix coordinates in square brackets. For example M[1,] shows the first row of a matrix, M[,7] shows the 7th column etc: hands_on Hands-on: Exercise 3 question Question How to estimate density of first sample ? solution Solution # estimate density of first sample densityOfSample1 = density(M[,1]) Look at the object densityOfSample1 by simply entering the object name into the command prompt. Next try to visualize it with plot(densityOfSample1). In this plot you see that most genera are at 0 abundance, some genera have an abundance 0 The sum of each column in this matrix will tell us how many species were detected in total within the respective sample, use the apply and sum functions , saving the result in rich1. hands_on Hands-on: Exercise 11 question Question How to calculate the sum of each column ? solution Solution # Calculate sum of each column rich1 = apply(OnceOrMoreOftenPresent,2,sum) Compare the richness values in rich1 to the richness obtained on the rarefied matrix M2, calculated with a shortened command: # Calculate number of present species in each sample using the rarefied data rich2 = apply(M2>0,2,sum) Compare rich1 and rich2 in a matrix value by value. We use the cbind command to bind two vectors column wise together, so we get a matrix with 2 columns. Order this matrix by the richness values in rich1, using the order command and accessing the vector representation with [] square brackets. # Create new matrix with two columns: rich1 and rich2 and order rows according to rich1 values cbind(rich1,rich2)[order(rich1),] What does the second part of the formula do? What happens if you change that to order(rich2)? Discuss which richness values have the highest value to the researcher and why the order is very different between these two richness estimates. Is one way clearly wrong? Why did we choose 1000 as cutoff for the sequences per sample? What is the maximum value we could choose? First samples are clustered to see underlying data structures. For this tutorial we will choose a hierarchical clustering, based on a bray-curtis distance between samples, using the function vegdist. Make sure the distances are calculated between Samples and not Genera. Next, use the function hclust on the distance matrix, saving the output in variable cluster, and subsequently plot the clustering of the samples (using plot). Take a guess of how many groups there might be in this clustering? hands_on Hands-on: Exercise 11 question Question How to cluster samples and plot results ? solution Solution # cluster samples and plot results BCD = vegdist(t(M1), dist=\"bray\") cluster = hclust(BCD) plot(cluster) To visualize the samples and their relatedness to each other in a two-dimensional space, we can use an ordination to visualize the data in a low dimensional space. The dimensionality of the original matrix (73 genera=73 dimensions) is reduced to two dimensions. If you know what a PCA (Principal component analysis) is, this step will use a conceptually similar, but methodologically quite different technique to perform an ordination of the data, NMDS (non-metric multidimensional scaling). Start by calculating a 2-dimensional NMDS of the data using M1, using the Bray-Curtis distance in the function metaMDS, saving the result to nmds. Again, make sure that samples are being ordinated and not Genera. hands_on Hands-on: Exercise 11 question Question How to calculate the NMDS ? solution Solution # calculate NMDS nmds = metaMDS(t(M1),distance = \"bray\") #actual NMDS command, matrix needs to be transformed to conform with vegan’s standards Take a look at the nmds object and explore some of its features (e.g. type str(nmds) to see what variables are stored within the NMDS object). Try to find out what the stress of your ordination is. What does stress stand for (tip: go to the R help on metaMDS)? Next we can visualize the NMDS, similar to what you get out of PCA’s, displaying samples only: # plot NMDS plot(nmds,display =\"sites\") The important difference of NMDS compared to PCA is, that NMDS works with any kind of distance metric, while PCA can only use Euclidean distances between samples. A second important feature of NMDS is, that this method finds non-parametric, monotonic relationships between objects; in short: it doesn’t assume a specific data distribution. Why might these two features be important for ecologists? You might have noticed that you see two clusters, similar to the hierarchical clustering of the data. We can get for each sample the identity within the two clusters using the cutree commands, specifying k=2 (2 clusters). This can be plotted into the NMDS with the following command: # identify clusters memb = cutree(cluster, k = 2) ordispider(nmds,memb) Congratulations, you have just visualized the mouse enterotypes. Next we are going to look closer at these. If you want to know the exact methods to detect enterotypes in your data visit [http://enterotype.embl.de/enterotypes.html http://enterotype.embl.de/enterotypes.html] In the last step, we will test for all the genera in the matrix whether they show significant differences between two clusters. The scientific question we are posing here is: what are the significant differences in the gut microbiota of between enterotypes? We will use a non-parametric test (kruskal-wallis) to do the tests, as ecological data is in most cases not normally distributed. This test is very similar to the student t-test, and the interpretation works just the same way. Use the function kruskal.test to test the first genera (M[1,]) for significant differences between the two cluster groups (in object memb). Save the output of this command in variable Kt. hands_on Hands-on: Exercise 12 question Question How to test if there is a difference between the two clusters for the first genus ? solution Solution # Test if there is a difference between the two clusters for the first genus Kt = kruskal.test(M1[1,],memb) Look at the output of this function. This will show you a human readable summary of the test and the result. You can access elements of a list (Kt is a list in this case) using the $ operator. Try to extract the p-value from the Kt object. Once you know how, we can start to calculate the significance for every genus in the M1 matrix,. These p-values we will store in a newly created vector pvals. Let’s add the first 2 p-values to the vector: # Test if there is a difference between the two clusters for the first and second genera. Store p-values in a vector. pvals = c() pvals[1] = kruskal.test(M1[1,], memb)$p.value pvals[2] = kruskal.test(M1[2,], memb)$p.value Since doing this 73 times takes a long time, we will be using a for-loop to loop over the matrix and do this for us. We could as well use the apply function, but the syntax would get a little more complicated, since we are only interested in a subpart of the result, the $p.value part. Try to write a for-loop, to calculate the p-value 73 times. hands_on Hands-on: Exercise 13 question Question How to test if there is a difference between the two clusters for all genera ? solution Solution # Test if there is a difference between the two clusters for all genera for (i in 1:dim(M1)[1]) { pvals[i] = kruskal.test(M1[i,], memb)$p.value } As an additional help, you can add the name of the taxa to the pvals vector using the names command (that will name a vector): # Add names to the vector names(pvals) = dimnames(M1)[[1]] Which taxa are significantly different? In this case we will use the normalized M1 matrix, can you explain why we do not use the M or M2 matrix? Would either be wrong to use? In total we were testing in 73 genera, if their p-value was below a threshold of 0.05. What is the chance of observing data with a p-value >0.05 by random chance? How many genera do you expect to be below this threshold by random chance? To avoid statistical errors of this kind, we will use a Benjamini-Hochberg multiple testing correction, implemented in the R function p.adjust. Save the result as qvals. hands_on Hands-on: Exercise 14 question Question How to perform multiple testing correction of p-values using Benjamini-Hochberg method ? solution Solution # Multiple testing correction of p-values using Benjamini-Hochberg method qvals = p.adjust(pvals,method = \"hochberg\") What do you see in this test? What would you report on this dataset, based on these values? Try sorting the q-values to see the most significant differences first: # Sorting q-values sort(qvals) Now that you have finished the tutorials, you should be able to analyze any new dataset of amplicon data, using the LotuS pipeline and performing a basic analysis with R, including Data normalization Clustering analysis Ordination Univariate statistics You can always expand upon these concepts, using this tutorial as starting point. Just remember that R is a very flexible language, and all these commands can be expanded for new purposes and visualizations. Data sources All the material provided in this tutorial are from metagenomic study on mice knockouts. Further analysis of the data can be found in the reference below. Reference Hildebrand, F., Nguyen, A. T. L., Brinkman, B., Yunta, R. G., Cauwe, B., Vandenabeele, P., … Raes, J. (2013). Inflammation-associated enterotypes, host genotype, cage and inter-individual effects drive gut microbiota variation in common laboratory mice. Genome Biology, 14(1), R4. doi:10.1186/gb-2013-14-1-r4"},{"title":"Prepare: Data collection","url":"/topics/data-management-plans/tutorials/prepare-collect/tutorial.html","tags":[],"body":"Introduction Introduction to data collection By now you will have obtained some idea of what research data management is all about. Now we will have a more in-depth look into the different phases of your research by starting with data collection. Data collection involves understanding the different types of data you collect. Depending on the nature of your research, there are different methods of collecting data and thus different types of data. Your data may be physical (paper records or archival forms) or digital (database contents or Excel data). The source of your data may be external, you collect it yourself or you generate it from a machine. When you write your data management plan you will need to take into account the type of data you collect, the source of the data, and how you will process and analyse your data. You can watch the video below, provided by TU Delft, about data collection. The video stops at 1:12. Preferred formats for your research data This part is based on the online Research Data Management training ‘MANTRA’ of The University of Edinburgh (CC BY: https://mantra.edina.ac.uk/) and Managing Data @ Melbourne. Figure 1: Learning objectives The file formats you use to generate your research data will influence how you can manage them over time, i.e. a program or application must be able to recognise the file format in order to access your data within the file. For example, a web browser is able to process and display a file in the HTML file format so that it appears as a web page. If the browser encounters another file type, it may need to call on a special plug-in to view it. Or it may simply let you download the file to view if it can recognise it in another program. To identify the file format, files usually have a file name extension, or suffix that follows a full stop in the file name and contains three or four letters, like for example: TODO: add PDF with links to preferred file formats .txt text .pdf portable document format .jpg joint photographic experts group .csv comma separated values .html hypertext markup language .xml extensible markup language .rtf rich text format Figure 2: Background on proprietary and open formats Figure 3: Background on proprietary and open formats question Question Determine which format is proprietary and which is an open format .xml .pdf .psd .odf .ppt .docx .csv .xls Check your answers! Proprietary: .psd, .docx, .xls, .ppt Open format: .csv, .xml, .odf, .pdf TODO: list of preferred formats Figure 4: Risks of file conversion question Question While file conversion or migration sometimes has to be done, there are also risks. Which ones can you think of? Check your answers! file size may change and even become surprisingly large blanks used as missing data code special characters and end of line returns may change relation among items in a table and among tables may be lost layers, color fidelity and resolution may be lost or changed in image files fonts, footnotes and links to other documents may change frame rate, sound quality, codecs and wrappers may be altered in multimedia files last characters in rows (due to row size limitations) may be altered hands_on Hands On Open the following .docx file to the preferred format .txt: PreferredFormatsExcersizePenguinDOC.docx Convert this docx file to the preferred format .txt Open the text file in an editor Is all formatting preserved OK? Check your answers! No, the format Microsoft Word creates saves the layout together with the textual and other elements. The .txt format created by Word is only the textual information in your file. hands_on Hands On Open the following .docx file to the preferred format .txt: PreferredFormatsExcersizePenguinDOC.docx Convert this docx file to the preferred format .odt Open the .odt file Is all formatting preserved OK? Check your answers! No, ODT files are formatted using the OASIS OpenDocument XML-based standard. When you open an OpenDocument Text file in Word, it might not have the same formatting as it did in the original application it was created in. This is because of the differences between applications that use the OpenDocument Format. Discovering existing data Where to discover existing data? Watch the screencast below. In this screencast, you will be guided through different ways to find data. hands_on Hands On You have just learned that there are different places to find data. By actively searching the different places, you will get an understanding of the differences. Look at the different portals below. Some of them have been showcased in the screencast, some of them are additional. Google - add “database OR registry OR dataset OR archive OR statistics” to your search Registry of Research Data Repositories re3data - find appropriate repositories holding interesting data ZanRan - search engine for tables and graphes within .pdf or .html on the internet Elsevier Data Search - try out chip-seq drosophila Google Dataset Search - try out chip-seq drosophila. Google Dataset Search indexes OmicsDI, an index providing a knowledge discovery framework across heterogeneous omics data (genomics, proteomics, transcriptomics and metabolomics). Assess the usefullness of existing data How useful is a dataset? Follow this short tutorial. Assess the usefullness of existing data yourself In the previous activity, the lecture described four main points to check if you want to reuse existing data: Condition for reuse Context Trustworthiness Persistence In the following quizzes, take a closer look at the description or metadata of some datasets and assess the usefulness of these datasets yourself. As the description or metadata of datasets can be lacking in several different areas at the same time, it will be indicated per assignment on which of the four main points your focus should be. hands_on Hands On Can you re-use this dataset on Spatial Patterns of Water-dispersed Seed Deposition along Stream Riparian Gradients in DataverseNL? Maybe Yes No Check your answer! Yes, the Terms of use indicate that there is a Creative Commons license ‘Public Domain Dedication’, which means you can copy, modify, distribute and perform thge work, even for commercial purposes, all without asking permission. hands_on Hands On Can you re-use this weather dataset? Maybe Yes No Check your answer! Maybe, although the website states ‘We hope that you will enjoy using ClimaTempss as much as we have enjoyed developing it!”, there is no clear license or use agreement and directions on how to cite the data are lacking. The use has not been defined nor explained. In this case of re-use you should simply contact the creators. hands_on Hands On Given the follwing description of a dataset: can you assess the usefulness of this dataset to establish cholestasis (an unhealthy condition of the liver) parameters in livers in the age group of puberty through adulthood? Please focus on the context. Description: “We measured livers for several parameters of cholestasis. The subjects were in advanced stages of liver cancer.” Maybe Yes No Check your answer! No, the dataset is not useful because the subjets have cancer. This should affect the values of parameters for cholestasis. You would rather have a dataset of healthy subjects. hands_on Hands On Would you trust the following dataset on heart rate under severe physical stress? Heart rate (beats per minute): 124, 160, 240, 0, 120, 400, 198, 156, 167 Please focus on the trustworthiness. Maybe Yes No Check your answer! No, there are weird values in the dataset, a value of zero is unlikely. And overall, the values are on the high side. hands_on Hands On Is your research likely to be reproducible when you use the following the following infrastructure? The datasets is created during a PhD. Conditions for use state that it is a dataset stored and shared by the PhD student on his university account. Maybe Yes No Check your answer! No, it is unlikely that the dataset can be reused since you do not have certainty that the files stored on the university file drives are availble for at least 10 years which is the current rule for data availablity. Describe what kind of data you will generate Having a clear view of what data you will generate will enable you to plan its management. You can create an overview of the data you produce or collect by drawing the data in a workflow, or noting down in a table. Please watch the video below. Tessa Pronk will explain to you how to describe your data. Order elements in your data flow TODO: add H5P quiz Copyright and Intellectual Property Rights (IPR) issues Copyright is a form of intellectual property right which arises automatically if an original work is created. Copyright may affect the way data may be stored, shared and reused. You should ask yourself who the copyright holder of your datasets is, especially when you use existing data or when you collaborate with external parties. Using someone else’s research data SURF provides a brief guide to determining what consent is necessary to reuse someone else’s data (see “A brief guide … someone else’s data” in the resources below) Clarifying the ownership of your research data TODO: change accordingly for VIB Officially VIB, as your employer, is considered the rights holder to the research data you create. You, as a researcher, have the primary responsibility for taking care of the data. Questions on data exploitation may be even more important than those of ownership. Who can use the data? Who can publish it? Who can provide it to third parties? We strongly recommend that you deal with the issues around data exploitation at an early stage of your research project. Write down agreements between yourself, your supervisor, project members and other interested parties in your Data Management Plan. TODO: change accordingly RDM Support offers you a Guide to legal instruments and agreements for research data management (see the Guide ‘Legal instruments and agreements’) Confidential or privacy-sensitive data When your research project has received data under confidentiality or under legal privacy restrictions, you will have to identify and explain how you will deal with these restrictions in your data management plan (also see ‘Learning Unit: Handle - Data security’). Costs involved with managing your data TODO: https://www.uu.nl/en/research/research-data-management/guides/costs-of-data-management The costs of data management and sharing activities must be included into your research, in terms of time and resources needed. 1. Data Management Cost Guide When you plan your research you may not be able to oversee all costs involved. Nevertheless, it is useful to have an idea of possible costs at an early stage. You can use the Guide ‘Costs of Data Management’, which is a practical overview of possible costs per activity within each phase of the research process. Note: The Cost Guide offers cost indications and examples. These are not real prices. 2. Budget your data management costs You are advised to budget the data management costs as separate data management costs. These costs are eligible for funding with funders like NWO and the European Commission, as long as the costs are invoiced before the end of the project. 3. Planning can save time and money Planning an early start for certain activities within your research project can lower the costs for data management in the run of your project. You can save time by: Properly describing your data while collecting it, instead of doing it afterwards Choosing the right file format so that file conversion afterwards is not necessary Hiring an experienced data manager Spending time to think about data activities beforehand can help prevent unexpected extra efforts and costs later on in your research project. Check the current and expected costs for your research data You have just learned that in many parts of a research project there are data related costs. These costs depend on the type and volume of data you produce, analyse and store. TODO: link to file (calculation) https://lll-platform.uu.nl/pluginfile.php/4907/format_elevated/resource/0/Cost%20overview.docx Write your data management plan for your data collection Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module Data collection. You should be able to complete the following questions in the section Data collection: Will you use existing data? What data will you collect or create? How will the data be collected or created? How will you manage rights issues? What are the costs involved in managing and storing your data?"},{"title":"Basic Statistics Theory","url":"/topics/basic-statistics/tutorials/Introduction/tutorial.html","tags":[],"body":"Basic Statistics Theory This introductory video has been created during a livestream session in March 2020."},{"title":"Introduction to Data Management Plans","url":"/topics/data-management-plans/tutorials/Introduction/tutorial.html","tags":[],"body":"Why manage your research data? In this video Katarzyna Biernacka explains what data in a research context is. CC-BY-4.0: Katarzyna Biernacka, HU Berlin & Discipline Workshops 2019 Managing your data effectively is crucial to the success of your research. This doesn’t only apply to the immediate context of your thesis or publications. Managing your data is a practice that will benefit you throughout your research career. The following list gives an overview of what benefits are evident. Access, Re-use & Recognition Facilitating future research by allowing others to build on or add to your research data. Increased citations of research data and of publications based on that data. Efficiency Increasing your research efficiency by saving time and resources. Preventing duplication of effort by enabling others to use your data. Quality & Security Ensuring the integrity and reproducibility of your research. Ensuring that research data and records are accurate, complete, authentic and reliable. Enhancing data security and minimising the risk of data loss. Compliance Meeting legal obligations, restrictions and codes of conduct. Meeting the University policy for research data requirements. Meeting funding body grant requirements. Meeting publisher requirements for data access. A case to consider Marleen is an early career researcher. She completed her PhD about four years ago and is now a postdoctoral research fellow at a different university. Since she obtained her PhD, she has published a number of journal articles based on her doctoral research. Her papers have been cited widely in the literature of her field. But just recently a fellow researcher has questioned her findings. He has gone so far as to suggest that the data on which her research was based is inaccurate. One implication is that the data could even have been falsified. Marleen is confident that her research is valid and that her data is accurate. What steps could Marleen take to verify her research findings? What evidence would she need to demonstrate that she hasn’t falsified her data? Think about your own research. If someone accused you of research misconduct, would you be in a position to defend your research and reputation? List some strategies you could implement right now that would assist you, should you ever find yourself in Marleen’s situation. Data disasters – postcards from the edge The following are real examples where researchers or data centers have lost crucial data. Could any of these ever happen to you? With good planning you could avoid or reduce the impact of such occurrences. TODO: add H5P University policy framework for research data For the Flemish universities, it is important that all researchers honour scientific standards, including the meticulous and ethical treatment of research data. This policy is intended to set out parameters to safeguard the quality, availability and accessibility of research data within any Flemish university. It provides a basis for evaluating compliance with laws, regulations and codes of conduct. The policy also clarifies the various roles and responsibilities of university staff in managing research data. The highlights of the policy are: Archive (relevant and valuable) research data for a minimum of ten years; Store data in a structure that is suitable for long-term preservation and later consultation; Provide metadata to describe the data with sufficient clarity to ensure they are findable for further research; Make archived research data available for access and reuse at and outside VIB insofar as is reasonably possible; Each individual researcher / research leader is responsible to draw up a Data Management Plan (DMP) at the start of the research project and to follow up the agreements made in this plan; Scientific directors are responsible for the implementation and monitoring of the University policy framework and for drawing up additional faculty guidelines to this end if needed. Links to the Policy Frameworks of the Flemish Universities Policy Framework from Ghent University Policy Framework from KU Leuven Policy Framework from UHasselt Policy Framework from VUB Policy in Practise In this short video Prof. dr. Chantal Kemner explains the importance of good data management for Utrecht University. Chantal is full professor of Biological Developmental Psychology in Utrecht at the faculty of social sciences and since 2013 also at the UMCU. Funder requirements More and more research funders explicitly require you to consider the management and publication of your research data, both during and after your research project. The European Commission and the Flemish funders FWO have explicit policies on research data management. European Commission - Horizon 2020 The European Commission wants “Horizon 2020 beneficiaries to make their research data findable, accessible, interoperable and reusable (FAIR), to ensure it is soundly managed. Good research data management is not a goal in itself, but rather the key conduit leading to knowledge discovery and innovation, and to subsequent data and knowledge integration and reuse.” Horizon 2020 is the biggest research and innovation program of the European Commission. FWO FWO states that “FWO has made data management a key element of its policy for all support channels provided by the FWO. The FWO expects researchers to pay due attention to this dimension before, during and for at least five years after their research.” FWO Overview Data Management Plan Funder guidelines and templates Most funders require you to write a Data Management Plan. A DMP outlines all key aspects of collecting, storing and managing research data during and after a project. For this they provide you with guidelines, forms, templates and examples. For more information you can download the documents under Resources or check out the websites. You can also contact your faculty Research Support Office: EC – Horizon 2020: guidelines FWO template Writing a data management plan By now it should be clear that data needs to be properly managed throughout its lifecycle. The most effective way to do this is to create a Data Management Plan (DMP). This will take into account all the stages of the research data lifecycle. As outlined earlier, each individual researcher or research leader is responsible to draw up a data management plan. He or she should do this at the start of the research project. And during the research you should actively follow up on the agreements made in this plan. Think about our early career researcher Sasha (introduced in ‘Why manage your research materials and data?’) who needs to defend herself against accusations of researcher misconduct. As well as defending against misconduct accusations, some additional benefits of creating a data management plan include: Accessing your data more easily; Prioritising and balancing activities relating to research data collection and storage; Mitigating data loss; Reaching agreement between stakeholders about ownership of data; Reducing time and effort in the long term. The good news is that this online training will take you through the necessary steps to create a plan during the subsequent modules. Getting started with DMPonline We offer you DMPonline to create your Data Management Plan. DMPonline is an international online service that guides you in creating a DMP by answering a series of questions about your research project. It allows you to create, share, store, and revise your data management plans online. You will be asked to complete different sections of your DMP as we go through the other modules. As a result you will have written your own data management plan at the end of this course. With DMPonline you can: Write your plan and keep it up-to-date You can easily update your DMP throughout the lifecycle of a project Share plans online DMPonline allows collaborative access, so you can share your DMP with other researchers, within and outside of your university. Create multiple plans You can store different DMPs for different projects. And you can make a copy of a previous plan as the basis for writing a new one. Download plans You can download your DMP in a variety of formats. We recommend that graduate researchers share their data management plans with their supervisor(s). About RDM Support RDM Support provides all kinds of research data management assistance to researchers of VIB in all stages of their research. This can range from one-off individual advice to large-scale infrastructure coordination. You can find an overview of the contact details of the main host institutions for DMP related questions and guidance are as follows: AMS: Bart Cambré (bart.cambre@ams.ac.be) Hogere Zeevaartschool: Marc Vervoort (marc.vervoort@hzs.be) ITG: Ann Verlinden (averlinden@itg.be) KU Leuven: rdm@kuleuven.be UAntwerpen: RDM-support@uantwerpen.be UGent: Myriam Mertens and Annik Leyman (rdm.support@ugent.be) UHasselt: Sadia Vancauwenbergh (rdm@uhasselt.be) Vlerick: Eva Cools (eva.cools@vlerick.com) VUB: dmp@vub.be VIB: bits@vib.be"},{"title":"01 Installation and support","url":"/topics/eln/tutorials/installation/tutorial.html","tags":[],"body":"Installation Windows Requirements to install E-Notebook 2014: Microsoft Windows MS Office, Adobe Reader (or similar) ChemBioDraw (optional - see STEP 2) Valid VIB login credentials. Check your login and password on https://storefront.vib.be/. STEP 1: E-Notebook 2014 Browse to https://eln.vib.be/clickonce/ Click “Install” and open the file After the installation, the software is automatically launched and the login window appears Log in with your VIB credentials (see requirements) Close E-Notebook after successful launch: File - Exit or ‘X’ in the right upper corner Generate a shortcut on the desktop (right click - Send to - Desktop): All Programs - PerkinElmer - E-Notebook 2014 Client Install ChemBioDraw (STEP 2) STEP 2: ChemBioDraw Note: In case you only reinstall the ELN client, you don’t have to reinstall the ChemBioDraw component Download the ChemBioDraw installation file from the same website as E-Notebook 2014: https://eln.vib.be/clickonce Start the installation Install ChemBioDraw ActiveX component in suggested destination Follow the installation wizard instructions Click on “Install” and subsequently on “Finish” Why use ELN throught Citrix on Windows? Some older Windows versions cause problems with the E-Notebook 2014 Client installation. STEP 1: Citrix Workspace app Browse to [http://www.citrix.com www.citrix.com] Click on Download Select Citrix Workspace app from the list of possible downloads Download and install Citrix Workspace app STEP 2: Launch ELN online Browse to https://storefront.vib.be Login with your VIB credentials Launch the ELN application by clicking on the icon If your browser asks to download and open an .ica file, please agree Citrix Workspace will open en launch the application MacOS, Linux, mobile devices STEP 1: Citrix Workspace app Browse to [https://www.citrix.com www.citrix.com] Click on Download Select Citrix Workspace app from the list of possible downloads Download and install Citrix Workspace app After the installation on Linux execute the following command: sudo cp -a /usr/share/ca-certificates/mozilla/DigiCert_Assured_ID_ Root_ CA.crt /opt/Citrix/ICAClient/keystore/cacerts/ STEP 2: Launch ELN online Browse to https://storefront.vib.be Login with your VIB credentials Launch the ELN application by clicking on the icon If your browser asks to download and open an .ica file, please agree Citrix Workspace will open en launch the application Support Call us at +32 (0)9 248 16 15 Mail us at eln@vib.be"},{"title":"01 ELN Functionalities","url":"/topics/eln/tutorials/functionalities/tutorial.html","tags":[],"body":"Login When launching the application (Windows: double-click the E-notebook 2014 client icon – Citrix: click on the ELN 2014 icon and open the .ica file, Citrix Workspace will launch the application), you will see the following login window: In order to login on ELN, you need a valid VIB account. The VIB username usually has a format like: firstname lastname. More information on https://help.vib.be or mail eln@vib.be. When clicking on Connect the application will retrieve your data. The Work Offline option is only available with the client installation and will allow you to make adjustments to the data in your Offline folder. Note: when launching the application for the first time, a download of all collections will start, this usually takes 1 or 2 minutes. Layout The layout is resembling to Microsoft Office. It has 3 main parts; the ribbon with options on top, the navigation and history area on the left and the working area on the right. The default starting point is the Home location, this gives an overview of all data in the navigation area on the left and any modified experiments since one month on the right. In the Audit Trail (bottom left) you can find the history of the object selected above. This history allow you to access previous versions of an experiment and retrieve a file in order to bring it back to the present. Every version has a timestamp and operator (= user that pressed the save button). Previous versions of an experiment can**t be modified, only the last version is adjustable. Navigating to your colleagues or Home can be done with the orange icons in the upper left corner. Next to the navigation buttons you find the Save button. When saving you can add annotations as well. Ribbon The Ribbon is where you can find the options corresponding with your selection (navigation area or section). By default, there are three tabs: Home, View and Data. Sections have specific tabs in the ribbon, e.g. Document, Image, Text, Table, Property List, etc. An example can be found below (Text): Project, Notebook, Experiment There are 3 basic levels to organize your data: Project, Notebook and Experiment (see icons below). You can see them as folders with a certain hierarchy. Only an experiment contains files. To add one of the levels click on the icon in the Home tab in the ribbon. Sections An experiment consists of sections, every section is a file or page. To add a section, select the icon in the Home tab in the ribbon. Some sections are hidden behind the Other button. You can add sections automatically by drag and dropping them into your experiment. E-Notebook will recognize Word, Excel and PowerPoint files, PDF documents and images. GraphPad Prism files are not native to E-Notebook and will result in an Ancillary data section, this will happen with any other file type that is not native to the program. General Page Creating a new experiment will give you a blank experiment with only one section, by default this is the General page. This is an example of a General Page: Every lab group has a slightly different version of this General page. The universal parts of this section are the General Information and the Reference to experiment field. In the first field you have the option to enter general properties of your experiment such as start date, project, etc. Adding extra properties is available in the Property List tab in the ribbon. Adding a reference to your experiment can be very useful to link similar experiment to each other or make a series of experiments. This refence can be any experiment within your group. To add a reference, click on the option in the Home tab in the ribbon. As last there are 3 or 4 text boxes to add keywords, aim of experiment, results, specifications or a conclusion. Microsoft Office sections Three MS Office applications are supported in the E-Notebook software: Word, Excel and PowerPoint. All other MS Office files can be uploaded using the Ancillary Data section. For the supported application you can add files using the corresponding section. This will initially display a (print) preview of the file, double-clicking the preview will launch the MS Office application to make adjustments. All other options are displayed in the ribbon: Images Using the Image section in E-Notebook will allow you to import one (1) image file. All common image extensions are supported, camera brand specific files (e.g. RAW or DNG) can be uploaded using a non-file-specific section. Next to the image file itself you can add a title and notes. PDF files and Captured Image Using the PDF section in E-Notebook will allow you to import 1 PDF file. Next to the PDF file itself you can add a description, date and a document name. Ancillary Data (a.k.a. Binder) This non-file-specific section will save 1 file. In order to open the file , you must double-clicking on it, this will launch the according application outside ELN. Closing the external application again (e.g. after making adjustments) will result in this window: Click Continue to save your changes and re-upload the new file in ELN or click Cancel to ignore the changes. Supplementary Data Management (SDM) Files imported in this section will be saved on an internal network drive linked to ELN. This means that files in SDM wont be accessible outside of your research center or university network. Files in the SDM section are not limited to the file size limit of 30 MB. Next to the default list of sections, there are some lab-specific sections for PCR or Western Blot. To add one of these lab-specific sections, click on the **Other icon and select your section. Sharing data and linking experiments Access rights for others To grant a colleague access to your data, you simple select the object and click on the View tab in the ribbon. In the Properties field you click on Security. A new window will appear (left picture). The inherited privileges are default settings, you’re not able to modify this. The assigned privileges on the other hand can be modified by clicking ‘Grant’. By filtering on user group or user you can select the group/person (right picture). The type of privilege can be: read, read and write, full control. You can define this in the next window. Removing the privilege can de done by selecting the person or group and click on ‘Remove’. For both granting or removing access privileges there is no notification system, you have to tell them yourself. Experiment shortcuts When a colleague granted you access to a project/notebook/experiment you can place a link to this object in your own ELN. This makes navigating to this object easier and allows you to group all your collaborations within your own ELN hierarchy. To create such a shortcut, follow these steps: Select the object of interest Right click – Copy Navigate to your own ELN Right-click on the location you want the link to appear Select Paste Reference Note: shortcuts can be removed, the original data however is not deleted. Templates Templates can be created by every user and can be shared with your colleagues. To create a template, follow this procedure: navigate to ‘User Configuration’ – ‘Templates’ create new experiment build your new default experiment/template by adding information/sections save your template Next time you want to create a new experiment, you will have the option to create a blank or template experiment. Search The collection search can be used for users, projects, notebooks and experiments. No content can be found with the search box in the upper right corner. The Advanced Search option can find experiment content. You can find it in ‘Quick Links’ above the navigation pane."},{"title":"Handle: Data storage","url":"/topics/data-management-plans/tutorials/handle-store/tutorial.html","tags":[],"body":"Introduction Click to expand! *Heading* 1. A 2. list * With some * Sub bullets Figure 1: Seqselector.png TODO: specific chapter on storage Write your data management plan for your data storage Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module on data storage. You should be able to complete the following questions in the section ‘Data documentation’: Where will you store your data? How will the data be backed up? After finishing this part in DMPonline, please return to the learning environment and click on [Complete]. This takes you back to the course overview. Continue with the next learning unit. You can ask your faculty or project data manager or RDM Support for a review of your DMP once you have finished writing all or parts of your DMP."},{"title":"Round up","url":"/topics/data-management-plans/tutorials/roundup/tutorial.html","tags":[],"body":"Introduction Click to expand! *Heading* 1. A 2. list * With some * Sub bullets Figure 1: Seqselector.png Introduction to rounding up You have almost reached the end of this course on research data management. You have learned about data collection, data documentation, data storage and security, selection and preservation and making data available for reuse. We are very curious to know if this course has helped you write your Data Management Plan (DMP). To round up: We want to remind you of the DMP review service of RDM Support; We want to share some good practices of data management with you; We invite you to fill out the evaluation of this online training. This will help us to further develop this training and future learners can benefit from this. Thank you very much! DMP review service You can have your data management plan (DMP) checked by the specialists of Research Data Management Support. You can get in touch if you are unsure about sections in your DMP or when you doubt whether your plan fits the requirements of your research funder. When you are in the process of writing a proposal for a research funder and you want a check on the data section, you can also contact the Research Support Office (RSO) of your faculty. Researchers sharing their experiences TODO: add stories if available or links to resources TODO: merge with experiences More data stories Challenges in irreproducible research special issue in Nature, 7 Oct 2015 There is growing alarm about results that cannot be reproduced. Explanations include increased levels of scrutiny, complexity of experiments and statistics, and pressures on researchers. Journals, scientists, institutions and funders all have a part in tackling reproducibility. Nature has taken substantive steps to improve the transparency and robustness in what they publish, and to promote awareness within the scientific community. Data stories in environmental science collected by DataONE Success stories and cautionary tales from researchers related to their experiences with managing and sharing scientific research data as collected by DataONE. Advantages of data sharing by John-Alan Pascoe of Delft University of Technology John-Alan Pascoe, researcher at the Faculty of Aerospace Engineering at Delft University of Technology, explains the advantages he experienced after sharing his raw and derived data in the data archive of 4TU.ResearchData. Evaluation of training TODO: link to questionnaire"},{"title":"Handle: Data Security","url":"/topics/data-management-plans/tutorials/handle-security/tutorial.html","tags":[],"body":"Introduction to data security By now you know more about how to manage your data collection, how to organise and document your research data and where and how to store your data. Now we will take you into the world of keeping data safe and secure. Loss of data, loss of academic career The loss of scientific data can have a devastating impact on careers. Imagine that you loose all of the research data you’ve been diligently collecting for four years. Now imagine the knock-on effect: you won’t get the PhD you’ve been working towards, affecting your future career. This nightmare happened to Billy Hinchen, a biologist at Cambridge University. Listen to his story. Data breaches There are several examples of (mainly online) data storage going wrong, leading to leaks of sensitive and personal information. The picture below shows the biggest cases of data breaches in the past 10 years. They involve some well-known, highly regarded and trusted companies as well as some practices from the academic world. Read about the story Figure 1: Biggest data breaches Prevent unauthorised access Data security may be needed to protect intellectual property rights, commercial interests, or to keep personal or sensitive information safe. Data security involves security of data files, computer system security and physical data security. All three need to be considered to ensure the security of your data files and to prevent unauthorised access, changes, disclosure or even destruction. Data security arrangements need to be proportionate to the nature of the data and the risks involved. Attention to security is also needed when data are to be destroyed. If data destruction is in order, you need to make sure that the destruction process is irreversible. Learn about different measures depending on the kind of security you need. Security of data files The information in data files can be protected by: Controlling access to restricted materials with encryption. By coding your data, your files will become unreadable to anyone who does not have the correct encryption key. You may code an individual file, but also (part of) a hard disk or USB stick Procedural arrangements like imposing non-disclosure agreements for managers or users of confidential data Not sending personal or confidential data via email or through File Transfer Protocol (FTP), but rather by transmitting it as encrypted data e.g. FileSender Destroying data in a consistent and reliable manner when needed Authorisation and authentication: for personal data you have to give very selective access rights to specified individuals. Computer security systems The computer you use to consult, process and store your data, must be secured: Use a firewall Install anti-virus software Install updates for your operating system and software Only use secured wireless networks Use passwords and do not share them with anyone. Do not use passwords on your UU computer only, but also on your laptop or home computer. If necessary, secure individual files with a password. Encrypt your devices (laptop, smartphone, USB stick/disk). Physical data security With a number of simple measures, you can ensure the physical security of your research data: Lock your computer when leaving it for just a moment (Windows key + L) Lock your door if you are not in your room Keep an eye on your laptop Transport your USB stick or external hard disk in such a way that you cannot lose it Keep non-digital material which should not be seen by others, in a locked cupboard or drawer. Data classification TODO: what to do with classified data Data that contain personal information These data should be treated with higher levels of security than data which do not. You will learn more about privacy-sensitive data in the e-module. What is your experience with unauthorised access to your research data? TODO: implementation form widget We are interested to know if you have ever experienced unauthorized access to any of your research data. When you give your reply, we will show you an overview with the responses of other researchers in this course. All responses will be processed anonymously. [(1)] No, I am sure about that [(2)] Not that I am aware of [(3)] Yes, without much consequences [(0)] Yes, with severe consequences Legal agreements and contracts Often other people are required to handle your data, or you might be the person that handles other people’s data. To arrange the security of the research data you work with, in many cases you have to make a (legal) agreement with other people involved. These agreements will make explicit permitted uses, retention time, and agreed upon security measures. Find out what legal contracts you can use by studying the figure below. TODO: Visit the Guide ‘Legal instruments and agreements’ for more information For tailored advice and templates, contact Legal Affairs via your faculty Research Support Officer (RSO) TODO: add link Figure 2: Agreements types for data When to use which legal contract? You have been acquainted with the different flavors of legal agreements. Is it clear to you when you need which agreement? Please answer the following questions by choosing the right kind of agreement. TODO: add quiz or H5P quiz Privacy-sensitive data Figure 3: Personal data - learning objectives Privacy in a nutshell Privacy is a fundamental right. With regards to privacy, we all have two perspectives: How is your privacy protected? How can we, as a researcher, protect the privacy of the people involved in our research (the data subjects)? TODO: add link to document and image screenshot Figure 4: Privacy reference card Six principles from the European General Data Protection Regulation 1/2 The European General Data Protection Regulation (GDPR) outlines how we should work with privacy-sensitive data. TODO: create working infographics with images see http://gdprcoalition.ie/infographics Six principles from the European General Data Protection Regulation 2/2 According to the GDPR processing of personal data must be done according to 6 principles. TODO: create HP5 document The GDPR outlines six data protection principles you must comply with when processing personal data. These principles relate to: Lawfulness, fairness and transparency - you must process personal data lawfully, fairly and in a transparent manner in relation to the data subject. Purpose limitation - you must only collect personal data for a specific, explicit and legitimate purpose. You must clearly state what this purpose is, and only collect data for as long as necessary to complete that purpose. Data minimisation - you must ensure that personal data you process is adequate, relevant and limited to what is necessary in relation to your processing purpose. Accuracy - you must take every reasonable step to update or remove data that is inaccurate or incomplete. Individuals have the right to request that you erase or rectify erroneous data that relates to them, and you must do so within a month. Storage limitation - You must delete personal data when you no longer need it. The timescales in most cases aren’t set. They will depend on your business’ circumstances and the reasons why you collect this data. Integrity and confidentiality - You must keep personal data safe and protected against unauthorised or unlawful processing and against accidental loss, destruction or damage, using appropriate technical or organisational measures. Privacy by design To comply with the six principles from the GDPR, you can implement privacy by design. This means that you design a data management plan with measures on both IT and procedural level. Which data breach is breached? Can you recognise the principles that are breached in the different ways personal data is processed? TODO: H5P quiz 7 cases Storing personal data 1/2 Figure 5: Storing personal data Storing personal data 2/2 Only if the access can be unambiguously be restricted to authorised persons, can data be stored without such measures. Should you want an elaborate visualisation of what is considered identifiable data, check out the information sheet at the Future Privacy Forum. Download the visual guide to practical data de-identification Can you recognize identifiable data? question Can you recognize identifiable data? a collection of GPS data of daily routines a list of households sizes associated with number of pets MRI scans without identifying metadata. audio recordings with no metadata and no names of the recorded persons transcripts of interviews without any directly identifying information a list of gender and grades for a de-identified course Check the answers. Answer 1,3, and 4 are correct! GPS data holds information on where people go. In a daily routine, the track ends at a particular location which is likely the home of the subject. AN MRI scan from the profile of the head can be identifiable. Audio recordings can be identifiable from the tone of the voice. A list of surnames in itself is not identifying nor personal information. Access to privacy-sensitive data If and how you can make personal data available, depends n the level of sensitivity of your data. The more sensitive, the more restrictions and safeguards need to be put in place to make sure the data does not fall into the hands of unauthorised persons both during and after research. To determine where the privacy risks lie for your data you will have to do a Data Privacy Impact Assessment (DPIA). For more information: TODO: link to: https://www.uu.nl/en/research/research-data-management/guides/handling-personal-data Towards the data subjects, you need to be transparent regarding the possible reuse, or retaining of the data for verification requirements, and get their prior consent. Cases on how to make personal data accessible Case 1: YOUth cohort study YOUTH COHORT STUDY YOUth (Youth Of Utrecht) is a large-scale, longitudinal cohort following children in their development from pregnancy until early adulthood. A total of 6,000 babies and children from Utrecht and its surrounding areas will be included in two different age groups and followed at regular intervals. The YOUth data enables researchers to look for answers to all sorts of scientific questions on child development. A few examples of YOUth data: human bodily material, hours of videos, MRI images, questionnaires, ultrasounds and IQ scores. YOUth encourages and facilitates data sharing. It is one of the leading human cohorts in FAIR and open data in the Netherlands. More information at: https://www.uu.nl/en/research/youth-cohort-study Case 2: TODO: other example from Wings? An introduction to informed consent In the module ‘Legal agreements and contracts’ you learned about informed consent. Informed consent is very important when working with data which is in any way related to people. TODO: add graphics on informed consent One thing to arrange in your informed consent is the possibility for future use, for verification or reuse. In your informed consent, it is important to be clear on future use of data. Informed consent for data sharing One thing to arrange and to be crystal clear about in your informed consent is the possibility for future use of your data, for verification or reuse. question Question Check the sentences that do permit data sharing if used as a single statement. Any personal information that reasonably could identify you will be removed or changed before files are shared with other researchers or results are made public. Other genuine researchers (may) have acces to tis data only if they agree to preserve the confidentiality on the information as requested in this form. Any data that could identify you will be accessible only to the researchers responsible for performing this study. All personally identifying information collected about you will be destroyed after the study. Check the answers. Answer 1 and 2 are both correct! Sharing of research data that relates to people can often be achieved using a combination of obtaining consent, anonymizing data and regulating data access. If the statement towards the data only mentions the current study, sharing is not explicitly possible. You should add some sentence to make it clear to participants that the data could be used for further research, deidentified where possible, or identifiable with enough safeguards and security measures, if it is not. Write your data management plan for your data security Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module on data security. You should be able to complete the following questions in the section ‘Data security’: Will you use or collect any confidential or privacy-sensitive data? What measures will you take to ensure the security of any confidential or privacy-sensitive data? What measures will you take to comply with security requirements and mitigate risks? To whom will access be granted/restricted?"},{"title":"Prepare: Data documentation","url":"/topics/data-management-plans/tutorials/prepare-document/tutorial.html","tags":[],"body":"Introduction to documentation By now you understand how to describe your data collection in terms of, for example, type, size, and format. You have identified this for your own research data. Now we will look into the documentation and metadata which will accompany your data. Documentation and metadata are essential to understand what a dataset means and to make it reusable in the future. Figure 1: Why document your data: learning objectives Tips for data documentation - John MacInnes, professor of Sociology of the University of Edinburgh, explains why it is necessary to document each step of your research and how this will benefit you in the long term. Examples of data documentation Since there is a wide variety of types of data and types of research, there are many different ways of documenting data. A few examples of data documentation are: Laboratory notebooks and experimental procedures Questionnaires, codebooks, data dictionaries Software syntax and outout files; Information about equipment settings & instrument calibrations Database schemes Methodology reports Provenance information about sources of derived or digitised data question Question What data documentation will you use and why? Feedback on your reflections Data documentation has as goal to be used by people to understand the dataset. Such as specific conditions in which it was collected, what each column means and which methods were used to collect the data. When creating documentation, you need to ask yourself, can others (or I, myself) understand my dataset if I give them this information. There are many different ways to set up and organise your documentation. Project level Project level documentation documents what the study sets out to do; how it contributes to new knowledge in the field, what research questions/hypotheses are, what methodologies are used, what samples are used, what intruments and measures are used, etc. A complete academic thesis normally contains this information in details, but a published article may not. If a dataset is shared, a detailed technical report needs to be included for the user to understand how the data were collected and processed. You should also provide a sample bibliographic citation to indicate how you would like secondary users of your data to cite it in any publication. File or database level File or database level documentation documents how all the files (or tables in a database) that make up the dataset relate to each other, what format they are in, whether they supersede or are superseded by previous files, etc. A readme.txt file is the classic way of accounting for all the files and folders in a project. Variable or item level Variable or item level documentation documents how an object of analysis came about. For example, it does not just document a variable name at the top of a spreadsheet file, but also the full label explaining the meaning of that variable in terms of how it was operationalised. John MacInnes, professor of Sociology of the University of Edinburgh, speaks about how data documentation can help to find a way in often voluminous data collections of different copies, routings, syntaxes, samplings, etc. On the necessity of data documentation in secondary data analysis question Question Looking back at your previous research project: Did you ever have problems reusing other people’s data because of lack of documentation? Never tried Successfully reused Had to ask clarification Had to abandon the reuse attempt Feedback on your reflections Data documentation always provides advantages for yourself and for others such as better understandability, sharability and reusability in the future. Figure 2: Laboratory Notebooks for documentation Thorough and effective management of laboratory data and the routine documentation of all lab procedures is a highly important responsibility for all researchers. If you want to learn more about the electronic lab notebook system at VIB, please see these tutorials An introduction to metadata Watch this web lecture to learn about the different types of metadata and how metadata can help make your research data better findable. You are pointed to useful sources for metadata standards. identify different types of metadata TODO: HP5 quiz or matrix quiz Metadata for different disciplines Different disciplines like biology, earth sciences, physical sciences and social sciences and humanities have their own standards. By choosing a well-supported standard, you will maximise the chance that your data can be re)used and understood by other researchers. Metadata for different disciplines Useful links to metadata standards: Biology General Sciences A community-maintained directory of metadata schemas which has been set up under the auspices of the Research Data Alliance. A list of metadata standards and other standards developed by FairSharing. Controlled vocabulary Improve a record description question Question Take a look at the record descriptions n the table below and answer the question below and in the following pages. Soil Sample Condition Length Classx A1 low $458 III A2 low $391 II A3 medium $422 IV x according to the classification from last experiment Is the value of in the Soil sample column clear? Click your answers! Yes, it is sufficient to say this is a sample. The identifier for the sample needs to be unique, the content of the sample comes from the other metadata fields and their values. question Question Take a look at the record descriptions n the table below and answer the question below and in the following pages. Soil Sample Condition Length Classx A1 low $458 III A2 low $391 II A3 medium $422 IV x according to the classification from last experiment Is the value in the COndition column clear? Click your answers! No! It is not clear what low or medium as condition means. question Question Take a look at the record descriptions n the table below and answer the question below and in the following pages. Soil Sample Condition Length Classx A1 low $458 III A2 low $391 II A3 medium $422 IV x according to the classification from last experiment Is the value in the Length column clear? Click your answers! No, it is not clear what is meant by length. Also a unit for the values is missing. Is it meters, centimeters, or seconds? question Question Take a look at the record descriptions n the table below and answer the question below and in the following pages. Soil Sample Condition Length Classx A1 low $458 III A2 low $391 II A3 medium $422 IV x according to the classification from last experiment Is the value in the Class column clear? Click your answers! No! There is a reference that the classes are explained somewhere. But no link to the document is given. Data standards explained Your dataset can be standardised in various aspects. Standardisation, in general, makes data comparable and interpretable. In other words, your data becomes interoperable by applying standards. Datasets can be combined, compared or are simply easier to reuse. You have to plan standardisation, as it is for many aspects hard or impossible to apply afterwards. Standardise as much as possible between you and your collaborators or research group. If there are standards established and used in your field of research you are advised to use these. Here is a list of things you can standardise in your research. Standardise how, what and when you measure things by standardising your protocol, or methods and materials For instance, is there a standard set of questions for ‘quality of life’? Is there a standard procedure to house mice for your purpose? What aspects do you measure? At what parameter values (age, concentration, etc.)? When do you measure (every two hours, every gram of weight gain, etc.)? Standardise your file formats so you can easily exchange results without technical difficulties. Check for standard taxonomies or coding systems within your research discipline. Standardise the units in which you note down your results. For instance, do you use mm, cm, m? It is extra work to transform units between experiments. Standardise the metadata you use to describe your records or study. What fields will fill in by default, and according to what standard do you define the fields’ names? Will you design a metadata spreadsheet where you specify all things that you will note down? Standardise the vocabulary you use. If everyone has the same terminology, it can avoid confusion or misinterpretation. Check for standard taxonomies or coding systems within your research discipline. Check your knowledge on standards Follow the links below for examples of standards. What type of standardisation do the links refer to? Demographic market research Find via Google: “general morphology score (GMS)” Marine Geoscience Data International Union of crystallography The Cultural Objects Name Authority) SI Units UK data service TODO: add H5P exercise Folder structure and file naming Figure 3: Folder structure - learning objectives CC BY: https://mantra.edina.ac.uk/ Figure 4: Introduction to good file management Trying to find a data file that you need which has been stored or named incorrectly or inaccurately can be both frustrating and a waste of valuable time. In this short video Jeff Haywood, professor at the University of Edinburg, explains his experiences with good and bad file management. Project level Project level documentation documents what the study sets out to do; how it contributes to new knowledge in the field, what research questions/hypotheses are, what methodologies are used, what samples are used, what intruments and measures are used, etc. A complete academic thesis normally contains this information in details, but a published article may not. If a dataset is shared, a detailed technical report needs to be included for the user to understand how the data were collected and processed. You should also provide a sample bibliographic citation to indicate how you would like secondary users of your data to cite it in any publication. File or database level File or database level documentation documents how all the files (or tables in a database) that make up the dataset relate to each other, what format they are in, whether they supersede or are superseded by previous files, etc. A readme.txt file is the classic way of accounting for all the files and folders in a project. Variable or item level Variable or item level documentation documents how an object of analysis came about. For example, it does not just document a variable name at the top of a spreadsheet file, but also the full label explaining the meaning of that variable in terms of how it was operationalised. question Choose the best chronological file name Which of the file names below is the most appropriate? 2019-03-24_Attachment 24 March 2006 Attachment 240306attach Click your answers! 2019-03-24_Attachment is correct! Using a date in the format Year-Month-Day will maintain the chronological order of your files. question Choose the best descriptive file name Which of the file names below is the most appropriate? labtox_recent_110810_old_version.sps 2010-08-11_bioasssay_tox_V1.sps FFTX_3776438656.sps Click your answers! 2010-08-11_bioasssay_tox_V1.sps is correct! Keep the file names short and relevant while using sufficient characters to capture information. Do not name files recent or final or definitive_final, a date or version number will suffice. Figure 5: Batch renaming Figure 6: Suggestions for version control How would you treat your data question Choose the best descriptive file name Why should you discard or delete obsolete versions of data? The most current version is the only relevant version. You have several versions of files in a state between versions You are exceeding the storage space available to you. Click your answers! Correct answer: You have several versions of files in a state between versions! Too many similar or related files may be confusing to yourself and to anyone else wanting to access or use your data. You may think that you know which data file is which but that may not always be the case as time passes and the number of different versions increases. It is easier to maintain a manageable number of versions with a clear naming structure. As long as the original raw or definitive copy is retained and processing is well documented, the intermediate working files can and should be discarded. Fill the blanks TODO: add H5P Write your data management plan for your data documentation Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module Data documentation. You should be able to complete the following questions in the section Data documentation: How will you structure your data? How will the data be described and documented? What standards will you use?"},{"title":"Overview of other training resources collected by UGent RDM team","url":"/topics/data-management-plans/tutorials/other-material/tutorial.html","tags":[],"body":"Useful information & training resources on Research Data Management UGent RDM webpages in Dutch UGent RDM webpages in English Australian National Data Service (esp. “23 (research data) things”) Coursera Mooc “Research Data Management and Sharing” Data Management Training Clearinghouse (registry of RDM learning resources) DataOne (esp. education modules) Digital Curation Centre (esp. How-to Guides & Checklists) Essentials for Data Support EUDAT (esp. training materials) FOSTER training portal MANTRA – Research Data Management Training OpenAIRE webinars RDM open training materials on Zenodo UK Data Service (esp. “Prepare & Manage Data pages) UK Data Service webinars FAIRDOM Knowledge Hub Data4LifeSciences Handbook for Adquate Natural Data Stewardship"},{"title":"05 Reading and writing data","url":"/topics/R/tutorials/Reading_Writing/tutorial.html","tags":[],"body":"Reading and writing files Reading files Entering data in R can be done by typing the values when you create a variable. In most cases, however, you will have a file with data that was created by an instrument in your lab. How to import such a file into R? There is a manual available in the R documentation called R Data Import/Export. It’s accessible using help.start() and covers in detail the functionality R has to import and export data. Reading this is highly recommended. This manual covers importing data from spreadsheets, text files, and networks. Reading text files Most instruments put out data in text format: tab-delimited text (.txt) or comma-separated value files (.csv). Both can be easily opened in R. The most convenient method to import data into R is to use the read functions, like read.table(). These functions can read data in a text file. In Notepad you can save such a file as a regular text file (extension .txt). Many spreadsheet programs can save data in this format. Reading means opening the file and storing its content into a data frame. read.table(file,header=FALSE,sep=\"\",dec=?.?,skip=0,comment.char=\"#\") This function has a long list of arguments, the most important ones are: file: path on your computer to the file e.g. D:/trainingen/Hormone.csv If it is stored in the working directory, you can simply use its name. You can also use file=file.choose() to browse to the file and select it. File can be replaced by a url to load a file with data from the internet. header: does the first line of the file contain column names? dec: symbol used as decimal separator sep symbol used as column separator, default is a whitespace or tab skip: number of lines to skip in the file before starting to read data comment.char: symbol to define lines that must be ignored during reading See the documentation for an overview of all the arguments. The output of every read function is a data frame. There are functions to read specific file formats like .csv or tab-delimited .txt files. In the documentation of read.table() you see that these functions are called read.csv() and read.delim(). Both functions call read.table(), but with a bunch of arguments already set. Specifically they set up sep to be a tab or a comma, and they set header=TRUE. read.delim(file,header=TRUE,sep=\"\\t\") On the documentation page, you see that these functions each have two variants that have different default settings for the arguments they take: read.csv( file,header=TRUE,sep= \",\",dec=\".\", ...) read.csv2( file,header=TRUE,sep= \";\",dec=\",\", ...) read.delim( file,header=TRUE,sep=\"\\t\",dec=\".\", ...) read.delim2(file,header=TRUE,sep=\"\\t\",dec=\",\", ...) Originally the CSV format was designed to hold data values separated by commas. In .csv files that are made on American computers this is the case. However, in Europe the comma was already used as a decimal separator. This is why .csv files that are made on a European computer use the semicolon as a separator. For instance, the file below contains a header row and three columns, separated by semicolons. It uses the comma as decimal separator. Patient;Drug;Hormone 1;A;58,6 2;A;57,1 3;B;40,6 Obviously, the file is a European CSV file, to open it use read.csv2() Reading Excel files To import Excel files via a command the easiest way is to let Excel save the file in .csv or tab delimited text format and use the read functions. An easy way to import Excel files is to use the RStudio interface although I prefer to use commands. To use the interface go to the Environment tab and click the Import Dataset button. RStudio can import 3 categories of files: text files, Excel files and files generated by other statistical software. To read .xls or .xlsx files select From Excel. A dialog opens with options on the import. You can import data from your computer (Browse) or from the internet (provide a url and click Update). Click Browse, locate the Excel file and click Open. The Data Preview section shows what the data will look like in R. The Import Options section allows you to specify the import parameters. Name: name of the data frame that will hold the imported data. The default is the name of the file that you are opening. Skip: number of rows at the top of the file to skip during import. Some data formats contain a number of header rows with general info like parameter settings, sample names etc. These rows are followed by the actual data. Skip allows you to skip over the header rows and import the actual data. If the first row of the file contains column names, select First Row as Names Open data viewer shows the data in the script editor upon import Click Import. Behind the scenes RStudio uses the readxl package that comes with the tidyverse package. You can also use the functions of this package directly in commands. Compared to other packages for reading Excel files (gdata, xlsx, xlsReadWrite) readxl has no external dependencies, so it?s easy to install and use on all operating systems. It supports the .xls format and the .xlsx format. The easiest way to install it from CRAN is to install the whole tidyverse package but you have to load readxl explicitly, since it is not a core tidyverse package. Once imported into RStudio the data is stored in a data frame and you can use it as input of commands. The data frame appears in the list of Data in the Environment tab. Figure 1: Inspect Variables and Data Frames in the Environment tab If you want to view the data frame you can click its name in the Environment tab and it will appear in a separate tab in the script editor. Figure 2: View file content hands_on Hands-on: Demo From the demo script run the Reading files section hands_on Hands-on: Exercise 17a Import the file GeneEx.csv into a data frame called GeneEx Rename the two last columns Ct1 and Ct2 Create a new column containing the average Ct: (Ct1+Ct2)/2 solution Solution GeneEx 0) and a table of downregulated genes and store them in data frames called up and down. How many up- and downregulated genes are there? What is the gene with the highest log2 fold change? What is the data of the gene with the lowest adjusted p-value (= padj)? Write the Ensembl IDs (= row names) of the upregulated genes to a file called up.txt. You will use this file for functional enrichment analysis using online tools like ToppGene,EnrichR? These tools want a file with only Ensembl IDs as input (one per line, no double quotes, no column headers, no row names). solution Solution DE 0,] down 0) question Question What’s the difference between these 2 commands ? which.max(up$log2FoldChange) max(up$log2FoldChange) question Question Will this command write Ensembl IDs and log fold changes ? toprint <- as.data.frame(up$log2FoldChange) write.table(toprint,file=\"up.txt\",quote=FALSE,col.names=FALSE) hands_on Hands-on: Extra exercise 17c Which type of files are imported by read.delim ? solution Solution Check the documentation and look at the default for sep hands_on Hands-on: Extra exercise 17d Read the file ALLphenoData.tsv into a variable called pdata using one of the read functions What type of data structure is pdata ? What are the names of the columns of pdata ? How many rows and columns are in pdata ? solution Solution pdata <- read.delim(\"Rdata/ALLphenoData.tsv\") class(pdata) colnames(pdata) dim(pdata)"},{"title":"Share: Data availability for reuse","url":"/topics/data-management-plans/tutorials/share-reuse/tutorial.html","tags":[],"body":"Introduction to data availability for reuse Thanks to information and communication technology and globalisation new opportunities arise to exchange results of scientific research - publications and research data - and even of scientific methods and practices. This new way of practising science is called ‘open science’. Open data is a part of this movement towards open science. It is the ambition of universities, governments, funders and publishers to make research data optimally suited for reuse. There are different reasons why you may not be able to share your research data. Thinking about these issues and challenges when developing your data management plan will help you reflect on such reasons in an early stage. How frustrating a data request can be Not being prepared to share your data can lead to problems in using the data. In this short video, you see what shouldn’t happen when a researcher makes a data sharing request! Topics include storage, documentation, and file formats. A made up, yet not unrealistic story. Introduction to data repositories In order to preserve, manage, and provide access to your research data, you can deposit your data in a data repository. Data repositories allow permanent access to datasets in a trustworthy environment and enable search, discovery, and reuse of the data they host. Click on the topics below to find out more about data repositories. TODO: add repositories from Elixir A wide variety There is a wide variety of data repositories. Most have the option to publish your dataset using a persistent identifier and some provide the service of long-term preservation. Some repositories host data from various disciplines and others are domain- or discipline specific. Choosing a data repository When choosing a repository for your data be sure to check if the repository meets your criteria or the criteria set by your funder or journal editors. Criteria to select a certain repository can be: Is the repository certified with a CoreTrustSeal or Data Seal of Approval? Repositories with a Data Seal of Approval are recognised in the community as a trustworthy source of data. Is long term archiving guaranteed or not? Some repositories will guarantee the legibility of the data, even if the hardware and software become obsolete. What are the costs per dataset or gigabyte? Repositories differ in their cost model, some allow free deposits up to a certain amount of storage What is the physical storage location of data? The location of your data determines under which data protection law it falls. Some repositories store data in the US and others in the EU. What is the default license? Some repositories allow for open or restricted access, or you can specify which license for use you want for your data. You can use this repository selection tool to help you select a suitable repository. Registry of research data repositories You can browse or search for a data repository in re3data.org. This is a global registry of research data repositories covering different academic disciplines. You can search or browse by subject, content type or country. You can filter the search and browse results on criteria for choosing a data repository as described above. https://www.re3data.org/ Some well-known and more generic repositories Zenodo – a repository that enables researchers, scientists, EU projects and institutions to share and showcase multidisciplinary research results (data and publications) that are not part of the existing institutional or subject-based repositories of the research communities; Dryad – a curated general-purpose repository that makes the data underlying scientific publications discoverable, freely reusable and citable. Dryad has integrated data submission for a growing list of journals; Open Science Framework (OSF) - a scholarly commons to connect the entire research cycle. It is part network of research materials, part version control system, and part collaboration software; Figshare – a repository that allows researchers to publish all of their research outputs in an easily citable, sharable and discoverable manner. Explore data repositories You have just learned about the existence of a global registry of research data repositories that covers repositories from different academic disciplines. Re3data.org makes it possible to search for a repository that meets your criteria. Go to www.re3data.org/search and find a repository that meets all three of the following criteria: Certificate → CoreTrustSeal Data licenses → CC0 (Creative Commons 0) Persistent identifier (PID systems) → DOI (Digital Object Identifier) Make use of the filters offered on the left side of the screen, as visualized here: TODO: quiz with ELIXIR resources Give clarity with (Creative Commons) licenses In order to publish your data and make it reusable, you require a license. A license creates clarity and certainty for potential users of your data. A license is not an option for all data; some of it may be too confidential or privacy-sensitive to be published. Creative Commons licenses Licenses such as the Creative Commons (CC) licenses replace ‘all rights reserved’ copyright with ‘some rights reserved’. There are seven standard CC licenses. CC-BY is the most commonly used license, in which attribution is mandatory when using data. You can also choose restrictions like non-commercial, no derivatives, or share alike. Creative Commons offers a guide to help you determine your preferred license. Figure 1: Creative Commons Assigning a license to your data Assigning licenses to data can also have disadvantages. Licenses are static and do not change with the quick developments in the field of research data. Therefore, some data repositories work with a CC0 license whereby no rights are reserved. Instructions regarding use are completed with codes of conduct, which may be adapted more easily. A short movie explaining the different Creative Commons elements is shown below. Remember that sharing without a license can still lead to conflicts. TODO: add video on CC licenses? Question We are very interested to know what license you would choose if you were to share the underlying research data of your most recent publication. An explanation for each license can be found by clicking on the links below. CC BY: Attribution CC BY-SA: Attribution ShareAlike CC BY-ND: Attribution-NoDerivs CC BY-NC: Attribution-NonCommercial CC BY-NC-SA: Attribution-NonCommercial-ShareAlike CC BY-NC-ND: Attribution-NonCommercial-NoDerivs CC0: Public Domain Publishing in a data journal Data journals are publications whose primary purpose is to publish datasets. They enable you as an author to focus on the data itself, rather than producing an extensive analysis of the data which occurs in the traditional journal model. Fundamentally, data journals seek to: Promote scientific accreditation and reuse; Improve transparency of scientific methods and results; Support good data management practices; Provide an accessible and permanent route to the dataset. The benefits of publishing in a data journal Publishing in a data journal may be of interest to researchers and data producers for whom data is a primary research output. In some cases, the publication cycle may be quicker than that of traditional journals, and where there is a requirement to deposit data in an “approved repository”, long-term curation and access to the data is assured. Publishing a data paper may be regarded as best practice in data management as it: Includes an element of peer review of the dataset; Maximises opportunities for reuse of the dataset; Provides academic accreditation for data scientists as well as for front-line researchers. (source: ANDS Guide) General and disciplinary data journals There are data journals for various disciplines and also more general data journals exist. A widespread standard PID is the DOI. DOI stands for ‘Digital Object Identifier’. A DOI is an alphanumeric string assigned to an object which allows for an object to be identified over time. Often a DOI will be presented as a link which looks like: https://doi.org/10.1109/5.771073. There are other identifiers available which some repositories may use instead. If you are depositing in a reputable repository then you should be given some type of persistent identifier which you can use to cite and link to your data. Examples of generic data journals: Scientific Data Data in Brief Data Science Journal Examples of disciplinary data journals: TODO: check for life science additions Open archaeology data; Earth System Science Data; Research Data Journal for the Humanities and Social Sciences. How to cite a dataset Citations to your data can add to your academic impact. A citation should include enough information so that the exact version of the data being cited can be located. Including a Persistent Identifier (PID) in the citation ensures that even if the location of the data changes, the PID will always link to the data that were used. You can indicate in your (Creative Commons) license or user agreement that you want your data cited when reused. Data citations work just like book or journal article citations and can include the following information: Author; Year; Dataset title; Repository; Version; Persistent IDentifier (PID), often works as a functional link/URL. Examples A widespread standard PID is the DOI. DOI stands for ‘Digital Object Identifier’. A DOI is an alphanumeric string assigned to an object which allows for an object to be identified over time. Often a DOI will be presented as a link which looks like: https://doi.org/10.1109/5.771073. There are other identifiers available which some repositories may use instead. If you are depositing in a reputable repository then you should be given some type of persistent identifier which you can use to cite and link to your data. Irino, T; Tada, R (2009): Chemical and mineral compositions of sediments from ODP Site 127‐797. Geological Institute, University of Tokyo. http://dx.doi.org/10.1594/PANGAEA.726855 Tips Tip1: Get a PID at the data repository of your choice. Tip2: Is your PID a DOI and do you want to cite it in the format of a specific journal? Use the DOI formatter from CrossCite. TODO: add short quiz FAIR data FAIR stands for ‘Findable, Accessible, Interoperable, and Reusable’. The FAIR data principles act as an international guideline for the result of high-quality data management. With the increase in volume, complexity and creation speed of data, humans are more and more relying on computational support for dealing with data. The principles were defined with the focus on machine-actionability, i.e. the capacity of computational systems to find, access, interoperate and reuse data with none or minimal human intervention. F – Findable By using correct metadata to describe the data, it will be findable. By using a persistent identifier the data can be found by computer systems automatically. A – Accessible The data should be accessible for the long term. Even when underlying data is not accessible, the describing metadata should remain available. I – Interoperable The data can be used and combined with other datasets. To achieve this, the data should be stored in generic file types, not in software specific file types. R – Reusable The options for reuse should be stated clearly in a license. Without a license there is no certainty about the options for reuse and creator rights are implicit. How to achieve FAIR data In general, having a good data management plan will lead to FAIR data. In the case of privacy-sensitive data, it is possible to meet the criteria, but not to share the data openly. In this case you can make sure that a well-described dataset can be found online, while preventing the underlying data to be downloaded and used without permission. If you anonymise your data, presuming the data is of limited sensitivity and you are very sure the data cannot lead back to the persons involved, you can share your data openly. The FAIR Guiding Principles were put together and published in Scientific Data (Mark D. Wilkinson et al., “The FAIR Guiding Principles for Scientific Data Management and Stewardship,” Scientific Data 3 (March 15, 2016): 160018.). TODO: add question H5P quiz? Open science “Open Science is the practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely available, under terms that enable reuse, redistribution and reproduction of the research and its underlying data and methods.” (Source: FOSTER). You have learned that good data management contributes to the findability, accessibility, interoperability and reusability of your research data. This does not necessarily mean that you should make your data openly available. But to open up data, you do need good data management from the earliest possible stage of your research project. TODO: add links to ORION course or other relevant elements Flemish open science plan? Write your data management plan for your data reuse Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module on data sharing and availability for reuse. You should be able to complete the following questions in the section ‘Data availability for reuse’: What secondary use of your data is intended or foreseeable? Where will you make your data available? What access and usage conditions will apply?"},{"title":"01 R basics: installation, help, set up working directory, variables, syntax, scripts","url":"/topics/R/tutorials/Rbasics/tutorial.html","tags":[],"body":"What is R ? R is many things: a project, a language… As a project, R is part of the GNU free software project. The development of R is done under the philosophy that software should be free of charge. This is good for the user, although there are some disadvantages: R comes with ABSOLUTELY NO WARRANTY. This statement comes up on the screen every time you start R. There is no company regulating R as a product. The R project is largely an academic endeavor, and most of the contributors are statisticians, hence the sometimes incomprehensible documentation. As a computer language it was created to allow manipulation of data, statistical analysis and visualization. It is not easy to learn the language if you haven’t done any programming before but it is worth taking the time as it can be a very useful tool. An enormous variety of statistical analyses are available and R allows you to produce graphs exactly as you want them with publication quality. Good things about R It’s free It works on Windows, Mac and Linux It can deal with very large datasets (compared to Excel) A lot of freedom: graphs can be produced to your own taste Supports all statistical analyses: from basic to very complex Bad things about R It can struggle with extremely large datasets Difficult if you don’t have any programming experience Open source: many people contribute thus consistency can be low Open source: documentation can be poor or written by/for experts Can contain bugs and errors: packages that are widely used are probably correct, niche packages can contain errors, there is no central team assessing the quality of the code Installing R R is available on the CRAN website (Comprehensive R Archive Network]. It can be installed on Linux, Mac and Windows. On the top of the CRAN page is a section with Precompiled Binary Distribution: R versions you can download as an .exe file (for Windows users) and are easy to install. What you download is the basic R installation: it contains the base package and other packages considered essential enough to include in the main installation. Exact content may vary with different versions of R. As R is constantly being updated and new versions are constantly released, it is recommended to regularly install the newest version of R. Installing RStudio Although you can work directly in the R editor, most people find it easier to use RStudio on top of R. RStudio is free and available for Windows, Mac and Linux. You need to have R installed to run Rstudio. RStudio user interface Watch this video tutorial on the different components of the RStudio user interface and this video tutorial on how to use the RStudio user interface. The script editor A script is a text file that contains all the commands you want to run. You can write and run scripts and you can also save them so next time you need to do a similar analysis you can change and re-run the script with minimal effort. An R project can contain multiple scripts. The script editor highlights syntax in scripts making it easy to find and prevent errors. It has many features that will help you write scripts e.g. autocompletion, find/replace, commenting. Autocompletion It supports the automatic completion of code, e.g. if you have an object named relfreq in your workspace, type rel in the script editor and it will show a list of possibilities to complete the name. Figure 1: Example for autocompletion Find and replace Find and replace can be opened using Ctrl+F. Adding comments to scripts In scripts you must include comments to help you remember or tell collaborators what you did. Comments are lines that start with a # symbol. This symbol tells R to ignore this line. Comments are displayed in green. You can comment and uncomment large selections of code using: Comment/Uncomment Lines Figure 2: Menu Comment/Uncomment Lines Adding section headings to scripts Add section headings to your scripts using the following format: #Heading Name#### Figure 3: Define section headings At the bottom of the script editor you can quickly navigate to sections in your script. Especially in long scripts this is very useful. Creating a new script Click File in the top menu and select New File > R Script. Figure 4: File Menu / New File Besides a simple R script, there are many other file types you can create: R markdown file: incorporate R-code and its results in a report R Notebook: R Markdown file with chunks of code that can be executed interactively, with output visible beneath the code R Sweave file: incorporate R-code and its results in a Latex report Opening an existing script Click File in the top menu and select Open File. Scripts are opened as a tab in the script editor. You can open several scripts at the same time in RStudio. Running a script To run a script you select the code that you want to execute in the script editor and click the Run button at the top right of the script editor. The code will be executed in the console. Saving a script If there are unsaved changes in a script, the name of the script will be red and followed by an asterisk. To save the script click the Save button: R scripts should have the extension .R Once it is saved the asterisk disappears and the name becomes black. The console The > symbol in the console shows that R is ready to execute code e.g. type 10+3 and press return > 10 + 3 [1] 13 > The result is printed in the console. It is recommended to write commands in a script rather than typing them directly into the console. Creating a script makes it easier to reproduce, repeat and describe the analysis. If you select commands in the script editor and press the Run button, you will see the commands appearing in the console as they are executed. If the > symbol does not reappear upon execution of a command it means that R has crashed or is still calculating. To terminate a command press Esc. The console also has many features that make life easier like autocompletion, retrieving previous commands. Environment A list of all variables (numbers, vectors, plots, models…) that have been imported or generated. The variables that R creates and manipulates are called objects. To remove all variables that have been generated in the RStudio session: > rm(list=ls()) ls() lists the objects in the current workspace and rm() removes them. History An overview of the last 500 commands that were run in the console: see how to use the history. Connections An interface to easily connect to databases in R. Files The list of files and folders in the working directory. RStudio has a default working directory, typically your home folder. Changing the working directory Often you want to work in the folder that contains the data. In that case you can change the working directory. Check which folder R is using as a working directory: > getwd() Change the working directory: > setwd(\"D:/trainingen/zelfgegeven/R/\") comment Comment You need to use / or \\ in paths. Either will work but \\ will not since R sees it as the character that represents a division. Changing your working directory will make relative file references in your code invalid so you type this in the console at the start of the analysis. Alternatively you can change the working directory in the Files tab, expand More and select Set As Working Directory. hands_on Hands-on: Demo Download the demo script for this lesson and open it in RStudio Demo_1.R From the demo script run the Set working directory section hands_on Hands-on: Exercise 1 Set the working directory to the folder that contains the demo script that you have downloaded and check if it was changed. To list the files in the working directory: > list.files() Plots Plots that are generated by the code you run will appear here. To save a plot click the Export button: Packages R is popular because of the enormous diversity of packages. R is essentially a modular environment and you install and load the modules (packages) you need. Packages are available at the CRAN and Bioconductor websites. Installing a package means that a copy of the package is downloaded and unzipped on your computer. If you want to know in what directory R stores the packages, type: >.libPaths() [1] \"D:/R-3.6.0/library\" > to see the default path where R stores packages. If you want to change this folder use the destdir argument of the install.packages() function: > install.packages(\"car\",destdir=\"C:/Users/Janick/R\") You only need to install a package once, as it is saved on your computer. Installing R packages Watch this video tutorial on how to install CRAN packages. When you have made changes to the right side of the Rstudio user interface (packages, files tab…), R is sometimes slow to show these changes. In that case hit the refresh button: Some packages are not available on the CRAN site. Download in compressed format (as a .zip or .tar.gz file) from the source site. To install: select Install from Package Archive File (.zip; .tar.gz) in the Install Packages window and R will put it in the appropriate directory. Figure 5: Installing packages downloaded from their source site Installing Bioconductor packages Bioconductor is a set of R packages that provides tools for the analysis of high-throughput data, e.g. NGS data. Make sure you have the BiocManager package installed: > if (!requireNamespace(\"BiocManager\")) install.packages(\"BiocManager\") The if statement is checking if you already have the BiocManager package installed, if not then install.packages() will install it. BiocManager is a package to install and update Bioconductor packages. Once BiocManager is installed, you can install the Bioconductor core packages: > BiocManager::install() To install additional Bioconductor packages e.g. GenomicFeatures you type the following command: > BiocManager::install(\"GenomicFeatures\") Overview of all available Bioconductor packages and workflows. Installing packages from GitHub Git is a free and open source version control system. Version control helps software developers manage changes to code by keeping track of every change in a special database. If a mistake is made, the developer can turn back the clock and compare earlier versions of the code to fix the mistake. There is an install_github() function in the devtools packageto install R packages hosted on GitHub: > install.packages(\"devtools\") > library(devtools) > devtools::install_github(\"statOmics/MSqRob&copy;MSqRob0.7.6\") Loading packages Each time you want to use a package you have to load it (activate its functions). Loading a package is done by selecting it in the list of installed packages or by typing the following command: > library(\"name_of_package\") If R responds: Error in library(car) : there is no package called 'car' or similar, it means that the car package needs to be installed first. hands_on Hands-on: Demo Run commands of the Installation section of the demo script Help You can find a lot of documentation online: e.g. the getting help section of the R website. R documentation is not easily accessible nor well-structured so it can be a challenge to consult the help files of R packages online. By far the most user-friendly interface for searching the R documentation is the Rdocumentation website. Additional useful links: Documentation of RStudio Quick R by DataCamp: loads of basic and advanced tutorials R-bloggers: R-news and tutorials contributed by bloggers Rseek: Google specifically for R. Google’s R style guide: Programming rules for R designed in collaboration with the entire R user community at Google to make R code easier to read, share, and verify. Access the R documentation in RStudio using commands: help() or ? hands_on Hands-on: Demo From the demo script run the Get help section Viewer Views HTML files that are located on your computer. All RStudio keyboard shortcuts Expressions in R R can handle any kind of data: numerical, character, logical… Character data Character data like “green”, “cytoplasm” must be typed in between single or double quotes: > x x x = 1 > y = 2 > z = x > y is x larger than y? > z FALSE > u = TRUE > v = FALSE > u & v u AND v: FALSE > u | v u OR v: TRUE > !u NOT u: FALSE hands_on Hands-on: Exercise 2a question Question What’s the difference between x=2 and x==2 ? solution Solution The = operator attributes a value to a variable (see next section), x becomes 2. The == is a logical operator, testing whether the logical expression x equals 2 is TRUE or FALSE. hands_on Hands-on: Exercise 2b Check if the words UseR and user are equal. comment R is case sensitive As exercise 2b showed R is indeed case sensitive. Assigning variables A variable allows you to save a value or an object (a plot, a table, a list of values) in R. A value or object is assigned to a variable by the assignment operator v v v = 4 give the same result: a variable called v with value 4 After R has performed the assignment you will not see any output, as the value 4 has been saved to variable v. You can access and use this variable at any time and print its value in the console by running its name: > v [1] 4 You can now use v in expressions instead of 4 > v * v [1] 16 You can re-assign a new value to a variable at any time: > v v [1] \"a cool variable\" R is not very fussy as far as syntax goes. Variable names can be anything, though they cannot begin with a number or symbol. Informative names often involve using more than one word. Providing there are no spaces between these words you can join them using dots, underscores and capital letters though the Google R style guide recommends that names are joined with a dot. Using operators to create variables You can combine variables into a new one using operators (like + or /). Using functions to create variables A function is a piece of code that performs a specific task. Functions are called by another line of code that sends a request to the function to do something or return a variable. The call may pass arguments (inputs) to the function. In other words a function allows you to combine variables (arguments) into a new variable (returned variable). There are lots of built in functions in R and you can also write your own. Even the base package supplies a large number of pre-written functions to use. Other packages are filled with additional functions for related tasks. Calling a function in R has a certain syntax: output p ? ggplot > help(ggplot) This opens the documentation of the function in the Help tab including an overview of the arguments of the function. At the bottom of the documentation page you find examples on how to use the function. The function generates a plot so the plot p is the output of the function. hands_on Hands-on: Demo From the demo script run the Assigning variables section hands_on Hands-on: Exercise 3a Create a variable called patients with value 42 Print the value of patients divided by 2 Create a variable called patients_gr2 with value 24 Print the total number of patients solution Solution patients example(min) hands_on Hands-on: Exercise 3c Calculate and print the sum of patients and patients_gr2 using the sum() function. solution solution: answer sum(patients,patients_gr2) question Question Replace the sum() function with the mean() function. What happens ? solution solution: answer Look at the help of the sum() function. What’s the first argument ? Compare with the first argument of the mean() function question Question Will the code below work ? sum (patients,patients_gr2) question Question Will the code below work ? sum ( patients , patients_gr2 ) Sometimes functions from different packages have the same name. In that case use package::function to specify the package you want to use, e.g. ggplot2::ggplot() where ggplot2 is the name of the package and ggplot() is the name of the function. hands_on Hands-on: Extra exercise 3d Create a variable patients_gr3 with value “twenty” and print the total number of patients solution Solution patients_gr3 <- \"twenty\" patients + patients_gr3 hands_on Hands-on: Extra exercise 3e Create variable x with value 5 Create variable y with value 2 Create variable z as the sum of x and y and print the value of z Print x - y Print the product of x and y and add 2 to it solution Solution x <- 5 y <- 2 z <- x+y z x-y x*y+2 hands_on Hands-on: Extra exercise 3f What is the difference between: correctLogic <- TRUE incorrectLogic <- \"TRUE\" hands_on Hands-on: Extra exercise 3g Is there a difference between: name <- \"Janick\" name <- 'Janick' name <- Janick"},{"title":"03 Vectors and factors","url":"/topics/R/tutorials/Vectors_Factors/tutorial.html","tags":[],"body":"Data structures in R The power of R lies not in its ability to work with simple numbers but in its ability to work with large datasets. R has a wide variety of data structures including scalars, vectors, matrices, data frames, and lists. Vectors The simplest data structure is the vector, a single row consisting of data values of the same type, e.g. all numbers, characters, Booleans… Creating a vector The function c() (short for “combine values” in a vector) is used to create vectors. The only arguments that need to be passed to c() are the values that you want to combine into a vector. You can create a numeric (a), character (b) or logical (c) vector: a Plants_with_lesions days to then subtract increment, if from 5 & x x > 5] question Question What will happen when you run this code ? x(x > 5 & x 5] & x[x 2] question Question What will happen when you run this code ? days[4,5] question Question What will happen when you run this code ? days[4:5] question Question What will happen when you run this code ? days(4:5) hands_on Hands-on: Extra exercise 5c Create vector y with elements 9,2,4 and retrieve the second element of y. solution Solution y 100] hands_on Hands-on: Demo From the demo script run the Logical and arithmetic operations on variables section hands_on Hands-on: Extra exercise 5h Retrieve elements from newVector (exercise 4b) that are larger than the corresponding elements of vector threes (exercise 4d). solution Solution newVector[newVector > threes] Removing, changing or adding elements in a vector To remove an element from a vector use a negative index: ?-? indicates ?NOT? followed by the index of the element you want to remove, e.g. to remove the second element of vector z use: z <- z[-2] Change or add elements by assigning a new value to that element . hands_on Hands-on: Demo From the demo script run the Data removal vectors section hands_on Hands-on: Exercise 6a From vector x (exercise 5a) remove the first 8 elements and store the result in x2. solution Solution x2 <- x[-(1:8)] x2 question Question What will happen when you run this code ? x2 <- x[-1:8] hands_on Hands-on: Extra exercise 6b Retrieve the same elements from z as in exercise 5d2 but first replace the 3rd element by 7. solution Solution z[3] <- 7 z[3:7] Factors You can tell R that a variable is categorical (= text labels representing categories although sometimes numbers are also used) by making it a factor. The difference between a categorical variable and a continuous variable is that a categorical variable represents a limited number of categories. A continuous variable is the result of a measurement and can correspond to an infinite number of values. In most cases categorical data is used to describe other data, it is not used in calculations e.g. which group does a measurement belong to. Storing data as factors ensures that the graphing and statistical functions in R will treat such data correctly. There are two types of categorical data: unranked categorical data do not have an implied order ranked categorical data do have a natural ordering R will treat factors by default as unranked but you can create ordered (ranked) factors. To create a factor, first create a vector and then convert it to a factor using the factor() function: v <- c(1,4,4,4,3,5,4,4,5,3,2,5,4,3,1,3,1,5,3,4) v #[1] 1 4 4 4 3 5 4 4 5 3 2 5 4 3 1 3 1 5 3 4 f <- factor(v,ordered=TRUE) f #[1] 1 4 4 4 3 5 4 4 5 3 2 5 4 3 1 3 1 5 3 4 #Levels: 1 < 2 < 3 < 4 < 5 comment Comment The factor() function creates “Levels”: these are the labels of the categories. The only required argument of the factor() function is a vector of values which will be factorized. Both numeric and character vectors can be made into factors but you will use factor() typically for numerical data that represents categories. When you create a vector containing text values in R you have to factorize it but if you store the vector as a column in a data frame, text data is automatically converted to a factor. When you import data into R using read.() functions, the data is automatically stored in a data frame so text will be automatically converted into a factor. So in reality (since you mostly import data into R) you use factor() mainly to factorize numbers that represent categories. By default, factor() transforms a vector into an unordered factor, as does the automated factorization of the read.() functions. Unordered means that the categories are processed in alphabetical order: High will be plotted before Low since H comes first in the alphabet. If the categories are ranked, you have to create an ordered factor, you have to add two additional arguments: Set ordered to TRUE to indicate that the factor is ordered levels: a vector of category labels (as strings) in the correct order hands_on Hands-on: Demo From the demo script run the Data creation: factors section hands_on Hands-on: Extra exercise 7a Create a vector gender with the following elements: Male, Female, male. Convert gender into a factor with levels: Male and Female Print the content of the factor. What happens? solution Solution gender <- c(\"Male\",\"Female\",\"male\") gender <- factor(gender,levels=c(\"Male\",\"Female\")) gender"},{"title":"11 Modules","url":"/topics/python-programming/tutorials/11_modules/tutorial.html","tags":[],"body":"11.1 Introduction So now that we know how to make functions, how can you re-use them? Imagine that you’ve started writing code and functions in one file and the project has grown to such an extent that it would be easier to maintain it in different files each containing a specific part of the project. Or you want to re-use some of the functions in other projects as well. In Python you can import functions and chunks of code from files. Such a file containing the functions is called a module. Generally we say that we import a definition from a module. A module can have one or multiple functions in it. The file name is the module name with the suffix .py appended. Using the code from this module is possible by using import. In this way you can import your own functions, but also draw on a very extensive library of functions provided by Python (built-in modules). We will first look at the syntax for imports and how to import your own functions, then explore the most commonly used Python libraries. 11.2 How imports work The easiest way to import a module looks like this: import module1 Imagine that in the module module1, there is a function called getMeanValue(). This way of importing does not make the name of the function available; it only remembers the module name module1 which you can than use to access the functions within the module: import module1 module1.getMeanValue([1,2,3]) 11.3 How to create your own module The easiest example is importing a module from within the same working directory. Let’s create a Python module called module1.py with the code of the function getMeanValue() that we have written earlier (and you can find here below). hands_on Create your own module To create your own module from Jupyter Notebook, follow these steps: In order to create a module in Jupyter Lab, first create a new notebook Rename the notebook (e.g. ‘module1.ipynb’) and copy paste the code in the notebook Click ‘File’, ‘Download as’ and ‘Python’ Jupyter will not download it in some local folder, copy it to your current working directory (in our case in the same directory as we’re in right now). Unfortunately, Jupyter Notebook doesn’t have a streamlined & straightforward way of creating Python modules and Python scripts. When you export the notebook, it will always export the whole Notebook and not just a part of it, which makes it very messy if you have a very large notebook. Import the following code in the module1.py file. # When you download this as a Python script, Jupyter will automatically insert the environment shebang here. def getMeanValue(valueList): \"\"\" Calculate the mean (average) value from a list of values. Input: list of integers/floats Output: mean value \"\"\" valueTotal = 0.0 for value in valueList: valueTotal += value numberValues = len(valueList) return (valueTotal/numberValues) 11.4 Import syntax We can now use the module we just created by importing it. In this case where we import the whole ‘module1’ file, we can call the function as a method, similar to the methods for lists and strings that we saw earlier: import module1 print(module1.getMeanValue([4,6,77,3,67,54,6,5])) If we were to write code for a huge project, long names can get exhaustive. Programmers will intrinsically make shortcut names for functions they use a lot. Renaming a module is therefore a common thing to do (e.g. NumPy as np, pandas as pd, etc.): import module1 as m1 print(m1.getMeanValue([4,6,77,3,67,54,6,5])) When importing a file, Python only searches the current directory, the directory that the entry-point script is running from, and sys.path which includes locations such as the package installation directory (it’s actually a little more complex than this, but this covers most cases). However, you can specify the Python path yourself as well. If you’re using the materials from Github, note that within our folders there is a directory named modules and within this folder, there is a module named module2 (recognizable due to its .py extension). In that module there are two functions: ‘getMeanValue’ and ‘compareMeanValueOfLists’. from modules import module2 print(module2.getMeanValue([4,6,77,3,67,54,6,5])) from modules import module2 as m2 print(m2.getMeanValue([4,6,77,3,67,54,6,5])) Another way of writing this is with an absolute path to the module. You can explicitly import an attribute from a module. from modules.module2 import compareMeanValueOfLists print(compareMeanValueOfLists([1,2,3,4,5,6,7], [4,6,77,3,67,54,6,5])) So here we import the function compareMeanValueOfLists (without brackets!) from the file module2 (without .py extension!). In order to have an overview of all the different functions within a module, use dir(): dir(module2) 11.5 Built-in Modules There are several built-in modules in Python, which you can import whenever you like. Python has many ready-to-use functions that can save you a lot of time when writing code. The most common ones are time, sys, os/os.path and re. 11.5.1 time With time you can get information on the current time and date, …: import time time.ctime() # Print current day and time time.time() # Print system clock time time.sleep(10) # Sleep for 5 seconds - the program will wait here See the Python documentation for a full description of time. Also see datetime, which is a module to deal with date/time manipulations. 11.5.2 sys gives you system-specific parameters and functions: import sys sys.argv # A list of parameters that are given when calling this script # from the command line (e.g. ''python myScript a b c'') sys.platform # The platform the code is currently running on sys.path # The directories where Python will look for things to import help(sys.exit) # Exit the code immediately See the Python documentation for a full description. 11.5.3 os and os.path are very useful when dealing with files and directories: import os # Get the current working directory (cwd) currentDir = os.getcwd() currentDir # Get a list of the files in the current working directory myFiles = os.listdir(currentDir) myFiles # Create a directory, rename it, and remove it os.mkdir(\"myTempDir\") os.rename(\"myTempDir\",\"myNewTempDir\") os.removedirs(\"myNewTempDir\") # Create a full path name to the `module2` module in the modules folder myFileFullPath = os.path.join(currentDir,'modules','module2.py') myFileFullPath # Does this file exist? os.path.exists(myFileFullPath) # How big is the file? os.path.getsize(myFileFullPath) # Split the directory path from the file name (myDir,myFileName) = os.path.split(myFileFullPath) print(myDir) print(myFileName) See the Python documentation for os and os.path for a full description. 11.5.4 re A library that is very powerful for dealing with strings is re. It allows you to use regular expressions to examine text - using these is a course in itself, so just consider this simple example: import re myText = \"\"\"Call me Ishmael. Some years ago - never mind how long precisely - having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\"\"\" # Compile a regular expression, myPattern = re.compile(\"(w\\w+d)\") # Look for the first word that starts with a w, # is followed by 1 or more characters (\\w+) # and ends in a d mySearch = myPattern.search(myText) # mySearch will be None if nothing was found if mySearch: print(mySearch.groups()) See the full Python documentation on regular expressions for more information. 11.6 Putting everything together hands_on Exercise 11.6.1 Make a new directory in which you write out 5 files with a 2 second delay. Each file should contain the date and time when it was originally written out. solution Solution # 1 import time, os # Create a variable for the directory name myDir = \"timeTest\" # Check whether the directory exists, if not create it if not os.path.exists(myDir): os.mkdir(myDir) # Loop from 1 to 5 for i in range(1,6): # Get the current time currentTime = time.ctime() # Write out the file - use i to give a different name to each filePath = os.path.join(myDir,\"myFile{}.txt\".format(i)) outFileHandle = open(filePath,'w') outFileHandle.write(\"{}\\n\".format(currentTime)) outFileHandle.close() print(\"Written file {}...\".format(filePath)) # Sleep for 2 seconds time.sleep(2) hands_on Exercise 11.6.2 Write a function to read in a FASTA file with an RNA sequence and return the RNA sequence (in 3 base unit chunks). solution Solution # 2 import os def readRnaFastaFile(fileName): if not os.path.exists(fileName): print(\"Error: File {} not available!\".format(fileName)) return (None,None,None) fconnect = open(fileName) lines = fconnect.readlines() fconnect.close() sequenceInfo = [] moleculeName = None description = None # Get information from the first line - ignore the > firstLine = lines[0] firstLineCols = firstLine[1:].split() moleculeName = firstLineCols[0] description = firstLine[1:].replace(moleculeName,'').strip() # Now get the full sequence out fullSequence = \"\" for line in lines[1:]: line = line.strip() fullSequence += line # Divide up the sequence depending on type (amino acid or nucleic acid) for seqIndex in range(0,len(fullSequence),3): sequenceInfo.append(fullSequence[seqIndex:seqIndex+3]) return (moleculeName,description,sequenceInfo) print(readRnaFastaFile(\"data/rnaSeq.txt\")) hands_on Exercise 11.6.3 Write a program where you ask the user for a one-letter amino acid sequence, and print out the three-letter amino acid codes. Download the dictionary from section 8.2 and save it as a module named SequenceDicts.py first. solution Solution # 3 # Note how you can import a function (or variable) with a different name for your program! from modules.SequenceDicts import proteinOneToThree as oneToThreeLetterCodes oneLetterSeq = input('Give one letter sequence:') if oneLetterSeq: for oneLetterCode in oneLetterSeq: if oneLetterCode in oneToThreeLetterCodes.keys(): print(oneToThreeLetterCodes[oneLetterCode]) else: print(\"One letter code '{}' is not a valid amino acid code!\".format(oneLetterCode)) else: print(\"You didn't give me any information!\") hands_on Exercise 11.6.4 Write a program where you translate the RNA sequence data/rnaSeq.txt into 3 letter amino acid codes. Use the dictionary from section 8.2 (called myDictionary) and save it as a module named SequenceDicts.py first. You can use the readFasta.py module from the modules folder. solution Solution from modules.SequenceDicts import standardRnaToProtein, proteinOneToThree from modules.readFasta import readRnaFastaFile (molName,description,sequenceInfo) = readRnaFastaFile(\"data/rnaSeq.txt\") proteinThreeLetterSeq = [] for rnaCodon in sequenceInfo: aaOneLetterCode = standardRnaToProtein[rnaCodon] aaThreeLetterCode = proteinOneToThree[aaOneLetterCode] proteinThreeLetterSeq.append(aaThreeLetterCode) print(proteinThreeLetterSeq) hands_on Exercise 11.6.5 Write a program that: Has a function readSampleInformationFile() to read the information from this sample data file into a dictionary. Also check whether the file exists. Has a function getSampleIdsForValueRange() that can extract sample IDs from this dictionary. Print the sample IDs for pH 6.0-7.0, temperature 280-290 and volume 200-220 using this function. solution Solution import os def readSampleInformationFile(fileName): # Read in the sample information file in .csv (comma-delimited) format # Doublecheck if file exists if not os.path.exists(fileName): print(\"File {} does not exist!\".format(fileName)) return None # Open the file and read the information fileHandle = open(fileName) lines = fileHandle.readlines() fileHandle.close() # Now read the information. The first line has the header information which # we are going to use to create the dictionary! fileInfoDict = {} headerCols = lines[0].strip().split(',') # Now read in the information, use the first column as the key for the dictionary # Note that you could organise this differently by creating a dictionary with # the header names as keys, then a list of the values for each of the columns. for line in lines[1:]: line = line.strip() # Remove newline characters cols = line.split(',') sampleId = int(cols[0]) fileInfoDict[sampleId] = {} # Don't use the first column, is already the key! for i in range(1,len(headerCols)): valueName = headerCols[i] value = cols[i] if valueName in ('pH','temperature','volume'): value = float(value) fileInfoDict[sampleId][valueName] = value # Return the dictionary with the file information return fileInfoDict def getSampleIdsForValueRange(fileInfoDict,valueName,lowValue,highValue): # Return the sample IDs that fit within the given value range for a kind of value #sampleIdList = fileInfoDict.keys() #sampleIdList.sort() sampleIdList = sorted(fileInfoDict.keys()) sampleIdsFound = [] for sampleId in sampleIdList: currentValue = fileInfoDict[sampleId][valueName] if lowValue <= currentValue <= highValue: sampleIdsFound.append(sampleId) return sampleIdsFound if __name__ == '__main__': fileInfoDict = readSampleInformationFile(\"../data/SampleInfo.txt\") print(getSampleIdsForValueRange(fileInfoDict,'pH',6.0,7.0)) print(getSampleIdsForValueRange(fileInfoDict,'temperature',280,290)) print(getSampleIdsForValueRange(fileInfoDict,'volume',200,220))"},{"skip_index":true,"title":"Search results","url":"/search/","tags":[],"body":""},{"title":"","url":"/CODE_OF_CONDUCT.md","tags":[],"body":"Project Code of Conduct ============================== This code of conduct outlines our expectations for participants within the community, as well as steps to reporting unacceptable behavior. We are committed to providing a welcoming and inspiring community for all and expect our code of conduct to be honored. Anyone who violates this code of conduct may be banned from the community. Our open source community strives to: * **Be friendly and patient.** * **Be welcoming**: We strive to be a community that welcomes and supports people of all backgrounds and identities. This includes, but is not limited to members of any race, ethnicity, culture, national origin, colour, immigration status, social and economic class, educational level, sex, sexual orientation, gender identity and expression, age, size, family status, political belief, religion, and mental and physical ability. * **Be considerate**: Your work will be used by other people, and you in turn will depend on the work of others. Any decision you take will affect users and colleagues, and you should take those consequences into account when making decisions. Remember that we're a world-wide community, so you might not be communicating in someone else's primary language. * **Be respectful**: Not all of us will agree all the time, but disagreement is no excuse for poor behavior and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It’s important to remember that a community where people feel uncomfortable or threatened is not a productive one. * **Be careful in the words that we choose**: We are a community of professionals, and we conduct ourselves professionally. Be kind to others. Do not insult or put down other participants. Harassment and other exclusionary behavior aren't acceptable. This includes, but is not limited to: Violent threats or language directed against another person, Discriminatory jokes and language, Posting sexually explicit or violent material, Posting (or threatening to post) other people’s personally identifying information (“doxing”), Personal insults, especially those using racist or sexist terms, Unwelcome sexual attention, Advocating for, or encouraging, any of the above behavior, Repeated harassment of others. In general, if someone asks you to stop, then stop. * **Try to understand why we disagree**: Disagreements, both social and technical, happen all the time. It is important that we resolve disagreements and differing views constructively. Remember that we’re different. Diversity contributes to the strength of our community, which is composed of people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn’t mean that they’re wrong. Don’t forget that it is human to err and blaming each other doesn’t get us anywhere. Instead, focus on helping to resolve issues and learning from mistakes. ### Diversity Statement We encourage everyone to participate and are committed to building a community for all. Although we will fail at times, we seek to treat everyone both as fairly and equally as possible. Whenever a participant has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, it is our responsibility to listen carefully and respectfully, and do our best to right the wrong. Although this list cannot be exhaustive, we explicitly honor diversity in age, gender, gender identity or expression, culture, ethnicity, language, national origin, political beliefs, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, including participants with disabilities. ### Reporting Issues If you experience or witness unacceptable behavior, or have any other concerns, please report it by contacting Alexander Botzki (bits@vib.be). To report an issue involving Alexander Botzki please email hr@vib.be. All reports will be handled with discretion. In your report please include: - Your contact information. - Names (real, nicknames, or pseudonyms) of any individuals involved. If there are additional witnesses, please include them as well. Your account of what occurred, and if you believe the incident is ongoing. If there is a publicly available record (e.g. a mailing list archive or a public IRC logger), please include a link. - Any additional information that may be helpful. After filing a report, a representative will contact you personally, review the incident, follow up with any additional questions, and make a decision as to how to respond. If the person who is harassing you is part of the response team, they will recuse themselves from handling your incident. If the complaint originates from a member of the response team, it will be handled by a different member of the response team. We will respect confidentiality requests for the purpose of protecting victims of abuse. ### Attribution & Acknowledgements This code of conduct is based on the Open Code of Conduct from the TODOGroup."},{"title":"","url":"/badges/index.html","tags":[],"body":"VIB Bioinformatics Core Bioinformatics Training Collection of courses developed and maintained by VIB Bioinformatics Core help Help Contact us Server Training Badges Collection of badges denoting compatibility with training workflows from the training materials repository. github View on GitHub SCIENCE MEETS LIFE"},{"title":"","url":"/courses/index.html","tags":[],"body":"Lia This page requires JavaScript to be enabled! LiaScript - Problem Hello! Welcome to LiaScript. This site is not fully supported in Internet Explorer 11 (and earlier) versions. As an alternative, you can use either of the options below to browse the site: Use Firefox browser. Here is the download link. Use Google Chrome browser. Here is the download link. Thanks."},{"title":"","url":"/shared/literature.md","tags":[],"body":"# Literature ##Deep sequencing **Zentner and Henikoff (2012):** [Surveying the epigenomic landscape, one base at a time](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2012-13-10-250), (doi:10.1186/gb-2012-13-10-250) - Overview of popular *-seq techniques; very nice description of DNase-seq, MNase-seq, FAIRE-seq etc. **Son and Taylor (2011):** [Preparing DNA Libraries for Multiplexed Paired-End Deep Sequencing for Illumina GA Sequencers](https://www.ncbi.nlm.nih.gov/pubmed/21400673), (doi:10.1002/9780471729259.mc01e04s20) - Paper on multiplexing; describes the individual steps of the Illumina deep sequencing protocols quite in detail **Illumina's technical report** - focuses on [Illumina's sequencing technology](https://www.illumina.com/technology.html); nice educative figures ##NGS data formats - UCSC has a very good overview with brief descriptions of BED, bedGraph, bigWig etc.: https://genome.ucsc.edu/FAQ/FAQformat.html - [VCF format](https://gatkforums.broadinstitute.org/gatk/discussion/1268/how-should-i-interpret-vcf-files-produced-by-the-gatk) (encoding SNPs, indels etc.): Very readable, albeit not exhausting description - Transcriptomes are often saved in [GFF3 format](https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md) (this is what TopHat needs, for example), but just to make things more complicated, GTF is another format used for transcriptome information, too ##Bioinformatic Tools (Linux, R, BEDTools etc.) - Manuals, courses, original papers - Why and how is bioinformatics software special? **Altschul et a. (2013)** [The anatomy of successful computational biology software](https://www.ncbi.nlm.nih.gov/pubmed/24104757), (doi:10.1038/nbt.2721) **(Highly recommended to read!)** - **Bild et al. (2014)** [A Field Guide to Genomics Research](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001744), (doi:10.1371/journal.pbio.1001744) - Very readable introduction about the different caveats of genomics research (with cute cartoons!) **Linux Command Line** - [Linux & Perl Primer for Biologists](http://korflab.ucdavis.edu/Unix_and_Perl/unix_and_perl_v3.1.1.html) - Very entertaining introduction to command line commands and perl scripts with a focus on bioinformatic application, i.e. handling of DNA sequences - [Linux Tutorial for Beginners](http://www.ee.surrey.ac.uk/Teaching/Unix/) - Thorough, but concise online tutorial introducing the very basics of handling the Linux command line - [Writing Linux shell scripts](http://www.freeos.com/guides/lsst/index.html) - Useful for slightly more advanced Linux command line users **R** - [Hands on R course](https://www.uwyo.edu/mdillon/hor.html) - For beginners - R is probably the most widely used open-source statistical software; through our epicenter website you can also access RStudio which provides are very nice interface to working and plotting with R. In fact, most of the plots generated within Galaxy are generated through R scripts, so if you're not happy with the default formats of the Galaxy graphs, definitely have a look at R yourself. The learning curve is steep, but it is worth it. **BEDTools** - [BEDTools Manual](https://bedtools.readthedocs.org) - When working with genomic intervals (e.g. genes, peaks, enriched regions...), BEDTools are invaluable! The manual is a very good read and we refer to it almost daily."},{"title":"","url":"/snippets/add_custom_build.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Adding a custom Database/Build (dbkey) > > - In the top menu bar, go to the *User*, and select *Custom Builds* > > - Choose a name for your reference build `{{ include.name }}` > > - Choose a dbkey for your reference build `{{ include.dbkey }}` > > - Under **Definition**, select the option `FASTA-file from history` > > - Under **FASTA-file**, select your fasta file `{{ include.fasta }}` > > - Click the **Save** button > {: .tip} >"},{"title":"","url":"/snippets/add_tag.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Adding a tag > > * Click on the dataset > > * Click on {% icon galaxy-tags %} **Edit dataset tags** > > * Add a tag starting with `#` > > > > Tags starting with `#` will be automatically propagated to the outputs of tools using this dataset. > > > > * Check that the tag is appearing below the dataset name > > > {: .tip} >"},{"title":"","url":"/snippets/ansible_local.md","tags":[],"body":"> ### {% icon tip %} Tip: Running Ansible on your remote machine > It is possible to have ansible installed on the remote machine and run it there, not just from your local machine connecting to the remote machine. > > Your hosts file will need to use `localhost`, and whenever you run playbooks with `ansible-playbook -i hosts playbook.yml`, you will need to add `-c local` to your command. > > Be **certain** that the playbook that you're writing on the remote machine is stored somewhere safe, like your user home directory, or backed up on your local machine. The cloud can be unreliable and things can disappear at any time. {: .tip}"},{"title":"","url":"/snippets/build_dataset_list.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Build a dataset list > > * Click on {% icon galaxy-selector %} **Operations on multiple datasets** at the top of the history panel > > * Check the boxes next the datasets to select > > * In **For all selected...**, choose **Build dataset list** > > * Ensure that only the wanted samples are selected > > * Enter a name for the new collection > > * Click on **Create list** > {: .tip} >"},{"title":"","url":"/snippets/build_list_collection.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Creating a dataset collection > > > > * Click on **Operations on multiple datasets** (check box icon) at the top of the history panel ![Operations on multiple datasets button](../../../galaxy-data-manipulation/images/historyItemControls.png) > > * Check all the datasets in your history you would like to include > > * Click **For all selected..** and choose **Build dataset list** > > > > ![build list collection menu item]({{site.baseurl}}/topics/galaxy-data-manipulation/images/buildList.png){:width=\"15%\"} > > > > * Enter a name for your collection > > * Click **Create List** to build your collection > > * Click on the checkmark icon at the top of your history again > {: .tip} >"},{"title":"","url":"/snippets/change_datatype.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Changing the datatype > > * Click on the {% icon galaxy-pencil %} **pencil icon** for the dataset to edit its attributes > > * In the central panel, click on the {% icon galaxy-chart-select-data %} **Datatypes** tab on the top > > * Select `{{ include.datatype }}` > > * Click the **Change datatype** button > {: .tip} >"},{"title":"","url":"/snippets/change_dbkey.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Changing Database/Build (dbkey) > > - Click on the {% icon galaxy-pencil %} **pencil icon** for the dataset to edit its attributes > > - In the central panel, change the **Database/Build** field > > - Select your desired database key from the dropdown list: `{{ include.dbkey }}` > > - Click the **Save** button > {: .tip} >"},{"title":"","url":"/snippets/create_dataset_collection.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Create a dataset collection > > > > 1. Click on {% icon galaxy-selector %} icon (**Operation on multiple datasets**) on the top of the history > > 2. Select all the datasets for the collection > > 3. Expand **For all selected** menu > > 4. Select **Build dataset list** > > 5. Enter a name for the collection > > 6. Tick **Hide original elements?** > > 5. Click on **Create list** (and wait a bit) > {: .tip} >"},{"title":"","url":"/snippets/create_new_file.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Creating a new file > > > > * Open the Galaxy Upload Manager > > * Select **Paste/Fetch Data** > > * Paste the file contents into the text field > > {% if include.convertspaces %} * From the Settings menu ({% icon galaxy-gear %}) select **Convert spaces to tabs** {% endif %} > > * Press **Start** and **Close** the window > {: .tip} >"},{"title":"","url":"/snippets/create_new_history.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Creating a new history > > > > 1. Click on the {% icon galaxy-gear %} icon (**History options**) on the top of the history panel > > 2. Click on **Create New** > {: .tip} >"},{"title":"","url":"/snippets/display_extra_training.md","tags":[],"body":"{% for training in include.extra_trainings %} {% if training.type == \"internal\" %} {% assign extra_topic_metadata = site.data[training.topic_name] %} {% assign extra_topic = site.pages | topic_filter:training.topic_name %} {{ extra_topic_metadata.title }} {% if training.tutorials %} {% for extra_tuto in training.tutorials %} {% for topic_tuto in extra_topic %} {% if extra_tuto == topic_tuto.tutorial_name %} {% assign sep = \"\" %} {{ topic_tuto.title }}: {% if topic_tuto.slides %} {% icon slides %} slides {% assign sep = \"-\" %} {% endif %} {% if topic_tuto.hands_on %} {% if topic_tuto.hands_on_url %} {{ sep }} {% icon tutorial %} hands-on {% else %} {{ sep }} {% icon tutorial %} hands-on {% endif %} {% endif %} {% endif %} {% endfor %} {% endfor %} {% endif %} {% elsif training.type == \"none\" %} {{ training.title }} {% else %} {{ training.title }} {% endif %} {% endfor %}"},{"title":"","url":"/snippets/display_extra_training_slides.md","tags":[],"body":"{% capture newLine %} {% endcapture %} {% for training in include.extra_trainings %} {% if training.type == \"internal\" %} {% assign extra_topic_metadata = site.data[training.topic_name] %} {% assign extra_topic = site.pages | topic_filter:training.topic_name %} {% capture topic_desc %}[{{ extra_topic_metadata.title }}]({{ site.baseurl }}/topics/{{ training.topic_name }}){% endcapture %} {% if training.tutorials %} {% for extra_tuto in training.tutorials %} {% for topic_tuto in extra_topic %} {% if extra_tuto == topic_tuto.tutorial_name %} {% assign tuto_desc = topic_tuto.title | append: \": \" | prepend: \" - \" | prepend: newLine %} {% if topic_tuto.slides %} {% capture tuto_slide_desc %}[{% icon slides %} slides]({{ site.baseurl }}/topics/{{ training.topic_name }}/tutorials/{{ topic_tuto.tutorial_name }}/slides.html){% endcapture %} {% assign tuto_desc = tuto_desc | append: tuto_slide_desc %} {% assign sep = \" - \" %} {% endif %} {% if topic_tuto.hands_on %} {% if topic_tuto.hands_on_url %} {% capture tuto_hands_on_desc %}{{ sep }}[{% icon tutorial %} hands-on]({{ topic_tuto.hands_on_url }}){% endcapture %} {% else %} {% capture tuto_hands_on_desc %}{{ sep }}[{% icon tutorial %} hands-on]({{ site.baseurl }}/topics/{{ training.topic_name }}/tutorials/{{ topic_tuto.tutorial_name }}/tutorial.html){% endcapture %} {% endif %} {% assign tuto_desc = tuto_desc | append: tuto_hands_on_desc %} {% endif %} {% assign topic_desc = topic_desc | append: tuto_desc %} {% endif %} {% endfor %} {% endfor %} {% endif %} - {{ topic_desc }} {% elsif training.type == \"none\" %} - {{ training.title }} {% else %} - [{{ training.title }}]({{ training.link }}) {% endif %} {% endfor %}"},{"title":"","url":"/snippets/extra_protein.md","tags":[],"body":"additional HTML formatted text could be inserted as well technical setup of the course could be included or we develop a semiautomated mechanism or we add it as a lesson"},{"title":"","url":"/snippets/extract_workflow.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Extracting a workflow from history > > > > 1. Remove any failed or unwanted jobs from your history. > > 2. Click on **History options** (gear icon {% icon galaxy-gear %}) at the top of your history panel. > > 3. Select **Extract workflow** > > 4. Check the steps, enter a name for your workflow, and press the **Create Workflow** button. > > > {: .tip} >"},{"title":"","url":"/snippets/history_create_new.md","tags":[],"body":"> > > ### {% icon tip %} Starting a new history > > > > * Click the **gear icon** at the top of the history panel > > * Select the option **Create New** from the menu > {: .tip} >"},{"title":"","url":"/snippets/import_from_data_library.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Importing data from a data library > > > > As an alternative to uploading the data from a URL or your computer, the files may also have been made available from a *shared data library*: > > > > * Go into **Shared data** (top panel) then **Data libraries** > > {% if include.path %} > > * {{ include.path }} > > {% else %} > > * Find the correct folder (ask your instructor) > > {% endif %} > > * Select the desired files > > * Click on the **To History** button near the top and select **{{ include.astype | default: \"as Datasets\" }}** from the dropdown menu > > * In the pop-up window, select the history you want to import the files to (or create a new one) > > * Click on **Import** > {: .tip} >"},{"title":"","url":"/snippets/import_via_link.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Importing data via links > > > > * Copy the link location > > * Open the Galaxy Upload Manager ({% icon galaxy-upload %} on the top-right of the tool panel) > > {% if include.collection %} > > * Click on **Collection** on the top > > {% endif %} > > {% if include.collection_type %} > > * Click on **Collection Type** and select `{{ include.collection_type }}` > > {% endif %} > > * Select **Paste/Fetch Data** > > * Paste the link into the text field > > {% if include.link %} > > `{{ include.link }}` > > {% endif %} > > {% if include.link2 %} > > `{{ include.link2 }}` > > {% endif %} > > {% if include.format %} > > * Change **Type** from \"Auto-detect\" to `{{ include.format }}` > > {% endif %} > > {% if include.genome %} > > * Change **Genome** to `{{ include.genome }}` > > {% endif %} > > * Press **Start** > > {% if include.collection %} > > * Click on **Build** when available > > {% if include.pairswaptext %} > > * Ensure that the forward and reverse reads are set to {{ include.pairswaptext }}, respectively. > > * Click **Swap** otherwise > > {% endif %} > > * Enter a name for the collection > > {% if include.collection_name_convention %} > > * A useful naming convention is to use {{ include.collection_name_convention }} > > {% endif %} > > {% if include.collection_name %} > > * {{ include.collection_name }} > > {% endif %} > > * Click on **Create list** (and wait a bit) > > {% else %} > > * **Close** the window > > {% endif %} > > By default, Galaxy uses the URL as the name, so rename the files with a more useful name. > {: .tip} >"},{"title":"","url":"/snippets/import_workflow.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Importing a workflow > > - Click on *Workflow* on the top menu bar of Galaxy. You will see a list of all your workflows. > > - Click on the upload icon {% icon galaxy-upload %} at the top-right of the screen > > - Provide your workflow > > - Option 1: Paste the URL of the workflow into the box labelled *\"Archived Workflow URL\"* > > - Option 2: Upload the worflow file in the box labelled *\"Archived Workflow File\"* > > - Click the **Import workflow** button > {: .tip} >"},{"title":"","url":"/snippets/rename_dataset.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Renaming a dataset > > - Click on the {% icon galaxy-pencil %} **pencil icon** for the dataset to edit its attributes > > - In the central panel, change the **Name** field {% if include.name %} to `{{ include.name }}` {% endif %} > > - Click the **Save** button > {: .tip} >"},{"title":"","url":"/snippets/rename_history.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Renaming a history > > > > 1. Click on **Unnamed history** (or the current name of the history) (**Click to rename history**) at the top of your history panel > > 2. Type the new name > > 3. Press Enter > {: .tip} >"},{"title":"","url":"/snippets/run_workflow.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Running a workflow > > - Click on *Workflow* on the top menu bar of Galaxy. You will see a list of all your workflows. > > - Click on the dropdown menu {% icon galaxy-dropdown %} next to your workflow > > - Select **Run** from the list > > - Configure the workflow as needed > > - Click the **Run Workflow** button at the top-right of the screen > > - You may have to refresh your history to see the queued jobs > {: .tip} >"},{"title":"","url":"/snippets/select_collection.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Select collection of datasets > > > > 1. Click on {% icon param-collection %} **Dataset collection** > > 2. Select the interesting collection in the list > {: .tip} >"},{"title":"","url":"/snippets/select_multiple_datasets.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Select multiple datasets > > > > 1. Click on {% icon param-files %} **Multiple datasets** > > 2. Select several files by keeping the Ctrl (or > > COMMAND) key pressed and clicking on the files of interest > {: .tip} >"},{"title":"","url":"/snippets/use_scratchbook.md","tags":[],"body":"> > > ### {% icon tip %} Tip : Using the Scratchbook > > > > Multiple plots can be compared side-by-side by enabling the *Scratchbook* > > 1. Click on the {% icon galaxy-scratchbook %} *Scratchbook* icon > > 1. Click on the {% icon galaxy-eye %} symbol of the first dataset > > 1. Resize the window of the dataset to desired dimensions > > 1. Click any point in the grey space to close the Scratchbook > > 1. Click on the {% icon galaxy-eye %} symbol of the second dataset > > 1. Resize both windows to desired dimensions > {: .tip} >"},{"title":"","url":"/snippets/warning_results_may_vary.md","tags":[],"body":"> ### {% icon comment %} Note: results may vary > > Your results may be slightly different from the ones presented in this tutorial > due to differing versions of tools, reference data, external databases, or > because of stochastic processes in the algorithms. > {: .comment}"},{"title":"","url":"/topics/protein-structure-analysis/tutorials/visualise-structures/Visualizing_protein_structures_with_YASARA_exercises.md","tags":[],"body":"Exercises created by Joost Van Durme > Part of [Protein Structure Analysis > training](Protein_Structure_Analysis_training \"wikilink\") ## Install Python and PovRay Python and PovRay should be installed already, so you can skip this part. The programming language Python must be installed to use some very useful YASARA features. Simply start YASARA as administrator. Right click the YASARA icon on the desktop and choose \"Run as administrator\". Once the program is opened, click Help > Install program > Python PovRay is used to make high quality publication-ready images and should be downloaded first with: Help > Install program > Povray ## Tutorial movie Play the movie \"Working with YASARA\" movie: Help > Play help movie > General: Working with YASARA ## Scene styles Open the PDB with code 1TRZ in YASARA. File > Load > PDB file from Internet If this option is not there, it means you haven't installed Python yet. Please check above. The molecule will be loaded and presented in the ball style. Different scene styles exist to rapidly change the view: - F1: Ball - F2: Ball & Stick - F3: Stick - F4: C-alpha - F5: Tube - F6: Ribbon - F7: Cartoon - F8: Toggle sidechains on/off (press multiple times and see what happens) Be careful! If you have just made a nice close-up of e.g. an active site where you show some residues and hide others, and put some atoms in balls while others are in sticks, you will lose everything when you press one of the F-keys!!! The F-keys change the viewing style without asking. Try all the different scene styles! ## Showing and hiding residues The function keys F1-F3 show all atoms and residues by default. The keys F4-F7 do not explicitly show atoms and residues but are merely a impressionistic representation of the structure. The F8 keys does, to a certain extent, show atoms, but only of side chains, not main chain atoms. Mostly to do structure analysis, we want to show only the most interesting residues, the ones we want to analyze, and hide all the others. The structure of insulin was crystallized together with some water molecules. In many cases, it is no problem to permanently delete those waters. To visualize the waters, select an atom view such as F1, F2 or F3. See the red water (oxygen) atoms floating around the surface? Edit > Delete > Waters Then select the base scene style without any explicit atoms, e.g. tube style (F5). Press F5. This is our representation of the backbone. There are several ways to show the residues of interest: 1. From the menu - View > Show atoms in > Residue Select Cys7 from Molecule **A** and press OK 2. From the sequence selector ![seqselector.png](seqselector.png \"seqselector.png\") - Hover the mouse on the bottom of the screen, you will see the sequence selector opening. Open it permanently by pressing the blue nailpin on the left side of it. Search for Cys7 from Molecule **B**, right-click and select: Show > Residue Now show the atoms of His5 in Molecule B using a method of choice. And now that we're on it, what is special about the two cysteines we just visualized? **Hiding** individual atoms or residues works in the same way as showing them, only now you should go to **Hide atoms** in the menus. ## Showing and hiding secondary structure Most published molecular images show a detailed active site and all the rest is hidden for clarity. From the previous exercise we show the atoms of 3 residues (let's assume this is our active site). Now secondary structure of the rest of the molecule is also still visible. To hide all that, we do not have to hide atoms, but hide the secondary structure (the F5 tube view) from the rest of the structure. Atoms and residues in YASARA are not the same as the term 'secondary structure'. Atoms and residues are balls and sticks, 'secondary structure' is an artistic impression of the structure (beta sheet arrows, helix ribbons, ...). If you get this concept, you are a YASARA master. So let's hide many of the secondary structure, but keep just a few stretches around our active site. Our active site is Cys7 (A), Cys7 (B) and His 5 (B). This can be done in several ways. Since we would have to hide almost everything, I propose to hide first everything and then show again those stretches that we want. But if you have a better idea, I would like to hear it. Hide all secondary structure: View > Hide secondary structure of > All Then show stretches of residues 2-10 in Mol B and residues 4-10 in Mol A in tube view as: View > Show secondary structure > Tube through > Residue Then select the correct stretches of residues by keeping the CTRL key pressed to select multiple residues. There are still some metal-bound histidines flying around that weren't hidden because they are metal bound (a YASARA specific thing). Hide those histidines by clicking on one of the sidechain atoms, then right-click and select: Hide atoms > Residue The nasty dative bonds and metals can be removed simply by deleting all of them: Edit > Delete > Residue > Name In the name column select all the metals and ions you can find. Et voilà, a publication ready image\\! [center](image:Insulin_hires.jpg \"wikilink\") ## Labels You can put labels on the residues you want to highlight by going to the main menu or selecting an atom from a residue and right-click. In the latter case you select: Label > Residue Note that *residue name* and *residue number* is automatically selected. Change the height to 0.5 or so and select a nice color for the label. Presto\\! ## Colors You can color on all levels: atoms, residues, molecules and objects. So be careful, if you color a residue, all of its atoms will get that color. If you color a molecule, all atoms in that molecule will get that color. Let's color the secondary structure (the backbone in our case) of our active site in orange. But the sidechains should keep their Element colors. So we shouldn't color entire residues, but only a selected atom set. Therefore our selection will be at the atom level, not the residue level. Go to: View > Color > Atom > Belongs to or has > Backbone Then select the orange color (color code 150) and select 'Apply unique color'. Beautiful, isn't it? ## Saving all the beautiful work It would be a pitty that you spent hours creating fancy molecular graphics for that next Nature paper while you can't continue on the work the next day. That's why YASARA can save the entire Scene including orientations, colors, views, everything. To save the current scene, go to: File > Save as > YASARA Scene Choose a filename such as MyInsulin.sce To load the work again in YASARA go to: File > Load > YASARA Scene Careful: loading a Scene will erase everything else! ## Creating high quality images To save the current view to a high quality publication ready image file, go to: File > Save as > Ray-traced hires screenshot This requires that the PovRay program has been installed. See the first item on this page. Usually, you prefer to have a transparent background, so check the respective box. ## Distances **Distances** between atoms are calculated as follows: - select the first atom - keep CTRL pressed and select the second atom. - left of the screen indicates the 'Marked Distance' in Angstrom. What is the distance between the C-alpha (CA) atoms of Tyr19 and Leu16? To solve the question you need to select a view that shows you atoms including C-alphas. Possible views or scene styles that show these atoms can be F1 (ball), F2 (stick), F3 (ball\\&stick) and F4 (C-alpha). The views F5-F8 won't show you any CA's explicitly. Try it. So you've probably noticed that pressing the CTRL button allows you to select multiple atoms. This is important for the next exercise. ## Hydrogen bonds To show hydrogen bonds, YASARA needs the actual hydrogens to be present. In NMR structures these are normally there. But in X-Ray structures hydrogens are missing. Luckily YASARA can add the hydrogens for you. Select tube view (F5) and toggle on the sidechains with F8. Add hydrogens with: Edit > Add > Hydrogens to all Then show the hydrogen-bonds: View > Show interactions > Hydrogen bonds of> All > OK If the view is to chaotic for you, toggle off the sidechains with F8 (press untill the sidechains are hidden). Do you see the typical helix and beta sheet pattern? Arg22 from Molecule/Chain B is making an hydrogen bonded electrostatic interaction (salt bridge) with another residue. Which residue? To remove the hydrogen bonds, you have multiple choices: View > Hide hydrogen bonds of > All or just delete all hydrogens (this will also delete all hydrogen bonds): Edit > Delete > Hydrogens ## Surfaces It can be very useful and informative to show the molecular surface of a protein. you can visualize cavities, ligand binding sites, etc ... To show the molecular surface of one monomer of dimeric insulin, go to: View > Show surface of > Molecule Select in the *Name* column A and B (these are the two chains in 1 subunit). Press *Continue with surface color* and make sure Alpha is 100. Any number lower than 100 will create transparency in the surface (could be nice as well). ## Molecular graphics exercise Try to reproduce the following image of the 1TRZ insulin structure (hints below): [image:insulin.png](image:insulin.png \"wikilink\") Hints: - choose the proper secondary structure scene style (F6 was used here) - find the correct orientation first - color all backbone atoms in gray - find the residue numbers of the 2 colored helices - color those residues magenta - show the sidechain atoms and the CA of the two histidines and the glutamate - color the sidechain atoms of all residues in the Element color - label the histidines and the glutamate - if you need some help how to change the parameters for the label, please have a look at Help -\\> Show user manual and search in Commands / Index ## More coloring Download GroEL via PDB code 1WE3 in YASARA. Try to reproduce (approximately) the following image (hints below): [image:groel.png](image:groel.png \"wikilink\") Hints: - load the PDB as File \\> Load \\> PDB file from internet - zoom out and find the correct orientation - delete the ADP, DMS and Mg molecules (are treated as residues in YASARA). So Edit \\> Delete \\> Residue \\> Adp ... - color by molecule (every molecule will get another color) and color by gradient (now you need to specify 2 colors, the begin and end color). - choose a first color (eg. color with code 0) - choose a second color (eg. color with code 300, so you go over the entire color wheel spectrum) More exercises can be found on the [basic bioinformatics exercises page](http://wiki.bits.vib.be/index.php/Exercises_on_Protein_Structure)."}]}